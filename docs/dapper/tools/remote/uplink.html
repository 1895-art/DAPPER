<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>dapper.tools.remote.uplink API documentation</title>
<meta name="description" content="Tools related to running experimentes remotely â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dapper.tools.remote.uplink</code></h1>
</header>
<section id="section-intro">
<p>Tools related to running experimentes remotely</p>
<p>Requires rsync, gcloud and ssh access to the DAPPER cluster.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Tools related to running experimentes remotely

Requires rsync, gcloud and ssh access to the DAPPER cluster.&#34;&#34;&#34;

# TODO 5: use Fabric? https://www.fabfile.org/

import time
from tqdm import tqdm
from dapper.dpr_config import rc
import dapper.tools.utils as utils
from datetime import timezone, timedelta
from dateutil.parser import parse as datetime_parse
import os
import tempfile
import subprocess


class SubmissionConnection:
    &#34;&#34;&#34;Establish multiplexed ssh to a given submit-node for a given xps_path.&#34;&#34;&#34;

    def __init__(self,
                 xps_path,
                 name=&#34;condor-submit&#34;,
                 zone=&#34;us-central1-f&#34;,
                 proj=&#34;mc-tut&#34;):
        # Job info
        self.xps_path = xps_path
        # Submit-node info
        self.name     = name
        self.proj     = proj
        self.zone     = zone
        self.host     = f&#34;{name}.{zone}.{proj}&#34;
        # instance name (as viewed by system ssh)
        self.ip       = get_ip(name)

        print(&#34;Preparing ssh connection&#34;)
        sys_cmd(&#34;gcloud compute config-ssh&#34;)
        # Use multiplexing to enable simultaneous connections.
        # Possible alternative: alter MaxStartups in sshd_config,
        # or other configurations:
        # - https://stackoverflow.com/a/36654900/38281
        # - https://unix.stackexchange.com/a/226460
        # - https://superuser.com/a/1032667/142925
        self.ssh_M = (
            &#39;&#39;&#39;ssh -o ControlMaster=auto&#39;&#39;&#39;
            &#39;&#39;&#39; -o ControlPath=~/.ssh/%r@%h:%p.socket -o ControlPersist=1m&#39;&#39;&#39;)

        # print_condor_status()
        print(&#34;autoscaler.py%s detected&#34; % (&#34;&#34; if detect_autoscaler(self) else &#34; NOT&#34;))

    def remote_cmd(self, cmd_string):
        &#34;&#34;&#34;Run command at self.host via multiplexed ssh.&#34;&#34;&#34;
        # Old version (uses gcloud):
        #     command = &#34;&#34;&#34;--command=&#34;&#34;&#34; + command
        #     connect = &#34;gcloud compute ssh condor-submit&#34;.split()
        #     output = sys_cmd(connect + [command], split=False)
        return sys_cmd([*self.ssh_M.split(), self.ip, cmd_string], split=False)

    def rsync(self, src, dst, opts=[], rev=False, prog=False, dry=False, use_M=True):
        # Prepare: opts
        if isinstance(opts, str):
            opts = opts.split()

        # Prepare: src, dst
        src = str(src)
        dst = str(dst)
        dst = self.ip + &#34;:&#34; + dst
        if rev:
            src, dst = dst, src

        # Show progress
        if prog:
            # TODO 3: Implement rsync check for when new rsync
            # isnt available which supports --info=progress2
            prog = (&#34;--info=progress2&#34;, &#34;--no-inc-recursive&#34;)
        else:
            prog = []

        # Use multiplex
        multiplex = []
        if use_M:
            multiplex = &#34;-e&#34;, self.ssh_M
        else:
            multiplex = []

        # Assemble command
        cmd = [&#34;rsync&#34;, &#34;-azh&#34;, *prog, *multiplex, *opts, src, dst]

        if dry:
            # Dry run
            return &#34; &#34;.join(cmd)
        else:
            # Sync
            _ = subprocess.run(cmd, check=True)
            return None


def submit_job_GCP(xps_path, **kwargs):
    &#34;&#34;&#34;GCP/HTCondor launcher&#34;&#34;&#34;
    sc = SubmissionConnection(xps_path, **kwargs)
    sync_job(sc)

    try:
        # Timeout functionality for ssh -c submit
        def submit():
            print(&#34;Submitting jobs&#34;)
            sc.remote_cmd(
                f&#34;&#34;&#34;cd {xps_path.name}; condor_submit&#34;&#34;&#34;
                f&#34;&#34;&#34; -batch-name {xps_path.name} submit-description&#34;&#34;&#34;)
        import multiprocessing_on_dill as mpd  # fails on GCP
        p = mpd.Process(target=submit)
        p.start()
        p.join(5*60)
        if p.is_alive():
            print(&#39;The ssh seems to hang, &#39;
                  &#39;but the jobs are probably submitted.&#39;)
            p.terminate()
            p.join()

        # Prepare download command
        print(&#34;To download results (before completion) use:&#34;)
        xcldd = [&#34;xp.com&#34;, &#34;DAPPER&#34;, &#34;runlog&#34;, &#34;err&#34;]  # &#34;out\\.*&#34;
        xcldd = [&#34;--exclude=&#34;+x for x in xcldd]
        print(sc.rsync(
            xps_path.parent, f&#34;~/{xps_path.name}&#34;,
            rev=True, opts=xcldd, dry=True, use_M=False))

        monitor_progress(sc)
    except Exception:
        inpt = input(&#34;Do you wish to clear the job queue? (Y/n): &#34;).lower()
        if inpt in [&#34;&#34;, &#34;y&#34;, &#34;yes&#34;]:
            print(&#34;Clearing queue&#34;)
            clear_queue(sc)
        raise
    else:
        print(&#34;Downloading results&#34;)
        sc.rsync(xps_path.parent, f&#34;~/{xps_path.name}&#34;, rev=True, opts=xcldd, prog=True)
    finally:
        # print(&#34;Checking for autoscaler cron job:&#34;) # let user know smth&#39;s happenin
        if not detect_autoscaler(sc):
            print(&#34;Warning: autoscaler.py NOT detected!\n    &#34;
                  &#34;Shut down the compute nodes yourself using:\n    &#34;
                  &#34;gcloud compute instance-groups managed &#34;
                  &#34;resize condor-compute-pvm-igm --size 0&#34;)


def detect_autoscaler(self, minutes=10):
    &#34;&#34;&#34;Grep syslog for autoscaler.

    Also get remote&#39;s date (time), to avoid another (slow) ssh.&#34;&#34;&#34;

    command = &#34;&#34;&#34;grep CRON /var/log/syslog | grep autoscaler | tail; date&#34;&#34;&#34;
    output  = self.remote_cmd(command).splitlines()
    recent_crons, now = output[:-1], output[-1]

    if not recent_crons:
        return False

    # Get timestamp of last cron job
    last_cron = recent_crons[-1].split(self.name)[0]
    log_time  = datetime_parse(last_cron).replace(tzinfo=timezone.utc)
    now       = datetime_parse(now)
    pause     = timedelta(minutes=minutes)

    if log_time + pause &lt; now:
        return False

    return True


def sync_job(self):
    xps_path = self.xps_path

    jobs = list_job_dirs(xps_path)
    print(&#34;Syncing %d jobs&#34; % len(jobs))

    # NB: --delete =&gt; Must precede other rsync&#39;s!
    self.rsync(xps_path, &#34;~/&#34;, &#34;--delete&#34;)

    htcondor = str(rc.dirs.DAPPER/&#34;dapper&#34;/&#34;tools&#34;/&#34;remote&#34;/&#34;htcondor&#34;) + os.sep
    self.rsync(htcondor, &#34;~/&#34;+xps_path.name)

    print(&#34;Copying xp.com to initdirs&#34;)
    self.remote_cmd(
        f&#34;&#34;&#34;cd {xps_path.name}; for ixp in [0-999999];&#34;&#34;&#34;
        f&#34;&#34;&#34; do cp xp.com $ixp/; done&#34;&#34;&#34;)

    sync_DAPPER(self)


def sync_DAPPER(self):
    &#34;&#34;&#34;Sync DAPPER (as it currently exists, not a specific version)

    to compute-nodes, which don&#39;t have external IP addresses.
    &#34;&#34;&#34;
    # Get list of files: whatever mentioned by .git
    repo  = f&#34;--git-dir={rc.dirs.DAPPER}/.git&#34;
    files = sys_cmd(f&#34;git {repo} ls-tree -r --name-only HEAD&#34;).split()

    def xcldd(f):
        return f.startswith(&#34;docs/&#34;) or f.endswith(&#34;.jpg&#34;) or f.endswith(&#34;.png&#34;)
    files = [f for f in files if not xcldd(f)]

    with tempfile.NamedTemporaryFile(&#34;w&#34;, delete=False) as synclist:
        print(&#34;\n&#34;.join(files), file=synclist)

    print(&#34;Syncing DAPPER&#34;)
    try:
        self.rsync(
            rc.dirs.DAPPER,
            f&#34;~/{self.xps_path.name}/DAPPER&#34;,
            &#34;--files-from=&#34;+synclist.name)
    except subprocess.SubprocessError as error:
        # Suggest common source of error in the message.
        msg = error.args[0] + \
            &#34;\nDid you mv/rm files (and not registering it with .git)?&#34;
        raise subprocess.SubprocessError(msg) from error


def print_condor_status(self):
    status = &#34;&#34;&#34;condor_status -total&#34;&#34;&#34;
    status = self.remote_cmd(status)
    if status:
        print(status, &#34;:&#34;)
        for line in status.splitlines()[::4]:
            print(line)
    else:
        print(&#34;[No compute nodes found]&#34;)


def clear_queue(self):
    &#34;&#34;&#34;Use condor_rm to clear the job queue of the submission.&#34;&#34;&#34;
    try:
        batch = f&#34;&#34;&#34;-constraint &#39;JobBatchName == &#34;{self.xps_path.name}&#34;&#39;&#34;&#34;&#34;
        self.remote_cmd(f&#34;&#34;&#34;condor_rm {batch}&#34;&#34;&#34;)
        print(&#34;Queue cleared.&#34;)
    except subprocess.SubprocessError as error:
        if &#34;matching&#34; in error.args[0]:
            # Queue probably already cleared, as happens upon
            # KeyboardInterrupt, when there&#39;s also &#34;held&#34; jobs.
            pass
        else:
            raise


def get_job_status(self):
    &#34;&#34;&#34;Parse condor_q to get number idle, held, etc, jobs&#34;&#34;&#34;
    # The autoscaler.py script from Google uses
    # &#39;condor_q -totals -format &#34;%d &#34; Jobs -format &#34;%d &#34; Idle -format &#34;%d &#34; Held&#39;
    # But in both condor versions I&#39;ve tried, -totals does not mix well with -format,
    # and the ClassAd attributes (&#34;Jobs&#34;, &#34;Idle&#34;, &#34;Held&#34;) are not available,
    # as listed by:
    #  - condor_q -l
    #  - Appendix &#34;Job ClassAd Attributes&#34; of the condor-manual (online).
    #  Condor version 8.6 (higher than 8.4 used by GCP tutorial)
    #  enables labelling jobs with -batch-name, and thus multiple jobs
    #  can be submitted and run (queried for progress, rm&#39;d, downloaded) simultaneously.
    #  One alternative is to query job status with
    #  condor_q -constraint &#39;JobStatus == 5&#39;,
    #  but I prefer to parse the -totals output instead.

    batch = f&#34;&#34;&#34;-constraint &#39;JobBatchName == &#34;{self.xps_path.name}&#34;&#39;&#34;&#34;&#34;
    qsum = self.remote_cmd(f&#34;&#34;&#34;condor_q {batch}&#34;&#34;&#34;).split()
    status = dict(jobs=&#34;jobs;&#34;, completed=&#34;completed,&#34;, removed=&#34;removed,&#34;,
                  idle=&#34;idle,&#34;, running=&#34;running,&#34;, held=&#34;held,&#34;, suspended=&#34;suspended&#34;)
    # Another way to get total num. of jobs:
    # int(self.remote_cmd(
    #     f&#34;&#34;&#34;cd {self.xps_path.name}; ls -1 | grep -o &#39;[0-9]*&#39; | wc -l&#34;&#34;&#34;))
    # Another way to parse qsum:
    # int(re.search(&#34;&#34;&#34;(\d+) idle&#34;&#34;&#34;,condor_q).group(1))
    return {k: int(qsum[qsum.index(v)-1]) for k, v in status.items()}


def monitor_progress(self):
    &#34;&#34;&#34;Use condor_q to monitor job progress.&#34;&#34;&#34;
    num_jobs = len(list_job_dirs(self.xps_path))
    pbar = tqdm(total=num_jobs, desc=&#34;Processing jobs&#34;)
    try:
        unfinished = num_jobs
        while unfinished:
            job_status     = get_job_status(self)
            unlisted       = num_jobs - job_status[&#34;jobs&#34;]  # completed w/ success
            finished       = job_status[&#34;held&#34;] + unlisted  # failed + suceeded
            unfinished_new = num_jobs - finished
            increment      = unfinished - unfinished_new
            unfinished     = unfinished_new
            # print(job_status)
            pbar.update(increment)
            time.sleep(1)  # dont clog the ssh connection
    except Exception:
        print(&#34;Some kind of exception occured,&#34;
              &#34; while %d jobs were not even run.&#34; % unfinished)
        raise
    else:
        print(&#34;All jobs finished without failure.&#34;)
    finally:
        pbar.close()
        if job_status[&#34;held&#34;]:
            print(&#34;NB: There were %d failed jobs&#34; % job_status[&#34;held&#34;])
            print(f&#34;View errors at {self.xps_path}/JOBNUMBER/out&#34;)
            clear_queue(self) # TODO 3: this runs also if sucessfull. Ok?


def list_job_dirs(xps_path):
    dirs = [xps_path/d for d in utils.sorted_human(os.listdir(xps_path))]
    return [d for d in dirs if d.is_dir() and d.stem.isnumeric()]


def get_ip(instance):
    &#34;&#34;&#34;Get ip-address of instance.

    NB: the use of IP rather than the ``Host`` listed in ``.ssh/config``
    (eg ``condor-submit.us-central1-f.mc-tut``,
    as generated by ``gcloud compute config-ssh``)
    requires ``AddKeysToAgent yes`` under ``Host *`` in ``.ssh/config``,
    and that you&#39;ve already logged into the instance once using (eg)
    ``ssh condor-submit.us-central1-f.mc-tut``.
    &#34;&#34;&#34;

    # cloud.google.com/compute/docs/instances/view-ip-address
    getip = &#39;get(networkInterfaces[0].accessConfigs[0].natIP)&#39;
    ip = sys_cmd(f&#34;gcloud compute instances describe {instance} --format={getip}&#34;)
    return ip.strip()

    # # Parse ssh/config for the &#34;Host&#34; of condor-submit.
    # # Q: how reliable/portable is it?
    # from pathlib import Path
    # with open(Path(&#34;~&#34;).expanduser()/&#34;.ssh&#34;/&#34;config&#34;) as ssh_config:
    #     for ln in ssh_config:
    #         if ln.startswith(&#34;Host condor-submit&#34;):
    #             break
    #     else:
    #         raise RuntimeError(
    #             &#34;Did not find condor-submit Host in .ssh/config.&#34;)
    #     return ln[ln.index(&#34;condor&#34;):].strip()


def sys_cmd(args, split=True):
    &#34;&#34;&#34;Run subprocess, capture output, raise exception.&#34;&#34;&#34;
    if split:
        args = args.split()
    try:
        ps = subprocess.run(args, check=True, capture_output=True)
    except subprocess.CalledProcessError as error:
        # CalledProcessError doesnt print its .stderr, so we raise it this way:
        raise subprocess.SubprocessError(
            f&#34;Command {error.cmd} returned non-zero exit status, &#34;
            f&#34;with stderr:\n{error.stderr.decode()}&#34;) from error
    output = ps.stdout.decode()
    return output</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dapper.tools.remote.uplink.submit_job_GCP"><code class="name flex">
<span>def <span class="ident">submit_job_GCP</span></span>(<span>xps_path, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>GCP/HTCondor launcher</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def submit_job_GCP(xps_path, **kwargs):
    &#34;&#34;&#34;GCP/HTCondor launcher&#34;&#34;&#34;
    sc = SubmissionConnection(xps_path, **kwargs)
    sync_job(sc)

    try:
        # Timeout functionality for ssh -c submit
        def submit():
            print(&#34;Submitting jobs&#34;)
            sc.remote_cmd(
                f&#34;&#34;&#34;cd {xps_path.name}; condor_submit&#34;&#34;&#34;
                f&#34;&#34;&#34; -batch-name {xps_path.name} submit-description&#34;&#34;&#34;)
        import multiprocessing_on_dill as mpd  # fails on GCP
        p = mpd.Process(target=submit)
        p.start()
        p.join(5*60)
        if p.is_alive():
            print(&#39;The ssh seems to hang, &#39;
                  &#39;but the jobs are probably submitted.&#39;)
            p.terminate()
            p.join()

        # Prepare download command
        print(&#34;To download results (before completion) use:&#34;)
        xcldd = [&#34;xp.com&#34;, &#34;DAPPER&#34;, &#34;runlog&#34;, &#34;err&#34;]  # &#34;out\\.*&#34;
        xcldd = [&#34;--exclude=&#34;+x for x in xcldd]
        print(sc.rsync(
            xps_path.parent, f&#34;~/{xps_path.name}&#34;,
            rev=True, opts=xcldd, dry=True, use_M=False))

        monitor_progress(sc)
    except Exception:
        inpt = input(&#34;Do you wish to clear the job queue? (Y/n): &#34;).lower()
        if inpt in [&#34;&#34;, &#34;y&#34;, &#34;yes&#34;]:
            print(&#34;Clearing queue&#34;)
            clear_queue(sc)
        raise
    else:
        print(&#34;Downloading results&#34;)
        sc.rsync(xps_path.parent, f&#34;~/{xps_path.name}&#34;, rev=True, opts=xcldd, prog=True)
    finally:
        # print(&#34;Checking for autoscaler cron job:&#34;) # let user know smth&#39;s happenin
        if not detect_autoscaler(sc):
            print(&#34;Warning: autoscaler.py NOT detected!\n    &#34;
                  &#34;Shut down the compute nodes yourself using:\n    &#34;
                  &#34;gcloud compute instance-groups managed &#34;
                  &#34;resize condor-compute-pvm-igm --size 0&#34;)</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.detect_autoscaler"><code class="name flex">
<span>def <span class="ident">detect_autoscaler</span></span>(<span>self, minutes=10)</span>
</code></dt>
<dd>
<div class="desc"><p>Grep syslog for autoscaler.</p>
<p>Also get remote's date (time), to avoid another (slow) ssh.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detect_autoscaler(self, minutes=10):
    &#34;&#34;&#34;Grep syslog for autoscaler.

    Also get remote&#39;s date (time), to avoid another (slow) ssh.&#34;&#34;&#34;

    command = &#34;&#34;&#34;grep CRON /var/log/syslog | grep autoscaler | tail; date&#34;&#34;&#34;
    output  = self.remote_cmd(command).splitlines()
    recent_crons, now = output[:-1], output[-1]

    if not recent_crons:
        return False

    # Get timestamp of last cron job
    last_cron = recent_crons[-1].split(self.name)[0]
    log_time  = datetime_parse(last_cron).replace(tzinfo=timezone.utc)
    now       = datetime_parse(now)
    pause     = timedelta(minutes=minutes)

    if log_time + pause &lt; now:
        return False

    return True</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.sync_job"><code class="name flex">
<span>def <span class="ident">sync_job</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sync_job(self):
    xps_path = self.xps_path

    jobs = list_job_dirs(xps_path)
    print(&#34;Syncing %d jobs&#34; % len(jobs))

    # NB: --delete =&gt; Must precede other rsync&#39;s!
    self.rsync(xps_path, &#34;~/&#34;, &#34;--delete&#34;)

    htcondor = str(rc.dirs.DAPPER/&#34;dapper&#34;/&#34;tools&#34;/&#34;remote&#34;/&#34;htcondor&#34;) + os.sep
    self.rsync(htcondor, &#34;~/&#34;+xps_path.name)

    print(&#34;Copying xp.com to initdirs&#34;)
    self.remote_cmd(
        f&#34;&#34;&#34;cd {xps_path.name}; for ixp in [0-999999];&#34;&#34;&#34;
        f&#34;&#34;&#34; do cp xp.com $ixp/; done&#34;&#34;&#34;)

    sync_DAPPER(self)</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.sync_DAPPER"><code class="name flex">
<span>def <span class="ident">sync_DAPPER</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Sync DAPPER (as it currently exists, not a specific version)</p>
<p>to compute-nodes, which don't have external IP addresses.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sync_DAPPER(self):
    &#34;&#34;&#34;Sync DAPPER (as it currently exists, not a specific version)

    to compute-nodes, which don&#39;t have external IP addresses.
    &#34;&#34;&#34;
    # Get list of files: whatever mentioned by .git
    repo  = f&#34;--git-dir={rc.dirs.DAPPER}/.git&#34;
    files = sys_cmd(f&#34;git {repo} ls-tree -r --name-only HEAD&#34;).split()

    def xcldd(f):
        return f.startswith(&#34;docs/&#34;) or f.endswith(&#34;.jpg&#34;) or f.endswith(&#34;.png&#34;)
    files = [f for f in files if not xcldd(f)]

    with tempfile.NamedTemporaryFile(&#34;w&#34;, delete=False) as synclist:
        print(&#34;\n&#34;.join(files), file=synclist)

    print(&#34;Syncing DAPPER&#34;)
    try:
        self.rsync(
            rc.dirs.DAPPER,
            f&#34;~/{self.xps_path.name}/DAPPER&#34;,
            &#34;--files-from=&#34;+synclist.name)
    except subprocess.SubprocessError as error:
        # Suggest common source of error in the message.
        msg = error.args[0] + \
            &#34;\nDid you mv/rm files (and not registering it with .git)?&#34;
        raise subprocess.SubprocessError(msg) from error</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.print_condor_status"><code class="name flex">
<span>def <span class="ident">print_condor_status</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_condor_status(self):
    status = &#34;&#34;&#34;condor_status -total&#34;&#34;&#34;
    status = self.remote_cmd(status)
    if status:
        print(status, &#34;:&#34;)
        for line in status.splitlines()[::4]:
            print(line)
    else:
        print(&#34;[No compute nodes found]&#34;)</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.clear_queue"><code class="name flex">
<span>def <span class="ident">clear_queue</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Use condor_rm to clear the job queue of the submission.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear_queue(self):
    &#34;&#34;&#34;Use condor_rm to clear the job queue of the submission.&#34;&#34;&#34;
    try:
        batch = f&#34;&#34;&#34;-constraint &#39;JobBatchName == &#34;{self.xps_path.name}&#34;&#39;&#34;&#34;&#34;
        self.remote_cmd(f&#34;&#34;&#34;condor_rm {batch}&#34;&#34;&#34;)
        print(&#34;Queue cleared.&#34;)
    except subprocess.SubprocessError as error:
        if &#34;matching&#34; in error.args[0]:
            # Queue probably already cleared, as happens upon
            # KeyboardInterrupt, when there&#39;s also &#34;held&#34; jobs.
            pass
        else:
            raise</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.get_job_status"><code class="name flex">
<span>def <span class="ident">get_job_status</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Parse condor_q to get number idle, held, etc, jobs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_job_status(self):
    &#34;&#34;&#34;Parse condor_q to get number idle, held, etc, jobs&#34;&#34;&#34;
    # The autoscaler.py script from Google uses
    # &#39;condor_q -totals -format &#34;%d &#34; Jobs -format &#34;%d &#34; Idle -format &#34;%d &#34; Held&#39;
    # But in both condor versions I&#39;ve tried, -totals does not mix well with -format,
    # and the ClassAd attributes (&#34;Jobs&#34;, &#34;Idle&#34;, &#34;Held&#34;) are not available,
    # as listed by:
    #  - condor_q -l
    #  - Appendix &#34;Job ClassAd Attributes&#34; of the condor-manual (online).
    #  Condor version 8.6 (higher than 8.4 used by GCP tutorial)
    #  enables labelling jobs with -batch-name, and thus multiple jobs
    #  can be submitted and run (queried for progress, rm&#39;d, downloaded) simultaneously.
    #  One alternative is to query job status with
    #  condor_q -constraint &#39;JobStatus == 5&#39;,
    #  but I prefer to parse the -totals output instead.

    batch = f&#34;&#34;&#34;-constraint &#39;JobBatchName == &#34;{self.xps_path.name}&#34;&#39;&#34;&#34;&#34;
    qsum = self.remote_cmd(f&#34;&#34;&#34;condor_q {batch}&#34;&#34;&#34;).split()
    status = dict(jobs=&#34;jobs;&#34;, completed=&#34;completed,&#34;, removed=&#34;removed,&#34;,
                  idle=&#34;idle,&#34;, running=&#34;running,&#34;, held=&#34;held,&#34;, suspended=&#34;suspended&#34;)
    # Another way to get total num. of jobs:
    # int(self.remote_cmd(
    #     f&#34;&#34;&#34;cd {self.xps_path.name}; ls -1 | grep -o &#39;[0-9]*&#39; | wc -l&#34;&#34;&#34;))
    # Another way to parse qsum:
    # int(re.search(&#34;&#34;&#34;(\d+) idle&#34;&#34;&#34;,condor_q).group(1))
    return {k: int(qsum[qsum.index(v)-1]) for k, v in status.items()}</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.monitor_progress"><code class="name flex">
<span>def <span class="ident">monitor_progress</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Use condor_q to monitor job progress.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def monitor_progress(self):
    &#34;&#34;&#34;Use condor_q to monitor job progress.&#34;&#34;&#34;
    num_jobs = len(list_job_dirs(self.xps_path))
    pbar = tqdm(total=num_jobs, desc=&#34;Processing jobs&#34;)
    try:
        unfinished = num_jobs
        while unfinished:
            job_status     = get_job_status(self)
            unlisted       = num_jobs - job_status[&#34;jobs&#34;]  # completed w/ success
            finished       = job_status[&#34;held&#34;] + unlisted  # failed + suceeded
            unfinished_new = num_jobs - finished
            increment      = unfinished - unfinished_new
            unfinished     = unfinished_new
            # print(job_status)
            pbar.update(increment)
            time.sleep(1)  # dont clog the ssh connection
    except Exception:
        print(&#34;Some kind of exception occured,&#34;
              &#34; while %d jobs were not even run.&#34; % unfinished)
        raise
    else:
        print(&#34;All jobs finished without failure.&#34;)
    finally:
        pbar.close()
        if job_status[&#34;held&#34;]:
            print(&#34;NB: There were %d failed jobs&#34; % job_status[&#34;held&#34;])
            print(f&#34;View errors at {self.xps_path}/JOBNUMBER/out&#34;)
            clear_queue(self) # TODO 3: this runs also if sucessfull. Ok?</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.list_job_dirs"><code class="name flex">
<span>def <span class="ident">list_job_dirs</span></span>(<span>xps_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_job_dirs(xps_path):
    dirs = [xps_path/d for d in utils.sorted_human(os.listdir(xps_path))]
    return [d for d in dirs if d.is_dir() and d.stem.isnumeric()]</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.get_ip"><code class="name flex">
<span>def <span class="ident">get_ip</span></span>(<span>instance)</span>
</code></dt>
<dd>
<div class="desc"><p>Get ip-address of instance.</p>
<p>NB: the use of IP rather than the <code>Host</code> listed in <code>.ssh/config</code>
(eg <code>condor-submit.us-central1-f.mc-tut</code>,
as generated by <code>gcloud compute config-ssh</code>)
requires <code>AddKeysToAgent yes</code> under <code>Host *</code> in <code>.ssh/config</code>,
and that you've already logged into the instance once using (eg)
<code>ssh condor-submit.us-central1-f.mc-tut</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_ip(instance):
    &#34;&#34;&#34;Get ip-address of instance.

    NB: the use of IP rather than the ``Host`` listed in ``.ssh/config``
    (eg ``condor-submit.us-central1-f.mc-tut``,
    as generated by ``gcloud compute config-ssh``)
    requires ``AddKeysToAgent yes`` under ``Host *`` in ``.ssh/config``,
    and that you&#39;ve already logged into the instance once using (eg)
    ``ssh condor-submit.us-central1-f.mc-tut``.
    &#34;&#34;&#34;

    # cloud.google.com/compute/docs/instances/view-ip-address
    getip = &#39;get(networkInterfaces[0].accessConfigs[0].natIP)&#39;
    ip = sys_cmd(f&#34;gcloud compute instances describe {instance} --format={getip}&#34;)
    return ip.strip()</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.sys_cmd"><code class="name flex">
<span>def <span class="ident">sys_cmd</span></span>(<span>args, split=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Run subprocess, capture output, raise exception.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sys_cmd(args, split=True):
    &#34;&#34;&#34;Run subprocess, capture output, raise exception.&#34;&#34;&#34;
    if split:
        args = args.split()
    try:
        ps = subprocess.run(args, check=True, capture_output=True)
    except subprocess.CalledProcessError as error:
        # CalledProcessError doesnt print its .stderr, so we raise it this way:
        raise subprocess.SubprocessError(
            f&#34;Command {error.cmd} returned non-zero exit status, &#34;
            f&#34;with stderr:\n{error.stderr.decode()}&#34;) from error
    output = ps.stdout.decode()
    return output</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="dapper.tools.remote.uplink.SubmissionConnection"><code class="flex name class">
<span>class <span class="ident">SubmissionConnection</span></span>
<span>(</span><span>xps_path, name='condor-submit', zone='us-central1-f', proj='mc-tut')</span>
</code></dt>
<dd>
<div class="desc"><p>Establish multiplexed ssh to a given submit-node for a given xps_path.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SubmissionConnection:
    &#34;&#34;&#34;Establish multiplexed ssh to a given submit-node for a given xps_path.&#34;&#34;&#34;

    def __init__(self,
                 xps_path,
                 name=&#34;condor-submit&#34;,
                 zone=&#34;us-central1-f&#34;,
                 proj=&#34;mc-tut&#34;):
        # Job info
        self.xps_path = xps_path
        # Submit-node info
        self.name     = name
        self.proj     = proj
        self.zone     = zone
        self.host     = f&#34;{name}.{zone}.{proj}&#34;
        # instance name (as viewed by system ssh)
        self.ip       = get_ip(name)

        print(&#34;Preparing ssh connection&#34;)
        sys_cmd(&#34;gcloud compute config-ssh&#34;)
        # Use multiplexing to enable simultaneous connections.
        # Possible alternative: alter MaxStartups in sshd_config,
        # or other configurations:
        # - https://stackoverflow.com/a/36654900/38281
        # - https://unix.stackexchange.com/a/226460
        # - https://superuser.com/a/1032667/142925
        self.ssh_M = (
            &#39;&#39;&#39;ssh -o ControlMaster=auto&#39;&#39;&#39;
            &#39;&#39;&#39; -o ControlPath=~/.ssh/%r@%h:%p.socket -o ControlPersist=1m&#39;&#39;&#39;)

        # print_condor_status()
        print(&#34;autoscaler.py%s detected&#34; % (&#34;&#34; if detect_autoscaler(self) else &#34; NOT&#34;))

    def remote_cmd(self, cmd_string):
        &#34;&#34;&#34;Run command at self.host via multiplexed ssh.&#34;&#34;&#34;
        # Old version (uses gcloud):
        #     command = &#34;&#34;&#34;--command=&#34;&#34;&#34; + command
        #     connect = &#34;gcloud compute ssh condor-submit&#34;.split()
        #     output = sys_cmd(connect + [command], split=False)
        return sys_cmd([*self.ssh_M.split(), self.ip, cmd_string], split=False)

    def rsync(self, src, dst, opts=[], rev=False, prog=False, dry=False, use_M=True):
        # Prepare: opts
        if isinstance(opts, str):
            opts = opts.split()

        # Prepare: src, dst
        src = str(src)
        dst = str(dst)
        dst = self.ip + &#34;:&#34; + dst
        if rev:
            src, dst = dst, src

        # Show progress
        if prog:
            # TODO 3: Implement rsync check for when new rsync
            # isnt available which supports --info=progress2
            prog = (&#34;--info=progress2&#34;, &#34;--no-inc-recursive&#34;)
        else:
            prog = []

        # Use multiplex
        multiplex = []
        if use_M:
            multiplex = &#34;-e&#34;, self.ssh_M
        else:
            multiplex = []

        # Assemble command
        cmd = [&#34;rsync&#34;, &#34;-azh&#34;, *prog, *multiplex, *opts, src, dst]

        if dry:
            # Dry run
            return &#34; &#34;.join(cmd)
        else:
            # Sync
            _ = subprocess.run(cmd, check=True)
            return None</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="dapper.tools.remote.uplink.SubmissionConnection.remote_cmd"><code class="name flex">
<span>def <span class="ident">remote_cmd</span></span>(<span>self, cmd_string)</span>
</code></dt>
<dd>
<div class="desc"><p>Run command at self.host via multiplexed ssh.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remote_cmd(self, cmd_string):
    &#34;&#34;&#34;Run command at self.host via multiplexed ssh.&#34;&#34;&#34;
    # Old version (uses gcloud):
    #     command = &#34;&#34;&#34;--command=&#34;&#34;&#34; + command
    #     connect = &#34;gcloud compute ssh condor-submit&#34;.split()
    #     output = sys_cmd(connect + [command], split=False)
    return sys_cmd([*self.ssh_M.split(), self.ip, cmd_string], split=False)</code></pre>
</details>
</dd>
<dt id="dapper.tools.remote.uplink.SubmissionConnection.rsync"><code class="name flex">
<span>def <span class="ident">rsync</span></span>(<span>self, src, dst, opts=[], rev=False, prog=False, dry=False, use_M=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rsync(self, src, dst, opts=[], rev=False, prog=False, dry=False, use_M=True):
    # Prepare: opts
    if isinstance(opts, str):
        opts = opts.split()

    # Prepare: src, dst
    src = str(src)
    dst = str(dst)
    dst = self.ip + &#34;:&#34; + dst
    if rev:
        src, dst = dst, src

    # Show progress
    if prog:
        # TODO 3: Implement rsync check for when new rsync
        # isnt available which supports --info=progress2
        prog = (&#34;--info=progress2&#34;, &#34;--no-inc-recursive&#34;)
    else:
        prog = []

    # Use multiplex
    multiplex = []
    if use_M:
        multiplex = &#34;-e&#34;, self.ssh_M
    else:
        multiplex = []

    # Assemble command
    cmd = [&#34;rsync&#34;, &#34;-azh&#34;, *prog, *multiplex, *opts, src, dst]

    if dry:
        # Dry run
        return &#34; &#34;.join(cmd)
    else:
        # Sync
        _ = subprocess.run(cmd, check=True)
        return None</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="DAPPER" href="https://nansencenter.github.io/DAPPER">
<img src="https://nansencenter.github.io/DAPPER/logo.png" style="width:200px;" alt="">
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dapper.tools.remote" href="index.html">dapper.tools.remote</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="dapper.tools.remote.uplink.submit_job_GCP" href="#dapper.tools.remote.uplink.submit_job_GCP">submit_job_GCP</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.detect_autoscaler" href="#dapper.tools.remote.uplink.detect_autoscaler">detect_autoscaler</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.sync_job" href="#dapper.tools.remote.uplink.sync_job">sync_job</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.sync_DAPPER" href="#dapper.tools.remote.uplink.sync_DAPPER">sync_DAPPER</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.print_condor_status" href="#dapper.tools.remote.uplink.print_condor_status">print_condor_status</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.clear_queue" href="#dapper.tools.remote.uplink.clear_queue">clear_queue</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.get_job_status" href="#dapper.tools.remote.uplink.get_job_status">get_job_status</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.monitor_progress" href="#dapper.tools.remote.uplink.monitor_progress">monitor_progress</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.list_job_dirs" href="#dapper.tools.remote.uplink.list_job_dirs">list_job_dirs</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.get_ip" href="#dapper.tools.remote.uplink.get_ip">get_ip</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.sys_cmd" href="#dapper.tools.remote.uplink.sys_cmd">sys_cmd</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="dapper.tools.remote.uplink.SubmissionConnection" href="#dapper.tools.remote.uplink.SubmissionConnection">SubmissionConnection</a></code></h4>
<ul class="">
<li><code><a title="dapper.tools.remote.uplink.SubmissionConnection.remote_cmd" href="#dapper.tools.remote.uplink.SubmissionConnection.remote_cmd">remote_cmd</a></code></li>
<li><code><a title="dapper.tools.remote.uplink.SubmissionConnection.rsync" href="#dapper.tools.remote.uplink.SubmissionConnection.rsync">rsync</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>