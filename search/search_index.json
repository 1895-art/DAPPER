{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods. The tests provide experimental support and guidance for new developments in DA. Example diagnostics: The typical set-up is a synthetic (twin) experiment , where you specify a dynamic model * observational model * use these to generate a synthetic \"truth\" and observations thereof * assess how different DA methods perform in estimating the truth, given the above starred ( * ) items. Highlights DAPPER enables the numerical investigation of DA methods through a variety of typical test cases and statistics. It (a) reproduces numerical benchmarks results reported in the literature, and (b) facilitates comparative studies, thus promoting the (a) reliability and (b) relevance of the results. For example, this figure is generated by example_3.py and is a reproduction from this book on DA . DAPPER is (c) open source, written in Python, and (d) focuses on readability; this promotes the (c) reproduction and (d) dissemination of the underlying science, and makes it easy to adapt and extend. In summary, it is well suited for teaching and fundamental DA research. Also see its drawbacks . Installation Works on Linux/Windows/Mac. Prerequisite : Python>=3.8. If you're not an admin or expert: 1a. Install Python with Anaconda . 1b. Use the Anaconda terminal to run the commands below. Install : Download and extract (or git clone ) DAPPER, cd into the resulting folder (ensure you're in the folder with a setup.py file) : pip install -e . (don't forget the . ). If you want multiprocessing options, install with: pip install -e .[MP] Test by running: python example_1.py Step 2 can be replaced by running pip install da-dapper but this is not recommended since this hides away DAPPER as a library in your python path. If the installation fails, you probably need to create a new Python environment . Getting started Read, run, and understand the scripts example_{1,2,3}.py . Then, get familiar with the code. The documentation provide rendered docstrings, but are far from complete. Alternatively, see DA-tutorials for an intro to DA. Methods Method Literature reproduced EnKF 1 Sak08, Hot15 EnKF-N Boc12, Boc15 EnKS, EnRTS Raa16b iEnKS / iEnKF / EnRML / ES-MDA 2 Sak12, Boc12, Boc14 LETKF, local & serial EAKF Boc11 Sqrt. model noise methods Raa15 Particle filter (bootstrap) 3 Boc10 Optimal/implicit Particle filter 3 Boc10 NETF To\u0308d15, Wil16 Rank histogram filter (RHF) And10 4D-Var 3D-Var Extended KF Optimal interpolation Climatology References 1 : Stochastic, DEnKF (i.e. half-update), ETKF (i.e. sym. sqrt.). Serial forms are also available. Tuned with inflation and \"random, orthogonal rotations\". 2 : Also supports the bundle version, and \"EnKF-N\"-type inflation. 3 : Resampling: multinomial (including systematic/universal and residual). The particle filter is tuned with \"effective-N monitoring\", \"regularization/jittering\" strength, and more. For a list of ready-made experiments with suitable, tuned settings for a given method (e.g. the iEnKS ), use gnu's grep: $ cd dapper/mods $ grep -r \"iEnKS.*(\" Models Model Lin? TLM? PDE? Phys.dim. State len Lyap\u22650 Implementer Linear Advect. (LA) Yes Yes Yes 1d 1000 * 51 Evensen/Raanes DoublePendulum No Yes No 0d 4 2 Matplotlib/Raanes Ikeda No Yes No 0d 2 1 Raanes LotkaVolterra No Yes No 0d 5 * 1 Wikipedia/Raanes Lorenz63 No Yes \"Yes\" 0d 3 2 Sakov Lorenz84 No Yes No 0d 3 2 Raanes Lorenz96 No Yes No 1d 40 * 13 Raanes LorenzUV No Yes No 2x 1d 256 + 8 * \u224860 Raanes Kuramoto-Sivashinsky No Yes Yes 1d 128 * 11 Kassam/Raanes Quasi-Geost (QG) No No Yes 2d 129\u00b2\u224817k \u2248140 Sakov *: flexible; set as necessary The models are found as subdirectories within dapper/mods . A model should be defined in a file named core.py , and illustrated by a file named demo.py . Ideally, both of these files do not rely on the rest of DAPPER. More info . Most of the other files within a model subdirectory are usually named authorYEAR.py and define a HMM object, which holds the settings of a specific twin experiment, using that model, as detailed in the corresponding author/year's paper. At the bottom of each such file should be (in comments) a list of suitable, tuned settings for various DA methods, along with their expected, average rmse.a score for that experiment. The complete list of included experiment files can be obtained with gnu's find : $ cd dapper/mods $ find . -iname \"[a-z]*20[0-9].py\" Some of these files contain settings that have been used in several papers. As mentioned above , DAPPER reproduces literature results. There are also results in the literature that DAPPER does not reproduce. Typically, this means that the published results are incorrect. Alternative projects DAPPER is aimed at research and teaching (see discussion up top). Example of limitations: It is not suited for very big models (>60k unknowns). Time-dependent error covariances and changes in lengths of state/obs (although the Dyn and Obs models may otherwise be time-dependent). Non-uniform time sequences not fully supported. Also, DAPPER comes with no guarantees/support. Therefore, if you have an operational (real-world) application, such as WRF, you should look into one of the alternatives, sorted by approximate project size. Name Developers Purpose (approximately) DART NCAR Operational, general PDAF AWI Operational, general JEDI JCSDA (NOAA, NASA, ++) Operational, general (in develpmt?) ERT Statoil Operational, history matching (Petroleum) OpenDA TU Delft Operational, general Verdandi INRIA Biophysical DA PyOSSE Edinburgh, Reading Earth-observation DA SANGOMA Conglomerate* Unify DA research EMPIRE Reading (Met) Research (high-dim) MIKE DHI Oceanographic. Commercial? OAK Li\u00e8ge Oceaonagraphic Siroco OMP Oceaonagraphic FilterPy R. Labbe Engineering, general intro to Kalman filter DASoftware Yue Li, Stanford Matlab, large-scale Pomp U of Michigan R, general state-estimation PyIT CIPR Real-world petroleum DA (?) Datum Raanes Matlab, personal publications EnKF-Matlab Sakov Matlab, personal publications and intro EnKF-C Sakov C, light-weight EnKF, off-line IEnKS code Bocquet Python, personal publications pyda Hickman Python, personal publications The EnKF-Matlab and IEnKS codes have been inspirational in the development of DAPPER. *: AWI/Liege/CNRS/NERSC/Reading/Delft Contributors Patrick N. Raanes, Colin Grudzien, Maxime Tondeur, Remy Dubois If you use this software in a publication, please cite as follows. @misc { raanes2018dapper , author = {Patrick N. Raanes and others} , title = {nansencenter/DAPPER: Version 0.8} , month = December , year = 2018 , doi = {10.5281/zenodo.2029296} , url = {https://doi.org/10.5281/zenodo.2029296} } Publication list https://www.geosci-model-dev-discuss.net/gmd-2019-136/ https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3386 https://www.nonlin-processes-geophys-discuss.net/npg-2019-10 Powered by","title":"Home"},{"location":"#highlights","text":"DAPPER enables the numerical investigation of DA methods through a variety of typical test cases and statistics. It (a) reproduces numerical benchmarks results reported in the literature, and (b) facilitates comparative studies, thus promoting the (a) reliability and (b) relevance of the results. For example, this figure is generated by example_3.py and is a reproduction from this book on DA . DAPPER is (c) open source, written in Python, and (d) focuses on readability; this promotes the (c) reproduction and (d) dissemination of the underlying science, and makes it easy to adapt and extend. In summary, it is well suited for teaching and fundamental DA research. Also see its drawbacks .","title":"Highlights"},{"location":"#installation","text":"Works on Linux/Windows/Mac. Prerequisite : Python>=3.8. If you're not an admin or expert: 1a. Install Python with Anaconda . 1b. Use the Anaconda terminal to run the commands below. Install : Download and extract (or git clone ) DAPPER, cd into the resulting folder (ensure you're in the folder with a setup.py file) : pip install -e . (don't forget the . ). If you want multiprocessing options, install with: pip install -e .[MP] Test by running: python example_1.py Step 2 can be replaced by running pip install da-dapper but this is not recommended since this hides away DAPPER as a library in your python path. If the installation fails, you probably need to create a new Python environment .","title":"Installation"},{"location":"#getting-started","text":"Read, run, and understand the scripts example_{1,2,3}.py . Then, get familiar with the code. The documentation provide rendered docstrings, but are far from complete. Alternatively, see DA-tutorials for an intro to DA.","title":"Getting started"},{"location":"#methods","text":"Method Literature reproduced EnKF 1 Sak08, Hot15 EnKF-N Boc12, Boc15 EnKS, EnRTS Raa16b iEnKS / iEnKF / EnRML / ES-MDA 2 Sak12, Boc12, Boc14 LETKF, local & serial EAKF Boc11 Sqrt. model noise methods Raa15 Particle filter (bootstrap) 3 Boc10 Optimal/implicit Particle filter 3 Boc10 NETF To\u0308d15, Wil16 Rank histogram filter (RHF) And10 4D-Var 3D-Var Extended KF Optimal interpolation Climatology References 1 : Stochastic, DEnKF (i.e. half-update), ETKF (i.e. sym. sqrt.). Serial forms are also available. Tuned with inflation and \"random, orthogonal rotations\". 2 : Also supports the bundle version, and \"EnKF-N\"-type inflation. 3 : Resampling: multinomial (including systematic/universal and residual). The particle filter is tuned with \"effective-N monitoring\", \"regularization/jittering\" strength, and more. For a list of ready-made experiments with suitable, tuned settings for a given method (e.g. the iEnKS ), use gnu's grep: $ cd dapper/mods $ grep -r \"iEnKS.*(\"","title":"Methods"},{"location":"#models","text":"Model Lin? TLM? PDE? Phys.dim. State len Lyap\u22650 Implementer Linear Advect. (LA) Yes Yes Yes 1d 1000 * 51 Evensen/Raanes DoublePendulum No Yes No 0d 4 2 Matplotlib/Raanes Ikeda No Yes No 0d 2 1 Raanes LotkaVolterra No Yes No 0d 5 * 1 Wikipedia/Raanes Lorenz63 No Yes \"Yes\" 0d 3 2 Sakov Lorenz84 No Yes No 0d 3 2 Raanes Lorenz96 No Yes No 1d 40 * 13 Raanes LorenzUV No Yes No 2x 1d 256 + 8 * \u224860 Raanes Kuramoto-Sivashinsky No Yes Yes 1d 128 * 11 Kassam/Raanes Quasi-Geost (QG) No No Yes 2d 129\u00b2\u224817k \u2248140 Sakov *: flexible; set as necessary The models are found as subdirectories within dapper/mods . A model should be defined in a file named core.py , and illustrated by a file named demo.py . Ideally, both of these files do not rely on the rest of DAPPER. More info . Most of the other files within a model subdirectory are usually named authorYEAR.py and define a HMM object, which holds the settings of a specific twin experiment, using that model, as detailed in the corresponding author/year's paper. At the bottom of each such file should be (in comments) a list of suitable, tuned settings for various DA methods, along with their expected, average rmse.a score for that experiment. The complete list of included experiment files can be obtained with gnu's find : $ cd dapper/mods $ find . -iname \"[a-z]*20[0-9].py\" Some of these files contain settings that have been used in several papers. As mentioned above , DAPPER reproduces literature results. There are also results in the literature that DAPPER does not reproduce. Typically, this means that the published results are incorrect.","title":"Models"},{"location":"#alternative-projects","text":"DAPPER is aimed at research and teaching (see discussion up top). Example of limitations: It is not suited for very big models (>60k unknowns). Time-dependent error covariances and changes in lengths of state/obs (although the Dyn and Obs models may otherwise be time-dependent). Non-uniform time sequences not fully supported. Also, DAPPER comes with no guarantees/support. Therefore, if you have an operational (real-world) application, such as WRF, you should look into one of the alternatives, sorted by approximate project size. Name Developers Purpose (approximately) DART NCAR Operational, general PDAF AWI Operational, general JEDI JCSDA (NOAA, NASA, ++) Operational, general (in develpmt?) ERT Statoil Operational, history matching (Petroleum) OpenDA TU Delft Operational, general Verdandi INRIA Biophysical DA PyOSSE Edinburgh, Reading Earth-observation DA SANGOMA Conglomerate* Unify DA research EMPIRE Reading (Met) Research (high-dim) MIKE DHI Oceanographic. Commercial? OAK Li\u00e8ge Oceaonagraphic Siroco OMP Oceaonagraphic FilterPy R. Labbe Engineering, general intro to Kalman filter DASoftware Yue Li, Stanford Matlab, large-scale Pomp U of Michigan R, general state-estimation PyIT CIPR Real-world petroleum DA (?) Datum Raanes Matlab, personal publications EnKF-Matlab Sakov Matlab, personal publications and intro EnKF-C Sakov C, light-weight EnKF, off-line IEnKS code Bocquet Python, personal publications pyda Hickman Python, personal publications The EnKF-Matlab and IEnKS codes have been inspirational in the development of DAPPER. *: AWI/Liege/CNRS/NERSC/Reading/Delft","title":"Alternative projects"},{"location":"#contributors","text":"Patrick N. Raanes, Colin Grudzien, Maxime Tondeur, Remy Dubois If you use this software in a publication, please cite as follows. @misc { raanes2018dapper , author = {Patrick N. Raanes and others} , title = {nansencenter/DAPPER: Version 0.8} , month = December , year = 2018 , doi = {10.5281/zenodo.2029296} , url = {https://doi.org/10.5281/zenodo.2029296} }","title":"Contributors"},{"location":"#publication-list","text":"https://www.geosci-model-dev-discuss.net/gmd-2019-136/ https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3386 https://www.nonlin-processes-geophys-discuss.net/npg-2019-10","title":"Publication list"},{"location":"#powered-by","text":"","title":"Powered by"},{"location":"reference/dapper/","text":"Module dapper Data Assimilation with Python: a Package for Experimental Research (DAPPER). DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods using synthetic/twin experiments. View Source \"\"\"Data Assimilation with Python: a Package for Experimental Research (DAPPER). DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods using synthetic/twin experiments. \"\"\" __version__ = \"0.9.6\" import sys assert sys . version_info >= ( 3 , 8 ), \"Need Python>=3.8\" # Profiling. # Launch python script: $ kernprof -l -v myprog.py # Functions decorated with 'profile' from below will be timed. try : import builtins profile = builtins . profile # will exists if launched via kernprof except AttributeError : def profile ( func ): return func # provide a pass-through version. from dapper.tools.series import UncertainQtty from .admin import ( HiddenMarkovModel , Operator , da_method , get_param_setter , seed_and_simulate , xpList ) from .da_methods.baseline import Climatology , OptInterp , Var3D # DA methods from .da_methods.ensemble import LETKF , SL_EAKF , EnKF , EnKF_N , EnKS , EnRTS from .da_methods.extended import ExtKF , ExtRTS from .da_methods.other import LNETF , RHF from .da_methods.particle import OptPF , PartFilt , PFa , PFxN , PFxN_EnKF from .da_methods.variational import Var4D , iEnKS from .data_management import ( default_fig_adjustments , default_styles , discretize_cmap , load_xps , make_label , rel_index , xpSpace ) from .dpr_config import rc from .stats import register_stat from .tools.chronos import Chronology from .tools.magic import magic_naming , spell_out from .tools.math import ( ens_compatible , linspace_int , Id_Obs , partial_Id_Obs , round2 , with_recursion , with_rk4 ) from .tools.matrices import CovMat from .tools.randvars import RV , GaussRV from .tools.stoch import rand , randn , set_seed from .tools.viz import freshfig # import dapper.tools as tools # import dapper.admin as admin # import dapper.stats as stats # import dapper.data_management as data_management Sub-modules dapper.admin dapper.data_management dapper.dict_tools dapper.dpr_config dapper.stats Functions profile def profile ( func ) View Source def profile ( func ): return func # provide a pass-through version.","title":"Index"},{"location":"reference/dapper/#module-dapper","text":"Data Assimilation with Python: a Package for Experimental Research (DAPPER). DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods using synthetic/twin experiments. View Source \"\"\"Data Assimilation with Python: a Package for Experimental Research (DAPPER). DAPPER is a set of templates for benchmarking the performance of data assimilation (DA) methods using synthetic/twin experiments. \"\"\" __version__ = \"0.9.6\" import sys assert sys . version_info >= ( 3 , 8 ), \"Need Python>=3.8\" # Profiling. # Launch python script: $ kernprof -l -v myprog.py # Functions decorated with 'profile' from below will be timed. try : import builtins profile = builtins . profile # will exists if launched via kernprof except AttributeError : def profile ( func ): return func # provide a pass-through version. from dapper.tools.series import UncertainQtty from .admin import ( HiddenMarkovModel , Operator , da_method , get_param_setter , seed_and_simulate , xpList ) from .da_methods.baseline import Climatology , OptInterp , Var3D # DA methods from .da_methods.ensemble import LETKF , SL_EAKF , EnKF , EnKF_N , EnKS , EnRTS from .da_methods.extended import ExtKF , ExtRTS from .da_methods.other import LNETF , RHF from .da_methods.particle import OptPF , PartFilt , PFa , PFxN , PFxN_EnKF from .da_methods.variational import Var4D , iEnKS from .data_management import ( default_fig_adjustments , default_styles , discretize_cmap , load_xps , make_label , rel_index , xpSpace ) from .dpr_config import rc from .stats import register_stat from .tools.chronos import Chronology from .tools.magic import magic_naming , spell_out from .tools.math import ( ens_compatible , linspace_int , Id_Obs , partial_Id_Obs , round2 , with_recursion , with_rk4 ) from .tools.matrices import CovMat from .tools.randvars import RV , GaussRV from .tools.stoch import rand , randn , set_seed from .tools.viz import freshfig # import dapper.tools as tools # import dapper.admin as admin # import dapper.stats as stats # import dapper.data_management as data_management","title":"Module dapper"},{"location":"reference/dapper/#sub-modules","text":"dapper.admin dapper.data_management dapper.dict_tools dapper.dpr_config dapper.stats","title":"Sub-modules"},{"location":"reference/dapper/#functions","text":"","title":"Functions"},{"location":"reference/dapper/#profile","text":"def profile ( func ) View Source def profile ( func ): return func # provide a pass-through version.","title":"profile"},{"location":"reference/dapper/admin/","text":"Module dapper.admin Define high-level API in DAPPER. Used for experiment (xp) specification/administration, including: da_method decorator xpList save_data run_experiment run_from_file HiddenMarkovModel Operator View Source \"\"\"Define high-level API in DAPPER. Used for experiment (xp) specification/administration, including: - da_method decorator - xpList - save_data - run_experiment - run_from_file - HiddenMarkovModel - Operator \"\"\" import dapper.stats import dapper.dict_tools as dict_tools from dapper.tools.chronos import Chronology from dapper.tools.randvars import RV , GaussRV from dapper.tools.stoch import set_seed from dapper.dpr_config import rc from dapper.tools.remote.uplink import submit_job_GCP import dapper.tools.utils as utils from dapper.tools.localization import no_localization from dapper.tools.math import Id_op , Id_mat from pathlib import Path import dataclasses as dcs import copy from textwrap import dedent import os import sys import re import time import numpy as np import inspect import functools import dill import shutil from datetime import datetime class HiddenMarkovModel ( dict_tools . NicePrint ): \"\"\"Container for attributes of a Hidden Markov Model (HMM). This container contains the specification of a \"twin experiment\", i.e. an \"OSSE (observing system simulation experiment)\". \"\"\" def __init__ ( self , Dyn , Obs , t , X0 , ** kwargs ): # fmt: off self . Dyn = Dyn if isinstance ( Dyn , Operator ) else Operator ( ** Dyn ) # noqa self . Obs = Obs if isinstance ( Obs , Operator ) else Operator ( ** Obs ) # noqa self . t = t if isinstance ( t , Chronology ) else Chronology ( ** t ) # noqa self . X0 = X0 if isinstance ( X0 , RV ) else RV ( ** X0 ) # noqa # fmt: on # Name self . name = kwargs . pop ( \"name\" , \"\" ) if not self . name : name = inspect . getfile ( inspect . stack ()[ 1 ][ 0 ]) try : self . name = str ( Path ( name ) . relative_to ( rc . dirs . dapper / 'mods' )) except ValueError : self . name = str ( Path ( name )) # Kwargs abbrevs = { 'LP' : 'liveplotters' } for key in kwargs : setattr ( self , abbrevs . get ( key , key ), kwargs [ key ]) # Defaults if not hasattr ( self . Obs , \"localizer\" ): self . Obs . localizer = no_localization ( self . Nx , self . Ny ) if not hasattr ( self , \"sectors\" ): self . sectors = {} # Validation if self . Obs . noise . C == 0 or self . Obs . noise . C . rk != self . Obs . noise . C . M : raise ValueError ( \"Rank-deficient R not supported.\" ) # ndim shortcuts @property def Nx ( self ): return self . Dyn . M @property def Ny ( self ): return self . Obs . M printopts = { 'ordering' : [ 'Dyn' , 'Obs' , 't' , 'X0' ]} def simulate ( self , desc = 'Truth & Obs' ): \"\"\"Generate synthetic truth and observations.\"\"\" Dyn , Obs , chrono , X0 = self . Dyn , self . Obs , self . t , self . X0 # Init xx = np . zeros (( chrono . K + 1 , Dyn . M )) yy = np . zeros (( chrono . KObs + 1 , Obs . M )) xx [ 0 ] = X0 . sample ( 1 ) # Loop for k , kObs , t , dt in utils . progbar ( chrono . ticker , desc ): xx [ k ] = Dyn ( xx [ k - 1 ], t - dt , dt ) + np . sqrt ( dt ) * Dyn . noise . sample ( 1 ) if kObs is not None : yy [ kObs ] = Obs ( xx [ k ], t ) + Obs . noise . sample ( 1 ) return xx , yy class Operator ( dict_tools . NicePrint ): \"\"\"Container for operators (models).\"\"\" def __init__ ( self , M , model = None , noise = None , ** kwargs ): self . M = M # None => Identity model if model is None : model = Id_op () kwargs [ 'linear' ] = lambda x , t , dt : Id_mat ( M ) self . model = model # None/0 => No noise if isinstance ( noise , RV ): self . noise = noise else : if noise is None : noise = 0 if np . isscalar ( noise ): self . noise = GaussRV ( C = noise , M = M ) else : self . noise = GaussRV ( C = noise ) # Write attributes for key , value in kwargs . items (): setattr ( self , key , value ) def __call__ ( self , * args , ** kwargs ): return self . model ( * args , ** kwargs ) printopts = { 'ordering' : [ 'M' , 'model' , 'noise' ]} def da_method ( * default_dataclasses ): \"\"\"Make the decorator that makes the DA classes. Example: >>> @da_method() >>> class Sleeper(): >>> \"Do nothing.\" >>> seconds : int = 10 >>> success : bool = True >>> def assimilate(self,*args,**kwargs): >>> for k in utils.progbar(range(self.seconds)): >>> time.sleep(1) >>> if not self.success: >>> raise RuntimeError(\"Sleep over. Failing as intended.\") Example: >>> @dcs.dataclass >>> class ens_defaults: >>> infl : float = 1.0 >>> rot : bool = False >>> >>> @da_method(ens_defaults) >>> class EnKF: >>> N : int >>> upd_a : str = \"Sqrt\" >>> >>> def assimilate(self,HMM,xx,yy): >>> ... \"\"\" def dataclass_with_defaults ( cls ): \"\"\"Decorator based on dataclass. This adds __init__, __repr__, __eq__, ..., but also includes inherited defaults (see https://stackoverflow.com/a/58130805 ). Also: - Wraps assimilate() to provide gentle_fail functionality. - Initialises and writes the Stats object.\"\"\" # Default fields invovle: (1) annotations and (2) attributes. def set_field ( name , type , val ): if not hasattr ( cls , '__annotations__' ): cls . __annotations__ = {} cls . __annotations__ [ name ] = type if not isinstance ( val , dcs . Field ): val = dcs . field ( default = val ) setattr ( cls , name , val ) # APPend default fields without overwriting. # Don't implement (by PREpending?) non-default args -- to messy! for D in default_dataclasses : # NB: Calling dataclass twice always makes repr=True, so avoid this. for F in dcs . fields ( dcs . dataclass ( D )): if F . name not in cls . __annotations__ : set_field ( F . name , F . type , F ) # Create new class (NB: old/new classes have same id) cls = dcs . dataclass ( cls ) # Shortcut for self.__class__.__name__ cls . da_method = cls . __name__ def assimilate ( self , HMM , xx , yy , desc = None , ** stat_kwargs ): # Progressbar name pb_name_hook = self . da_method if desc is None else desc # noqa # Init stats self . stats = dapper . stats . Stats ( self , HMM , xx , yy , ** stat_kwargs ) # Assimilate time_start = time . time () old_assimilate ( self , HMM , xx , yy ) dapper . stats . register_stat ( self . stats , \"duration\" , time . time () - time_start ) old_assimilate = cls . assimilate cls . assimilate = functools . wraps ( old_assimilate )( assimilate ) return cls return dataclass_with_defaults def seed_and_simulate ( HMM , xp ): \"\"\"Default experiment setup. Set seed and simulate truth and obs. Note: if there is no ``xp.seed`` then then the seed is not set. Thus, different experiments will produce different truth and obs.\"\"\" set_seed ( getattr ( xp , 'seed' , False )) xx , yy = HMM . simulate () return xx , yy def run_experiment ( xp , label , savedir , HMM , setup = None , free = True , statkeys = False , fail_gently = False , ** stat_kwargs ): \"\"\"Used by xpList.launch() to run each single experiment. This involves steps similar to ``example_1.py``, i.e.: - setup() : Call function given by user. Should set params, eg HMM.Force, seed, and return (simulated/loaded) truth and obs series. - xp.assimilate() : run DA, pass on exception if fail_gently - xp.stats.average_in_time() : result averaging - xp.avrgs.tabulate() : result printing - dill.dump() : result storage \"\"\" # We should copy HMM so as not to cause any nasty surprises such as # expecting param=1 when param=2 (coz it's not been reset). # NB: won't copy implicitly ref'd obj's (like L96's core). => bug w/ MP? hmm = copy . deepcopy ( HMM ) # GENERATE TRUTH/OBS xx , yy = setup ( hmm , xp ) # ASSIMILATE try : xp . assimilate ( hmm , xx , yy , label , ** stat_kwargs ) except Exception as ERR : if fail_gently : xp . crashed = True if fail_gently not in [ \"silent\" , \"quiet\" ]: utils . print_cropped_traceback ( ERR ) else : raise ERR # AVERAGE xp . stats . average_in_time ( free = free ) # PRINT if statkeys : statkeys = () if statkeys is True else statkeys print ( xp . avrgs . tabulate ( statkeys )) # SAVE if savedir : with open ( Path ( savedir ) / \"xp\" , \"wb\" ) as FILE : dill . dump ({ 'xp' : xp }, FILE ) # TODO 2: check collections.userlist # TODO 2: __add__ vs __iadd__ class xpList ( list ): \"\"\"List, subclassed for holding experiment (\"xp\") objects. Main use: administrate experiment **launches**. Also see: ``xpSpace`` for experiment **result presentation**. Modifications to ``list``: - ``__iadd__`` (append) also for single items; this is hackey, but convenience is king. - ``append()`` supports ``unique`` to enable lazy xp declaration. - ``__getitem__`` supports lists. - pretty printing (using common/distinct attrs). Add-ons: - ``launch()`` - ``print_averages()`` - ``gen_names()`` - ``inds()`` to search by kw-attrs. \"\"\" def __init__ ( self , * args , unique = False ): \"\"\"Initialize without args, or with a list of configs. If ``unique``: duplicates won't get appended. This makes ``append()`` (and ``__iadd__()``) relatively slow. Use ``extend()`` or ``__add__()`` to bypass this validation.\"\"\" self . unique = unique super () . __init__ ( * args ) def __iadd__ ( self , xp ): if not hasattr ( xp , '__iter__' ): xp = [ xp ] for item in xp : self . append ( item ) return self def append ( self , xp ): \"\"\"Append if not unique & present.\"\"\" if not ( self . unique and xp in self ): super () . append ( xp ) def __getitem__ ( self , keys ): \"\"\"Indexing, also by a list\"\"\" try : B = [ self [ k ] for k in keys ] # if keys is list except TypeError : B = super () . __getitem__ ( keys ) # if keys is int, slice if hasattr ( B , '__len__' ): B = xpList ( B ) # Cast return B def inds ( self , strict = True , missingval = \"NONSENSE\" , ** kws ): \"\"\"Find (all) indices of configs whose attributes match kws. If strict, then xp's lacking a requested attr will not match, unless the missingval (e.g. None) matches the required value. \"\"\" def match ( xp ): def missing ( v ): return missingval if strict else v matches = [ getattr ( xp , k , missing ( v )) == v for k , v in kws . items ()] return all ( matches ) return [ i for i , xp in enumerate ( self ) if match ( xp )] @property def da_methods ( self ): return [ xp . da_method for xp in self ] def split_attrs ( self , nomerge = ()): \"\"\"Compile the attrs of all xps; split as distinct, redundant, common. Insert None if an attribute is distinct but not in xp.\"\"\" def _aggregate_keys (): \"Aggregate keys from all xps\" if len ( self ) == 0 : return [] # Start with da_method aggregate = [ 'da_method' ] # Aggregate all other keys for xp in self : # Get dataclass fields try : dc_fields = dcs . fields ( xp . __class__ ) dc_names = [ F . name for F in dc_fields ] keys = xp . __dict__ . keys () except TypeError : # Assume namedtuple dc_names = [] keys = xp . _fields # For all potential keys: for k in keys : # If not already present: if k not in aggregate : # If dataclass, check repr: if k in dc_names : if dc_fields [ dc_names . index ( k )] . repr : aggregate . append ( k ) # Else, just append else : aggregate . append ( k ) # Remove unwanted excluded = [ re . compile ( '^_' ), 'avrgs' , 'stats' , 'HMM' , 'duration' ] aggregate = dict_tools . complement ( aggregate , excluded ) return aggregate distinct , redundant , common = {}, {}, {} for key in _aggregate_keys (): # Want to distinguish actual None's from empty (\"N/A\"). # => Don't use getattr(obj,key,None) vals = [ getattr ( xp , key , \"N/A\" ) for xp in self ] # Sort (assign dct) into distinct, redundant, common if dict_tools . flexcomp ( key , * nomerge ): # nomerge => Distinct dct , vals = distinct , vals elif all ( vals [ 0 ] == v for v in vals ): # all values equal => common dct , vals = common , vals [ 0 ] else : v0 = next ( v for v in vals if \"N/A\" != v ) if all ( v == \"N/A\" or v == v0 for v in vals ): # all values equal or \"N/A\" => redundant dct , vals = redundant , v0 else : # otherwise => distinct dct , vals = distinct , vals # Replace \"N/A\" by None def sub ( v ): return None if v == \"N/A\" else v if isinstance ( vals , str ): vals = sub ( vals ) else : try : vals = [ sub ( v ) for v in vals ] except TypeError : vals = sub ( vals ) dct [ key ] = vals return distinct , redundant , common def __repr__ ( self ): distinct , redundant , common = self . split_attrs () s = '<xpList> of length %d with attributes: \\n ' % len ( self ) s += utils . tab ( distinct , headers = \"keys\" , showindex = True ) s += \" \\n Other attributes: \\n \" s += str ( dict_tools . AlignedDict ({ ** redundant , ** common })) return s def gen_names ( self , abbrev = 6 , tab = False ): \"\"\"Similiar to ``self.__repr__()``, but: - returns *list* of names - tabulation is optional - attaches (abbreviated) labels to each attribute \"\"\" distinct , redundant , common = self . split_attrs ( nomerge = [ \"da_method\" ]) labels = distinct . keys () values = distinct . values () # Label abbreviation labels = [ utils . collapse_str ( k , abbrev ) for k in labels ] # Make label columns: insert None or lbl+\":\", depending on value def column ( lbl , vals ): return [ None if v is None else lbl + \":\" for v in vals ] labels = [ column ( lbl , vals ) for lbl , vals in zip ( labels , values )] # Interlace labels and values table = [ x for ( a , b ) in zip ( labels , values ) for x in ( a , b )] # Rm da_method label (but keep value) table . pop ( 0 ) # Transpose table = list ( map ( list , zip ( * table ))) # Tabulate table = utils . tab ( table , tablefmt = \"plain\" ) # Rm space between lbls/vals table = re . sub ( ': +' , ':' , table ) # Rm alignment if not tab : table = re . sub ( r ' +' , r ' ' , table ) return table . splitlines () def tabulate_avrgs ( self , * args , ** kwargs ): \"\"\"Pretty (tabulated) repr of xps & their avrgs. Similar to stats.tabulate_avrgs(), but for the entire list of xps.\"\"\" distinct , redundant , common = self . split_attrs () averages = dapper . stats . tabulate_avrgs ([ C . avrgs for C in self ], * args , ** kwargs ) columns = { ** distinct , '|' : [ '|' ] * len ( self ), ** averages } # merge return utils . tab ( columns , headers = \"keys\" , showindex = True ) . replace ( '\u2423' , ' ' ) def launch ( self , HMM , save_as = \"noname\" , mp = False , setup = seed_and_simulate , fail_gently = None , ** kwargs ): \"\"\"For each xp in self: run_experiment(xp, ...). The results are saved in ``rc.dirs['data']/save_as.stem``, unless ``save_as`` is False/None. Depending on ``mp``, run_experiment() is delegated to one of: - caller process (no parallelisation) - multiprocessing on this host - GCP (Google Cloud Computing) with HTCondor If ``setup == None``: use ``seed_and_simulate()``. The kwargs are forwarded to run_experiment(). See ``example_2.py`` and ``example_3.py`` for example use. \"\"\" # TODO 2: doc files and code options in mp, e.g # `files` get added to PYTHONPATH and have dir-structure preserved. # Setup: Experiment initialisation. Default: seed_and_simulate(). # Enables setting experiment variables that are not parameters of a da_method. # Collect common args forwarded to run_experiment kwargs [ 'HMM' ] = HMM kwargs [ \"setup\" ] = setup # Parse mp option if not mp : mp = False elif mp in [ True , \"MP\" ]: mp = dict ( server = \"local\" ) elif isinstance ( mp , int ): mp = dict ( server = \"local\" , NPROC = mp ) elif mp in [ \"GCP\" , \"Google\" ]: mp = dict ( server = \"GCP\" , files = [], code = \"\" ) assert isinstance ( mp , dict ) # Parse fail_gently if fail_gently is None : if isinstance ( mp , dict ) and mp [ \"server\" ] == \"GCP\" : fail_gently = False # coz cloud processing is entirely de-coupled anyways else : fail_gently = True # True unless otherwise requested kwargs [ \"fail_gently\" ] = fail_gently # Parse save_as if save_as in [ None , False ]: assert not mp , \"Multiprocessing requires saving data.\" # Parallelization w/o storing is possible, especially w/ threads. # But it involves more complicated communication set-up. def xpi_dir ( * args ): return None else : save_as = rc . dirs . data / Path ( save_as ) . stem save_as /= \"run_\" + datetime . now () . strftime ( \"%Y-%m- %d __%H:%M:%S\" ) os . makedirs ( save_as ) print ( f \"Experiment stored at { save_as } \" ) def xpi_dir ( i ): path = save_as / str ( i ) os . mkdir ( path ) return path # No parallelization if not mp : for ixp , ( xp , label ) in enumerate ( zip ( self , self . gen_names ())): run_experiment ( xp , label , xpi_dir ( ixp ), ** kwargs ) # Local multiprocessing elif mp [ \"server\" ] . lower () == \"local\" : def run_with_fixed_args ( arg ): xp , ixp = arg run_experiment ( xp , None , xpi_dir ( ixp ), ** kwargs ) args = zip ( self , range ( len ( self ))) utils . disable_progbar = True utils . disable_user_interaction = True NPROC = mp . get ( \"NPROC\" , None ) # None => mp.cpu_count() from dapper.tools.multiprocessing import mpd # will fail on GCP with mpd . Pool ( NPROC ) as pool : list ( utils . tqdm . tqdm ( pool . imap ( run_with_fixed_args , args ), total = len ( self ), desc = \"Parallel experim's\" , smoothing = 0.1 )) utils . disable_progbar = False utils . disable_user_interaction = False # Google cloud platform, multiprocessing elif mp [ \"server\" ] == \"GCP\" : for ixp , xp in enumerate ( self ): with open ( xpi_dir ( ixp ) / \"xp.var\" , \"wb\" ) as f : dill . dump ( dict ( xp = xp ), f ) with open ( save_as / \"xp.com\" , \"wb\" ) as f : dill . dump ( kwargs , f ) # mkdir extra_files extra_files = save_as / \"extra_files\" os . mkdir ( extra_files ) # Default files: .py files in sys.path[0] (main script's path) if not mp . get ( \"files\" , []): # Todo 4: also intersect(..., sys.modules). # Todo 4: use git ls-tree instead? ff = os . listdir ( sys . path [ 0 ]) mp [ \"files\" ] = [ f for f in ff if f . endswith ( \".py\" )] # Copy files into extra_files for f in mp [ \"files\" ]: if isinstance ( f , ( str , Path )): # Example: f = \"A.py\" path = Path ( sys . path [ 0 ]) / f dst = f else : # instance of tuple(path, root) # Example: f = (\"~/E/G/A.py\", \"G\") path , root = f dst = Path ( path ) . relative_to ( root ) dst = extra_files / dst os . makedirs ( dst . parent , exist_ok = True ) try : shutil . copytree ( path , dst ) # dir -r except OSError : shutil . copy2 ( path , dst ) # file # Loads PWD/xp_{var,com} and calls run_experiment() with open ( extra_files / \"load_and_run.py\" , \"w\" ) as f : f . write ( dedent ( \"\"\" \\ import dill from dapper.admin import run_experiment # Load with open(\"xp.com\", \"rb\") as f: com = dill.load(f) with open(\"xp.var\", \"rb\") as f: var = dill.load(f) # User-defined code %s # Run result = run_experiment(var['xp'], None, \".\", **com) \"\"\" ) % dedent ( mp [ \"code\" ])) with open ( extra_files / \"dpr_config.yaml\" , \"w\" ) as f : f . write ( \" \\n \" . join ([ \"data_root: '$cwd'\" , \"liveplotting: no\" , \"welcome_message: no\" ])) submit_job_GCP ( save_as ) return save_as def get_param_setter ( param_dict , ** glob_dict ): \"\"\"Mass creation of xp's by combining the value lists in the parameter dicts. The parameters are trimmed to the ones available for the given method. This is a good deal more efficient than relying on xpList's unique=True. Beware! If, eg., [infl,rot] are in the param_dict, aimed at the EnKF, but you forget that they are also attributes some method where you don't actually want to use them (eg. SVGDF), then you'll create many more than you intend. \"\"\" def for_params ( method , ** fixed_params ): dc_fields = [ f . name for f in dcs . fields ( method )] params = dict_tools . intersect ( param_dict , dc_fields ) params = dict_tools . complement ( params , fixed_params ) params = { ** glob_dict , ** params } # glob_dict 1st def xp1 ( dct ): xp = method ( ** dict_tools . intersect ( dct , dc_fields ), ** fixed_params ) for key , v in dict_tools . intersect ( dct , glob_dict ) . items (): setattr ( xp , key , v ) return xp return [ xp1 ( dct ) for dct in dict_tools . prodct ( params )] return for_params Functions da_method def da_method ( * default_dataclasses ) Make the decorator that makes the DA classes. Example: @da_method() class Sleeper(): \"Do nothing.\" seconds : int = 10 success : bool = True def assimilate(self, args, *kwargs): for k in utils.progbar(range(self.seconds)): time.sleep(1) if not self.success: raise RuntimeError(\"Sleep over. Failing as intended.\") Example: @dcs.dataclass class ens_defaults: infl : float = 1.0 rot : bool = False @da_method(ens_defaults) class EnKF: N : int upd_a : str = \"Sqrt\" def assimilate(self,HMM,xx,yy): ... View Source def da_method ( * default_dataclasses ) : \"\"\"Make the decorator that makes the DA classes. Example: >>> @da_method() >>> class Sleeper(): >>> \" Do nothing . \" >>> seconds : int = 10 >>> success : bool = True >>> def assimilate(self,*args,**kwargs): >>> for k in utils.progbar(range(self.seconds)): >>> time.sleep(1) >>> if not self.success: >>> raise RuntimeError(\" Sleep over . Failing as intended . \") Example: >>> @dcs.dataclass >>> class ens_defaults: >>> infl : float = 1.0 >>> rot : bool = False >>> >>> @da_method(ens_defaults) >>> class EnKF: >>> N : int >>> upd_a : str = \" Sqrt \" >>> >>> def assimilate(self,HMM,xx,yy): >>> ... \"\"\" def dataclass_with_defaults ( cls ) : \"\"\"Decorator based on dataclass. This adds __init__, __repr__, __eq__, ..., but also includes inherited defaults (see https://stackoverflow.com/a/58130805 ). Also: - Wraps assimilate() to provide gentle_fail functionality. - Initialises and writes the Stats object.\"\"\" # Default fields invovle : ( 1 ) annotations and ( 2 ) attributes . def set_field ( name , type , val ) : if not hasattr ( cls , '__annotations__' ) : cls . __annotations__ = {} cls . __annotations__ [ name ] = type if not isinstance ( val , dcs . Field ) : val = dcs . field ( default = val ) setattr ( cls , name , val ) # APPend default fields without overwriting . # Don ' t implement ( by PREpending ? ) non - default args -- to messy ! for D in default_dataclasses : # NB : Calling dataclass twice always makes repr = True , so avoid this . for F in dcs . fields ( dcs . dataclass ( D )) : if F . name not in cls . __annotations__ : set_field ( F . name , F . type , F ) # Create new class ( NB : old / new classes have same id ) cls = dcs . dataclass ( cls ) # Shortcut for self . __class__ . __name__ cls . da_method = cls . __name__ def assimilate ( self , HMM , xx , yy , desc = None , ** stat_kwargs ) : # Progressbar name pb_name_hook = self . da_method if desc is None else desc # noqa # Init stats self . stats = dapper . stats . Stats ( self , HMM , xx , yy , ** stat_kwargs ) # Assimilate time_start = time . time () old_assimilate ( self , HMM , xx , yy ) dapper . stats . register_stat ( self . stats , \"duration\" , time . time () - time_start ) old_assimilate = cls . assimilate cls . assimilate = functools . wraps ( old_assimilate )( assimilate ) return cls return dataclass_with_defaults get_param_setter def get_param_setter ( param_dict , ** glob_dict ) Mass creation of xp's by combining the value lists in the parameter dicts. The parameters are trimmed to the ones available for the given method. This is a good deal more efficient than relying on xpList's unique=True. Beware! If, eg., [infl,rot] are in the param_dict, aimed at the EnKF, but you forget that they are also attributes some method where you don't actually want to use them (eg. SVGDF), then you'll create many more than you intend. View Source def get_param_setter ( param_dict , ** glob_dict ): \"\"\"Mass creation of xp's by combining the value lists in the parameter dicts. The parameters are trimmed to the ones available for the given method. This is a good deal more efficient than relying on xpList's unique=True. Beware! If, eg., [infl,rot] are in the param_dict, aimed at the EnKF, but you forget that they are also attributes some method where you don't actually want to use them (eg. SVGDF), then you'll create many more than you intend. \"\"\" def for_params ( method , ** fixed_params ): dc_fields = [ f . name for f in dcs . fields ( method )] params = dict_tools . intersect ( param_dict , dc_fields ) params = dict_tools . complement ( params , fixed_params ) params = { ** glob_dict , ** params } # glob_dict 1st def xp1 ( dct ): xp = method ( ** dict_tools . intersect ( dct , dc_fields ), ** fixed_params ) for key , v in dict_tools . intersect ( dct , glob_dict ) . items (): setattr ( xp , key , v ) return xp return [ xp1 ( dct ) for dct in dict_tools . prodct ( params )] return for_params run_experiment def run_experiment ( xp , label , savedir , HMM , setup = None , free = True , statkeys = False , fail_gently = False , ** stat_kwargs ) Used by xpList.launch() to run each single experiment. This involves steps similar to example_1.py , i.e.: setup() : Call function given by user. Should set params, eg HMM.Force, seed, and return (simulated/loaded) truth and obs series. xp.assimilate() : run DA, pass on exception if fail_gently xp.stats.average_in_time() : result averaging xp.avrgs.tabulate() : result printing dill.dump() : result storage View Source def run_experiment ( xp , label , savedir , HMM , setup = None , free = True , statkeys = False , fail_gently = False , ** stat_kwargs ): \"\"\"Used by xpList.launch() to run each single experiment. This involves steps similar to ``example_1.py``, i.e.: - setup() : Call function given by user. Should set params, eg HMM.Force, seed, and return (simulated/loaded) truth and obs series. - xp.assimilate() : run DA, pass on exception if fail_gently - xp.stats.average_in_time() : result averaging - xp.avrgs.tabulate() : result printing - dill.dump() : result storage \"\"\" # We should copy HMM so as not to cause any nasty surprises such as # expecting param=1 when param=2 (coz it's not been reset). # NB: won't copy implicitly ref'd obj's (like L96's core). => bug w/ MP? hmm = copy . deepcopy ( HMM ) # GENERATE TRUTH/OBS xx , yy = setup ( hmm , xp ) # ASSIMILATE try : xp . assimilate ( hmm , xx , yy , label , ** stat_kwargs ) except Exception as ERR : if fail_gently : xp . crashed = True if fail_gently not in [ \"silent\" , \"quiet\" ]: utils . print_cropped_traceback ( ERR ) else : raise ERR # AVERAGE xp . stats . average_in_time ( free = free ) # PRINT if statkeys : statkeys = () if statkeys is True else statkeys print ( xp . avrgs . tabulate ( statkeys )) # SAVE if savedir : with open ( Path ( savedir ) / \"xp\" , \"wb\" ) as FILE : dill . dump ({ 'xp' : xp }, FILE ) seed_and_simulate def seed_and_simulate ( HMM , xp ) Default experiment setup. Set seed and simulate truth and obs. Note: if there is no xp.seed then then the seed is not set. Thus, different experiments will produce different truth and obs. View Source def seed_and_simulate ( HMM , xp ): \"\"\"Default experiment setup. Set seed and simulate truth and obs. Note: if there is no ``xp.seed`` then then the seed is not set. Thus, different experiments will produce different truth and obs.\"\"\" set_seed ( getattr ( xp , 'seed' , False )) xx , yy = HMM . simulate () return xx , yy Classes HiddenMarkovModel class HiddenMarkovModel ( Dyn , Obs , t , X0 , ** kwargs ) Container for attributes of a Hidden Markov Model (HMM). This container contains the specification of a \"twin experiment\", i.e. an \"OSSE (observing system simulation experiment)\". View Source class HiddenMarkovModel ( dict_tools . NicePrint ) : \"\"\"Container for attributes of a Hidden Markov Model (HMM). This container contains the specification of a \" twin experiment \", i.e. an \" OSSE ( observing system simulation experiment ) \". \"\"\" def __init__ ( self , Dyn , Obs , t , X0 , ** kwargs ) : # fmt : off self . Dyn = Dyn if isinstance ( Dyn , Operator ) else Operator ( ** Dyn ) # noqa self . Obs = Obs if isinstance ( Obs , Operator ) else Operator ( ** Obs ) # noqa self . t = t if isinstance ( t , Chronology ) else Chronology ( ** t ) # noqa self . X0 = X0 if isinstance ( X0 , RV ) else RV ( ** X0 ) # noqa # fmt : on # Name self . name = kwargs . pop ( \"name\" , \"\" ) if not self . name : name = inspect . getfile ( inspect . stack () [ 1 ][ 0 ] ) try : self . name = str ( Path ( name ). relative_to ( rc . dirs . dapper / 'mods' )) except ValueError : self . name = str ( Path ( name )) # Kwargs abbrevs = { 'LP' : 'liveplotters' } for key in kwargs : setattr ( self , abbrevs . get ( key , key ), kwargs [ key ] ) # Defaults if not hasattr ( self . Obs , \"localizer\" ) : self . Obs . localizer = no_localization ( self . Nx , self . Ny ) if not hasattr ( self , \"sectors\" ) : self . sectors = {} # Validation if self . Obs . noise . C == 0 or self . Obs . noise . C . rk != self . Obs . noise . C . M : raise ValueError ( \"Rank-deficient R not supported.\" ) # ndim shortcuts @property def Nx ( self ) : return self . Dyn . M @property def Ny ( self ) : return self . Obs . M printopts = { 'ordering' : [ 'Dyn', 'Obs', 't', 'X0' ] } def simulate ( self , desc = 'Truth & Obs' ) : \"\"\"Generate synthetic truth and observations.\"\"\" Dyn , Obs , chrono , X0 = self . Dyn , self . Obs , self . t , self . X0 # Init xx = np . zeros (( chrono . K + 1 , Dyn . M )) yy = np . zeros (( chrono . KObs + 1 , Obs . M )) xx [ 0 ] = X0 . sample ( 1 ) # Loop for k , kObs , t , dt in utils . progbar ( chrono . ticker , desc ) : xx [ k ] = Dyn ( xx [ k-1 ] , t - dt , dt ) + np . sqrt ( dt ) * Dyn . noise . sample ( 1 ) if kObs is not None : yy [ kObs ] = Obs ( xx [ k ] , t ) + Obs . noise . sample ( 1 ) return xx , yy Ancestors (in MRO) dapper.dict_tools.NicePrint Class variables printopts Instance variables Nx Ny Methods simulate def simulate ( self , desc = 'Truth & Obs' ) Generate synthetic truth and observations. View Source def simulate ( self , desc = 'Truth & Obs' ) : \"\"\"Generate synthetic truth and observations.\"\"\" Dyn , Obs , chrono , X0 = self . Dyn , self . Obs , self . t , self . X0 # Init xx = np . zeros (( chrono . K + 1 , Dyn . M )) yy = np . zeros (( chrono . KObs + 1 , Obs . M )) xx [ 0 ] = X0 . sample ( 1 ) # Loop for k , kObs , t , dt in utils . progbar ( chrono . ticker , desc ) : xx [ k ] = Dyn ( xx [ k-1 ] , t - dt , dt ) + np . sqrt ( dt ) * Dyn . noise . sample ( 1 ) if kObs is not None : yy [ kObs ] = Obs ( xx [ k ] , t ) + Obs . noise . sample ( 1 ) return xx , yy Operator class Operator ( M , model = None , noise = None , ** kwargs ) Container for operators (models). View Source class Operator ( dict_tools . NicePrint ): \"\"\"Container for operators (models).\"\"\" def __init__ ( self , M , model = None , noise = None , ** kwargs ): self . M = M # None => Identity model if model is None : model = Id_op () kwargs [ 'linear' ] = lambda x , t , dt : Id_mat ( M ) self . model = model # None/0 => No noise if isinstance ( noise , RV ): self . noise = noise else : if noise is None : noise = 0 if np . isscalar ( noise ): self . noise = GaussRV ( C = noise , M = M ) else : self . noise = GaussRV ( C = noise ) # Write attributes for key , value in kwargs . items (): setattr ( self , key , value ) def __call__ ( self , * args , ** kwargs ): return self . model ( * args , ** kwargs ) printopts = { 'ordering' : [ 'M' , 'model' , 'noise' ]} Ancestors (in MRO) dapper.dict_tools.NicePrint Class variables printopts xpList class xpList ( * args , unique = False ) List, subclassed for holding experiment (\"xp\") objects. Main use: administrate experiment launches . Also see: xpSpace for experiment result presentation . Modifications to list : __iadd__ (append) also for single items; this is hackey, but convenience is king. append() supports unique to enable lazy xp declaration. __getitem__ supports lists. pretty printing (using common/distinct attrs). Add-ons: launch() print_averages() gen_names() inds() to search by kw-attrs. View Source class xpList ( list ) : \"\"\"List, subclassed for holding experiment (\" xp \") objects. Main use : administrate experiment ** launches ** . Also see : `` xpSpace `` for experiment ** result presentation ** . Modifications to `` list `` : - `` __iadd__ `` ( append ) also for single items ; this is hackey , but convenience is king . - `` append () `` supports `` unique `` to enable lazy xp declaration . - `` __getitem__ `` supports lists . - pretty printing ( using common / distinct attrs ). Add - ons : - `` launch () `` - `` print_averages () `` - `` gen_names () `` - `` inds () `` to search by kw - attrs . \"\"\" def __init__ ( self , * args , unique = False ) : \"\"\"Initialize without args, or with a list of configs. If `` unique `` : duplicates won ' t get appended . This makes `` append () `` ( and `` __iadd__ () `` ) relatively slow . Use `` extend () `` or `` __add__ () `` to bypass this validation . \"\"\" self . unique = unique super (). __init__ ( * args ) def __iadd__ ( self , xp ) : if not hasattr ( xp , ' __iter__ ' ) : xp = [ xp ] for item in xp : self . append ( item ) return self def append ( self , xp ) : \"\"\"Append if not unique & present.\"\"\" if not ( self . unique and xp in self ) : super (). append ( xp ) def __getitem__ ( self , keys ) : \"\"\"Indexing, also by a list\"\"\" try : B = [ self [ k ] for k in keys ] # if keys is list except TypeError : B = super (). __getitem__ ( keys ) # if keys is int , slice if hasattr ( B , ' __len__ ' ) : B = xpList ( B ) # Cast return B def inds ( self , strict = True , missingval = \"NONSENSE\" , ** kws ) : \"\"\"Find (all) indices of configs whose attributes match kws. If strict , then xp ' s lacking a requested attr will not match , unless the missingval ( e . g . None ) matches the required value . \"\"\" def match ( xp ) : def missing ( v ) : return missingval if strict else v matches = [ getattr ( xp , k , missing ( v )) == v for k , v in kws . items ()] return all ( matches ) return [ i for i , xp in enumerate ( self ) if match ( xp )] @ property def da_methods ( self ) : return [ xp . da_method for xp in self ] def split_attrs ( self , nomerge = ()) : \"\"\"Compile the attrs of all xps ; split as distinct , redundant , common . Insert None if an attribute is distinct but not in xp . \"\"\" def _aggregate_keys () : \"Aggregate keys from all xps\" if len ( self ) == 0 : return [] # Start with da_method aggregate = [ ' da_method ' ] # Aggregate all other keys for xp in self : # Get dataclass fields try : dc_fields = dcs . fields ( xp . __class__ ) dc_names = [ F . name for F in dc_fields ] keys = xp . __dict__ . keys () except TypeError : # Assume namedtuple dc_names = [] keys = xp . _fields # For all potential keys: for k in keys : # If not already present: if k not in aggregate : # If dataclass, check repr: if k in dc_names : if dc_fields [ dc_names . index ( k )]. repr : aggregate . append ( k ) # Else, just append else : aggregate . append ( k ) # Remove unwanted excluded = [ re . compile ( ' ^ _ ' ), ' avrgs ' , ' stats ' , ' HMM ' , ' duration ' ] aggregate = dict_tools . complement ( aggregate , excluded ) return aggregate distinct , redundant , common = {}, {}, {} for key in _aggregate_keys () : # Want to distinguish actual None's from empty (\"N/A\"). # => Don't use getattr(obj,key,None) vals = [ getattr ( xp , key , \"N/A\" ) for xp in self ] # Sort (assign dct) into distinct, redundant, common if dict_tools . flexcomp ( key , * nomerge ) : # nomerge => Distinct dct , vals = distinct , vals elif all ( vals [ 0 ] == v for v in vals ) : # all values equal => common dct , vals = common , vals [ 0 ] else : v0 = next ( v for v in vals if \"N/A\" != v ) if all ( v == \"N/A\" or v == v0 for v in vals ) : # all values equal or \"N/A\" => redundant dct , vals = redundant , v0 else : # otherwise => distinct dct , vals = distinct , vals # Replace \"N/A\" by None def sub ( v ) : return None if v == \"N/A\" else v if isinstance ( vals , str ) : vals = sub ( vals ) else : try : vals = [ sub ( v ) for v in vals ] except TypeError : vals = sub ( vals ) dct [ key ] = vals return distinct , redundant , common def __repr__ ( self ) : distinct , redundant , common = self . split_attrs () s = ' < xpList > of length %d with attributes : \\ n ' % len ( self ) s += utils . tab ( distinct , headers = \"keys\" , showindex = True ) s += \" \\n Other attributes: \\n \" s += str ( dict_tools . AlignedDict ({ ** redundant , ** common })) return s def gen_names ( self , abbrev = 6 , tab = False ) : \"\"\"Similiar to ``self.__repr__()``, but: - returns * list * of names - tabulation is optional - attaches ( abbreviated ) labels to each attribute \"\"\" distinct , redundant , common = self . split_attrs ( nomerge = [ \"da_method\" ]) labels = distinct . keys () values = distinct . values () # Label abbreviation labels = [ utils . collapse_str ( k , abbrev ) for k in labels ] # Make label columns: insert None or lbl+\":\", depending on value def column ( lbl , vals ) : return [ None if v is None else lbl + \":\" for v in vals ] labels = [ column ( lbl , vals ) for lbl , vals in zip ( labels , values )] # Interlace labels and values table = [ x for ( a , b ) in zip ( labels , values ) for x in ( a , b )] # Rm da_method label (but keep value) table . pop ( 0 ) # Transpose table = list ( map ( list , zip ( * table ))) # Tabulate table = utils . tab ( table , tablefmt = \"plain\" ) # Rm space between lbls/vals table = re . sub ( ' : + ' , ':' , table ) # Rm alignment if not tab : table = re . sub ( r ' + ' , r ' ' , table ) return table . splitlines () def tabulate_avrgs ( self , * args , ** kwargs ) : \"\"\"Pretty (tabulated) repr of xps & their avrgs. Similar to stats . tabulate_avrgs (), but for the entire list of xps . \"\"\" distinct , redundant , common = self . split_attrs () averages = dapper . stats . tabulate_avrgs ([ C . avrgs for C in self ], * args , ** kwargs ) columns = { ** distinct , '|' : [ '|' ] * len ( self ), ** averages } # merge return utils . tab ( columns , headers = \"keys\" , showindex = True ). replace ( '\u2423' , ' ' ) def launch ( self , HMM , save_as = \"noname\" , mp = False , setup = seed_and_simulate , fail_gently = None , ** kwargs ) : \"\"\"For each xp in self: run_experiment(xp, ...). The results are saved in `` rc . dirs [ ' data ' ] / save_as . stem `` , unless `` save_as `` is False / None . Depending on `` mp `` , run_experiment () is delegated to one of : - caller process ( no parallelisation ) - multiprocessing on this host - GCP ( Google Cloud Computing ) with HTCondor If `` setup == None `` : use `` seed_and_simulate () `` . The kwargs are forwarded to run_experiment (). See `` example_2 . py `` and `` example_3 . py `` for example use . \"\"\" # TODO 2: doc files and code options in mp, e.g # `files` get added to PYTHONPATH and have dir-structure preserved. # Setup: Experiment initialisation. Default: seed_and_simulate(). # Enables setting experiment variables that are not parameters of a da_method. # Collect common args forwarded to run_experiment kwargs [ ' HMM ' ] = HMM kwargs [ \"setup\" ] = setup # Parse mp option if not mp : mp = False elif mp in [ True , \"MP\" ] : mp = dict ( server = \"local\" ) elif isinstance ( mp , int ) : mp = dict ( server = \"local\" , NPROC = mp ) elif mp in [ \"GCP\" , \"Google\" ] : mp = dict ( server = \"GCP\" , files = [], code = \"\" ) assert isinstance ( mp , dict ) # Parse fail_gently if fail_gently is None : if isinstance ( mp , dict ) and mp [ \"server\" ] == \"GCP\" : fail_gently = False # coz cloud processing is entirely de-coupled anyways else : fail_gently = True # True unless otherwise requested kwargs [ \"fail_gently\" ] = fail_gently # Parse save_as if save_as in [ None , False ] : assert not mp , \"Multiprocessing requires saving data.\" # Parallelization w/o storing is possible, especially w/ threads. # But it involves more complicated communication set-up. def xpi_dir ( * args ) : return None else : save_as = rc . dirs . data / Path ( save_as ). stem save_as /= \"run_\" + datetime . now (). strftime ( \"%Y-%m-%d__%H:%M:%S\" ) os . makedirs ( save_as ) print ( f \"Experiment stored at {save_as}\" ) def xpi_dir ( i ) : path = save_as / str ( i ) os . mkdir ( path ) return path # No parallelization if not mp : for ixp , ( xp , label ) in enumerate ( zip ( self , self . gen_names ())) : run_experiment ( xp , label , xpi_dir ( ixp ), ** kwargs ) # Local multiprocessing elif mp [ \"server\" ]. lower () == \"local\" : def run_with_fixed_args ( arg ) : xp , ixp = arg run_experiment ( xp , None , xpi_dir ( ixp ), ** kwargs ) args = zip ( self , range ( len ( self ))) utils . disable_progbar = True utils . disable_user_interaction = True NPROC = mp . get ( \"NPROC\" , None ) # None => mp . cpu_count () from dapper . tools . multiprocessing import mpd # will fail on GCP with mpd . Pool ( NPROC ) as pool : list ( utils . tqdm . tqdm ( pool . imap ( run_with_fixed_args , args ), total = len ( self ), desc = \"Parallel experim's\" , smoothing = 0.1 )) utils . disable_progbar = False utils . disable_user_interaction = False # Google cloud platform, multiprocessing elif mp [ \"server\" ] == \"GCP\" : for ixp , xp in enumerate ( self ) : with open ( xpi_dir ( ixp ) / \"xp.var\" , \"wb\" ) as f : dill . dump ( dict ( xp = xp ), f ) with open ( save_as / \"xp.com\" , \"wb\" ) as f : dill . dump ( kwargs , f ) # mkdir extra_files extra_files = save_as / \"extra_files\" os . mkdir ( extra_files ) # Default files: .py files in sys.path[0] (main script's path) if not mp . get ( \"files\" , []) : # Todo 4: also intersect(..., sys.modules). # Todo 4: use git ls-tree instead? ff = os . listdir ( sys . path [ 0 ]) mp [ \"files\" ] = [ f for f in ff if f . endswith ( \".py\" )] # Copy files into extra_files for f in mp [ \"files\" ] : if isinstance ( f , ( str , Path )) : # Example: f = \"A.py\" path = Path ( sys . path [ 0 ]) / f dst = f else : # instance of tuple ( path , root ) # Example: f = (\"~/E/G/A.py\", \"G\") path , root = f dst = Path ( path ). relative_to ( root ) dst = extra_files / dst os . makedirs ( dst . parent , exist_ok = True ) try : shutil . copytree ( path , dst ) # dir - r except OSError : shutil . copy2 ( path , dst ) # file # Loads PWD/xp_{var,com} and calls run_experiment() with open ( extra_files / \"load_and_run.py\" , \"w\" ) as f : f . write ( dedent ( \"\"\"\\ import dill from dapper . admin import run_experiment # Load with open ( \"xp.com\" , \"rb\" ) as f : com = dill . load ( f ) with open ( \"xp.var\" , \"rb\" ) as f : var = dill . load ( f ) # User-defined code %s # Run result = run_experiment ( var [ ' xp ' ], None , \".\" , ** com ) \"\"\") % dedent(mp[\" code \"])) with open ( extra_files / \"dpr_config.yaml\" , \"w\" ) as f : f . write ( \" \\n \" . join ([ \"data_root: '$cwd'\" , \"liveplotting: no\" , \"welcome_message: no\" ])) submit_job_GCP ( save_as ) return save_as Ancestors (in MRO) builtins.list Instance variables da_methods Methods append def append ( self , xp ) Append if not unique & present. View Source def append ( self , xp ): \"\"\"Append if not unique & present.\"\"\" if not ( self . unique and xp in self ): super (). append ( xp ) clear def clear ( self , / ) Remove all items from list. copy def copy ( self , / ) Return a shallow copy of the list. count def count ( self , value , / ) Return number of occurrences of value. extend def extend ( self , iterable , / ) Extend list by appending elements from the iterable. gen_names def gen_names ( self , abbrev = 6 , tab = False ) Similiar to self.__repr__() , but: returns list of names tabulation is optional attaches (abbreviated) labels to each attribute View Source def gen_names ( self , abbrev = 6 , tab = False ): \"\"\"Similiar to ``self.__repr__()``, but: - returns *list* of names - tabulation is optional - attaches (abbreviated) labels to each attribute \"\"\" distinct , redundant , common = self . split_attrs ( nomerge = [ \"da_method\" ]) labels = distinct . keys () values = distinct . values () # Label abbreviation labels = [ utils . collapse_str ( k , abbrev ) for k in labels ] # Make label columns : insert None or lbl + \":\" , depending on value def column ( lbl , vals ): return [ None if v is None else lbl + \":\" for v in vals ] labels = [ column ( lbl , vals ) for lbl , vals in zip ( labels , values )] # Interlace labels and values table = [ x for ( a , b ) in zip ( labels , values ) for x in ( a , b )] # Rm da_method label ( but keep value ) table . pop ( 0 ) # Transpose table = list ( map ( list , zip ( * table ))) # Tabulate table = utils . tab ( table , tablefmt = \"plain\" ) # Rm space between lbls / vals table = re . sub ( ': +' , ':' , table ) # Rm alignment if not tab : table = re . sub ( r ' +' , r ' ' , table ) return table . splitlines () index def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present. inds def inds ( self , strict = True , missingval = 'NONSENSE' , ** kws ) Find (all) indices of configs whose attributes match kws. If strict, then xp's lacking a requested attr will not match, unless the missingval (e.g. None) matches the required value. View Source def inds ( self , strict = True , missingval = \"NONSENSE\" , ** kws ): \"\"\"Find (all) indices of configs whose attributes match kws. If strict, then xp's lacking a requested attr will not match, unless the missingval (e.g. None) matches the required value. \"\"\" def match ( xp ): def missing ( v ): return missingval if strict else v matches = [ getattr ( xp , k , missing ( v )) == v for k , v in kws . items ()] return all ( matches ) return [ i for i , xp in enumerate ( self ) if match ( xp )] insert def insert ( self , index , object , / ) Insert object before index. launch def launch ( self , HMM , save_as = 'noname' , mp = False , setup =< function seed_and_simulate at 0x7f8a722404c0 > , fail_gently = None , ** kwargs ) For each xp in self: run_experiment(xp, ...). The results are saved in rc.dirs['data']/save_as.stem , unless save_as is False/None. Depending on mp , run_experiment() is delegated to one of: - caller process (no parallelisation) - multiprocessing on this host - GCP (Google Cloud Computing) with HTCondor If setup == None : use seed_and_simulate() . The kwargs are forwarded to run_experiment(). See example_2.py and example_3.py for example use. View Source def launch ( self , HMM , save_as = \"noname\" , mp = False , setup = seed_and_simulate , fail_gently = None , ** kwargs ) : \"\"\"For each xp in self: run_experiment(xp, ...). The results are saved in `` rc . dirs [ ' data ' ] / save_as . stem `` , unless `` save_as `` is False / None . Depending on `` mp `` , run_experiment () is delegated to one of : - caller process ( no parallelisation ) - multiprocessing on this host - GCP ( Google Cloud Computing ) with HTCondor If `` setup == None `` : use `` seed_and_simulate () `` . The kwargs are forwarded to run_experiment (). See `` example_2 . py `` and `` example_3 . py `` for example use . \"\"\" # TODO 2: doc files and code options in mp, e.g # `files` get added to PYTHONPATH and have dir-structure preserved. # Setup: Experiment initialisation. Default: seed_and_simulate(). # Enables setting experiment variables that are not parameters of a da_method. # Collect common args forwarded to run_experiment kwargs [ ' HMM ' ] = HMM kwargs [ \"setup\" ] = setup # Parse mp option if not mp : mp = False elif mp in [ True , \"MP\" ] : mp = dict ( server = \"local\" ) elif isinstance ( mp , int ) : mp = dict ( server = \"local\" , NPROC = mp ) elif mp in [ \"GCP\" , \"Google\" ] : mp = dict ( server = \"GCP\" , files = [], code = \"\" ) assert isinstance ( mp , dict ) # Parse fail_gently if fail_gently is None : if isinstance ( mp , dict ) and mp [ \"server\" ] == \"GCP\" : fail_gently = False # coz cloud processing is entirely de-coupled anyways else : fail_gently = True # True unless otherwise requested kwargs [ \"fail_gently\" ] = fail_gently # Parse save_as if save_as in [ None , False ] : assert not mp , \"Multiprocessing requires saving data.\" # Parallelization w/o storing is possible, especially w/ threads. # But it involves more complicated communication set-up. def xpi_dir ( * args ) : return None else : save_as = rc . dirs . data / Path ( save_as ). stem save_as /= \"run_\" + datetime . now (). strftime ( \"%Y-%m-%d__%H:%M:%S\" ) os . makedirs ( save_as ) print ( f \"Experiment stored at { save_as } \") def xpi_dir ( i ) : path = save_as / str ( i ) os . mkdir ( path ) return path # No parallelization if not mp : for ixp , ( xp , label ) in enumerate ( zip ( self , self . gen_names ())) : run_experiment ( xp , label , xpi_dir ( ixp ), ** kwargs ) # Local multiprocessing elif mp [ \"server\" ]. lower () == \"local\" : def run_with_fixed_args ( arg ) : xp , ixp = arg run_experiment ( xp , None , xpi_dir ( ixp ), ** kwargs ) args = zip ( self , range ( len ( self ))) utils . disable_progbar = True utils . disable_user_interaction = True NPROC = mp . get ( \"NPROC\" , None ) # None => mp . cpu_count () from dapper . tools . multiprocessing import mpd # will fail on GCP with mpd . Pool ( NPROC ) as pool : list ( utils . tqdm . tqdm ( pool . imap ( run_with_fixed_args , args ), total = len ( self ), desc = \"Parallel experim's\" , smoothing = 0.1 )) utils . disable_progbar = False utils . disable_user_interaction = False # Google cloud platform, multiprocessing elif mp [ \"server\" ] == \"GCP\" : for ixp , xp in enumerate ( self ) : with open ( xpi_dir ( ixp ) / \"xp.var\" , \"wb\" ) as f : dill . dump ( dict ( xp = xp ), f ) with open ( save_as / \"xp.com\" , \"wb\" ) as f : dill . dump ( kwargs , f ) # mkdir extra_files extra_files = save_as / \"extra_files\" os . mkdir ( extra_files ) # Default files: .py files in sys.path[0] (main script's path) if not mp . get ( \"files\" , []) : # Todo 4: also intersect(..., sys.modules). # Todo 4: use git ls-tree instead? ff = os . listdir ( sys . path [ 0 ]) mp [ \"files\" ] = [ f for f in ff if f . endswith ( \".py\" )] # Copy files into extra_files for f in mp [ \"files\" ] : if isinstance ( f , ( str , Path )) : # Example: f = \"A.py\" path = Path ( sys . path [ 0 ]) / f dst = f else : # instance of tuple ( path , root ) # Example: f = (\"~/E/G/A.py\", \"G\") path , root = f dst = Path ( path ). relative_to ( root ) dst = extra_files / dst os . makedirs ( dst . parent , exist_ok = True ) try : shutil . copytree ( path , dst ) # dir - r except OSError : shutil . copy2 ( path , dst ) # file # Loads PWD/xp_ { var , com } and calls run_experiment () with open ( extra_files / \"load_and_run.py\" , \"w\" ) as f : f . write ( dedent ( \"\"\"\\ import dill from dapper . admin import run_experiment # Load with open ( \"xp.com\" , \"rb\" ) as f : com = dill . load ( f ) with open ( \"xp.var\" , \"rb\" ) as f : var = dill . load ( f ) # User-defined code %s # Run result = run_experiment ( var [ ' xp ' ], None , \".\" , ** com ) \"\"\") % dedent(mp[\" code \"])) with open ( extra_files / \"dpr_config.yaml\" , \"w\" ) as f : f . write ( \" \\n \" . join ([ \"data_root: '$cwd'\" , \"liveplotting: no\" , \"welcome_message: no\" ])) submit_job_GCP ( save_as ) return save_as pop def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range. remove def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present. reverse def reverse ( self , / ) Reverse IN PLACE . sort def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order. split_attrs def split_attrs ( self , nomerge = () ) Compile the attrs of all xps; split as distinct, redundant, common. Insert None if an attribute is distinct but not in xp. View Source def split_attrs ( self , nomerge = ()) : \"\"\"Compile the attrs of all xps; split as distinct, redundant, common. Insert None if an attribute is distinct but not in xp.\"\"\" def _aggregate_keys () : \"Aggregate keys from all xps\" if len ( self ) == 0 : return [] # Start with da_method aggregate = [ 'da_method' ] # Aggregate all other keys for xp in self : # Get dataclass fields try : dc_fields = dcs . fields ( xp . __class__ ) dc_names = [ F.name for F in dc_fields ] keys = xp . __dict__ . keys () except TypeError : # Assume namedtuple dc_names = [] keys = xp . _fields # For all potential keys : for k in keys : # If not already present : if k not in aggregate : # If dataclass , check repr : if k in dc_names : if dc_fields [ dc_names.index(k) ] . repr : aggregate . append ( k ) # Else , just append else : aggregate . append ( k ) # Remove unwanted excluded = [ re.compile('^_'), 'avrgs', 'stats', 'HMM', 'duration' ] aggregate = dict_tools . complement ( aggregate , excluded ) return aggregate distinct , redundant , common = {} , {} , {} for key in _aggregate_keys () : # Want to distinguish actual None 's from empty (\"N/A\"). # => Don' t use getattr ( obj , key , None ) vals = [ getattr(xp, key, \"N/A\") for xp in self ] # Sort ( assign dct ) into distinct , redundant , common if dict_tools . flexcomp ( key , * nomerge ) : # nomerge => Distinct dct , vals = distinct , vals elif all ( vals [ 0 ] == v for v in vals ) : # all values equal => common dct , vals = common , vals [ 0 ] else : v0 = next ( v for v in vals if \"N/A\" != v ) if all ( v == \"N/A\" or v == v0 for v in vals ) : # all values equal or \"N/A\" => redundant dct , vals = redundant , v0 else : # otherwise => distinct dct , vals = distinct , vals # Replace \"N/A\" by None def sub ( v ) : return None if v == \"N/A\" else v if isinstance ( vals , str ) : vals = sub ( vals ) else : try : vals = [ sub(v) for v in vals ] except TypeError : vals = sub ( vals ) dct [ key ] = vals return distinct , redundant , common tabulate_avrgs def tabulate_avrgs ( self , * args , ** kwargs ) Pretty (tabulated) repr of xps & their avrgs. Similar to stats.tabulate_avrgs(), but for the entire list of xps. View Source def tabulate_avrgs ( self , * args , ** kwargs ): \"\"\"Pretty (tabulated) repr of xps & their avrgs. Similar to stats.tabulate_avrgs(), but for the entire list of xps.\"\"\" distinct , redundant , common = self . split_attrs () averages = dapper . stats . tabulate_avrgs ([ C . avrgs for C in self ], * args , ** kwargs ) columns = { ** distinct , '|' : [ '|' ] * len ( self ), ** averages } # merge return utils . tab ( columns , headers = \"keys\" , showindex = True ). replace ( '\u2423' , ' ' )","title":"Admin"},{"location":"reference/dapper/admin/#module-dapperadmin","text":"Define high-level API in DAPPER. Used for experiment (xp) specification/administration, including: da_method decorator xpList save_data run_experiment run_from_file HiddenMarkovModel Operator View Source \"\"\"Define high-level API in DAPPER. Used for experiment (xp) specification/administration, including: - da_method decorator - xpList - save_data - run_experiment - run_from_file - HiddenMarkovModel - Operator \"\"\" import dapper.stats import dapper.dict_tools as dict_tools from dapper.tools.chronos import Chronology from dapper.tools.randvars import RV , GaussRV from dapper.tools.stoch import set_seed from dapper.dpr_config import rc from dapper.tools.remote.uplink import submit_job_GCP import dapper.tools.utils as utils from dapper.tools.localization import no_localization from dapper.tools.math import Id_op , Id_mat from pathlib import Path import dataclasses as dcs import copy from textwrap import dedent import os import sys import re import time import numpy as np import inspect import functools import dill import shutil from datetime import datetime class HiddenMarkovModel ( dict_tools . NicePrint ): \"\"\"Container for attributes of a Hidden Markov Model (HMM). This container contains the specification of a \"twin experiment\", i.e. an \"OSSE (observing system simulation experiment)\". \"\"\" def __init__ ( self , Dyn , Obs , t , X0 , ** kwargs ): # fmt: off self . Dyn = Dyn if isinstance ( Dyn , Operator ) else Operator ( ** Dyn ) # noqa self . Obs = Obs if isinstance ( Obs , Operator ) else Operator ( ** Obs ) # noqa self . t = t if isinstance ( t , Chronology ) else Chronology ( ** t ) # noqa self . X0 = X0 if isinstance ( X0 , RV ) else RV ( ** X0 ) # noqa # fmt: on # Name self . name = kwargs . pop ( \"name\" , \"\" ) if not self . name : name = inspect . getfile ( inspect . stack ()[ 1 ][ 0 ]) try : self . name = str ( Path ( name ) . relative_to ( rc . dirs . dapper / 'mods' )) except ValueError : self . name = str ( Path ( name )) # Kwargs abbrevs = { 'LP' : 'liveplotters' } for key in kwargs : setattr ( self , abbrevs . get ( key , key ), kwargs [ key ]) # Defaults if not hasattr ( self . Obs , \"localizer\" ): self . Obs . localizer = no_localization ( self . Nx , self . Ny ) if not hasattr ( self , \"sectors\" ): self . sectors = {} # Validation if self . Obs . noise . C == 0 or self . Obs . noise . C . rk != self . Obs . noise . C . M : raise ValueError ( \"Rank-deficient R not supported.\" ) # ndim shortcuts @property def Nx ( self ): return self . Dyn . M @property def Ny ( self ): return self . Obs . M printopts = { 'ordering' : [ 'Dyn' , 'Obs' , 't' , 'X0' ]} def simulate ( self , desc = 'Truth & Obs' ): \"\"\"Generate synthetic truth and observations.\"\"\" Dyn , Obs , chrono , X0 = self . Dyn , self . Obs , self . t , self . X0 # Init xx = np . zeros (( chrono . K + 1 , Dyn . M )) yy = np . zeros (( chrono . KObs + 1 , Obs . M )) xx [ 0 ] = X0 . sample ( 1 ) # Loop for k , kObs , t , dt in utils . progbar ( chrono . ticker , desc ): xx [ k ] = Dyn ( xx [ k - 1 ], t - dt , dt ) + np . sqrt ( dt ) * Dyn . noise . sample ( 1 ) if kObs is not None : yy [ kObs ] = Obs ( xx [ k ], t ) + Obs . noise . sample ( 1 ) return xx , yy class Operator ( dict_tools . NicePrint ): \"\"\"Container for operators (models).\"\"\" def __init__ ( self , M , model = None , noise = None , ** kwargs ): self . M = M # None => Identity model if model is None : model = Id_op () kwargs [ 'linear' ] = lambda x , t , dt : Id_mat ( M ) self . model = model # None/0 => No noise if isinstance ( noise , RV ): self . noise = noise else : if noise is None : noise = 0 if np . isscalar ( noise ): self . noise = GaussRV ( C = noise , M = M ) else : self . noise = GaussRV ( C = noise ) # Write attributes for key , value in kwargs . items (): setattr ( self , key , value ) def __call__ ( self , * args , ** kwargs ): return self . model ( * args , ** kwargs ) printopts = { 'ordering' : [ 'M' , 'model' , 'noise' ]} def da_method ( * default_dataclasses ): \"\"\"Make the decorator that makes the DA classes. Example: >>> @da_method() >>> class Sleeper(): >>> \"Do nothing.\" >>> seconds : int = 10 >>> success : bool = True >>> def assimilate(self,*args,**kwargs): >>> for k in utils.progbar(range(self.seconds)): >>> time.sleep(1) >>> if not self.success: >>> raise RuntimeError(\"Sleep over. Failing as intended.\") Example: >>> @dcs.dataclass >>> class ens_defaults: >>> infl : float = 1.0 >>> rot : bool = False >>> >>> @da_method(ens_defaults) >>> class EnKF: >>> N : int >>> upd_a : str = \"Sqrt\" >>> >>> def assimilate(self,HMM,xx,yy): >>> ... \"\"\" def dataclass_with_defaults ( cls ): \"\"\"Decorator based on dataclass. This adds __init__, __repr__, __eq__, ..., but also includes inherited defaults (see https://stackoverflow.com/a/58130805 ). Also: - Wraps assimilate() to provide gentle_fail functionality. - Initialises and writes the Stats object.\"\"\" # Default fields invovle: (1) annotations and (2) attributes. def set_field ( name , type , val ): if not hasattr ( cls , '__annotations__' ): cls . __annotations__ = {} cls . __annotations__ [ name ] = type if not isinstance ( val , dcs . Field ): val = dcs . field ( default = val ) setattr ( cls , name , val ) # APPend default fields without overwriting. # Don't implement (by PREpending?) non-default args -- to messy! for D in default_dataclasses : # NB: Calling dataclass twice always makes repr=True, so avoid this. for F in dcs . fields ( dcs . dataclass ( D )): if F . name not in cls . __annotations__ : set_field ( F . name , F . type , F ) # Create new class (NB: old/new classes have same id) cls = dcs . dataclass ( cls ) # Shortcut for self.__class__.__name__ cls . da_method = cls . __name__ def assimilate ( self , HMM , xx , yy , desc = None , ** stat_kwargs ): # Progressbar name pb_name_hook = self . da_method if desc is None else desc # noqa # Init stats self . stats = dapper . stats . Stats ( self , HMM , xx , yy , ** stat_kwargs ) # Assimilate time_start = time . time () old_assimilate ( self , HMM , xx , yy ) dapper . stats . register_stat ( self . stats , \"duration\" , time . time () - time_start ) old_assimilate = cls . assimilate cls . assimilate = functools . wraps ( old_assimilate )( assimilate ) return cls return dataclass_with_defaults def seed_and_simulate ( HMM , xp ): \"\"\"Default experiment setup. Set seed and simulate truth and obs. Note: if there is no ``xp.seed`` then then the seed is not set. Thus, different experiments will produce different truth and obs.\"\"\" set_seed ( getattr ( xp , 'seed' , False )) xx , yy = HMM . simulate () return xx , yy def run_experiment ( xp , label , savedir , HMM , setup = None , free = True , statkeys = False , fail_gently = False , ** stat_kwargs ): \"\"\"Used by xpList.launch() to run each single experiment. This involves steps similar to ``example_1.py``, i.e.: - setup() : Call function given by user. Should set params, eg HMM.Force, seed, and return (simulated/loaded) truth and obs series. - xp.assimilate() : run DA, pass on exception if fail_gently - xp.stats.average_in_time() : result averaging - xp.avrgs.tabulate() : result printing - dill.dump() : result storage \"\"\" # We should copy HMM so as not to cause any nasty surprises such as # expecting param=1 when param=2 (coz it's not been reset). # NB: won't copy implicitly ref'd obj's (like L96's core). => bug w/ MP? hmm = copy . deepcopy ( HMM ) # GENERATE TRUTH/OBS xx , yy = setup ( hmm , xp ) # ASSIMILATE try : xp . assimilate ( hmm , xx , yy , label , ** stat_kwargs ) except Exception as ERR : if fail_gently : xp . crashed = True if fail_gently not in [ \"silent\" , \"quiet\" ]: utils . print_cropped_traceback ( ERR ) else : raise ERR # AVERAGE xp . stats . average_in_time ( free = free ) # PRINT if statkeys : statkeys = () if statkeys is True else statkeys print ( xp . avrgs . tabulate ( statkeys )) # SAVE if savedir : with open ( Path ( savedir ) / \"xp\" , \"wb\" ) as FILE : dill . dump ({ 'xp' : xp }, FILE ) # TODO 2: check collections.userlist # TODO 2: __add__ vs __iadd__ class xpList ( list ): \"\"\"List, subclassed for holding experiment (\"xp\") objects. Main use: administrate experiment **launches**. Also see: ``xpSpace`` for experiment **result presentation**. Modifications to ``list``: - ``__iadd__`` (append) also for single items; this is hackey, but convenience is king. - ``append()`` supports ``unique`` to enable lazy xp declaration. - ``__getitem__`` supports lists. - pretty printing (using common/distinct attrs). Add-ons: - ``launch()`` - ``print_averages()`` - ``gen_names()`` - ``inds()`` to search by kw-attrs. \"\"\" def __init__ ( self , * args , unique = False ): \"\"\"Initialize without args, or with a list of configs. If ``unique``: duplicates won't get appended. This makes ``append()`` (and ``__iadd__()``) relatively slow. Use ``extend()`` or ``__add__()`` to bypass this validation.\"\"\" self . unique = unique super () . __init__ ( * args ) def __iadd__ ( self , xp ): if not hasattr ( xp , '__iter__' ): xp = [ xp ] for item in xp : self . append ( item ) return self def append ( self , xp ): \"\"\"Append if not unique & present.\"\"\" if not ( self . unique and xp in self ): super () . append ( xp ) def __getitem__ ( self , keys ): \"\"\"Indexing, also by a list\"\"\" try : B = [ self [ k ] for k in keys ] # if keys is list except TypeError : B = super () . __getitem__ ( keys ) # if keys is int, slice if hasattr ( B , '__len__' ): B = xpList ( B ) # Cast return B def inds ( self , strict = True , missingval = \"NONSENSE\" , ** kws ): \"\"\"Find (all) indices of configs whose attributes match kws. If strict, then xp's lacking a requested attr will not match, unless the missingval (e.g. None) matches the required value. \"\"\" def match ( xp ): def missing ( v ): return missingval if strict else v matches = [ getattr ( xp , k , missing ( v )) == v for k , v in kws . items ()] return all ( matches ) return [ i for i , xp in enumerate ( self ) if match ( xp )] @property def da_methods ( self ): return [ xp . da_method for xp in self ] def split_attrs ( self , nomerge = ()): \"\"\"Compile the attrs of all xps; split as distinct, redundant, common. Insert None if an attribute is distinct but not in xp.\"\"\" def _aggregate_keys (): \"Aggregate keys from all xps\" if len ( self ) == 0 : return [] # Start with da_method aggregate = [ 'da_method' ] # Aggregate all other keys for xp in self : # Get dataclass fields try : dc_fields = dcs . fields ( xp . __class__ ) dc_names = [ F . name for F in dc_fields ] keys = xp . __dict__ . keys () except TypeError : # Assume namedtuple dc_names = [] keys = xp . _fields # For all potential keys: for k in keys : # If not already present: if k not in aggregate : # If dataclass, check repr: if k in dc_names : if dc_fields [ dc_names . index ( k )] . repr : aggregate . append ( k ) # Else, just append else : aggregate . append ( k ) # Remove unwanted excluded = [ re . compile ( '^_' ), 'avrgs' , 'stats' , 'HMM' , 'duration' ] aggregate = dict_tools . complement ( aggregate , excluded ) return aggregate distinct , redundant , common = {}, {}, {} for key in _aggregate_keys (): # Want to distinguish actual None's from empty (\"N/A\"). # => Don't use getattr(obj,key,None) vals = [ getattr ( xp , key , \"N/A\" ) for xp in self ] # Sort (assign dct) into distinct, redundant, common if dict_tools . flexcomp ( key , * nomerge ): # nomerge => Distinct dct , vals = distinct , vals elif all ( vals [ 0 ] == v for v in vals ): # all values equal => common dct , vals = common , vals [ 0 ] else : v0 = next ( v for v in vals if \"N/A\" != v ) if all ( v == \"N/A\" or v == v0 for v in vals ): # all values equal or \"N/A\" => redundant dct , vals = redundant , v0 else : # otherwise => distinct dct , vals = distinct , vals # Replace \"N/A\" by None def sub ( v ): return None if v == \"N/A\" else v if isinstance ( vals , str ): vals = sub ( vals ) else : try : vals = [ sub ( v ) for v in vals ] except TypeError : vals = sub ( vals ) dct [ key ] = vals return distinct , redundant , common def __repr__ ( self ): distinct , redundant , common = self . split_attrs () s = '<xpList> of length %d with attributes: \\n ' % len ( self ) s += utils . tab ( distinct , headers = \"keys\" , showindex = True ) s += \" \\n Other attributes: \\n \" s += str ( dict_tools . AlignedDict ({ ** redundant , ** common })) return s def gen_names ( self , abbrev = 6 , tab = False ): \"\"\"Similiar to ``self.__repr__()``, but: - returns *list* of names - tabulation is optional - attaches (abbreviated) labels to each attribute \"\"\" distinct , redundant , common = self . split_attrs ( nomerge = [ \"da_method\" ]) labels = distinct . keys () values = distinct . values () # Label abbreviation labels = [ utils . collapse_str ( k , abbrev ) for k in labels ] # Make label columns: insert None or lbl+\":\", depending on value def column ( lbl , vals ): return [ None if v is None else lbl + \":\" for v in vals ] labels = [ column ( lbl , vals ) for lbl , vals in zip ( labels , values )] # Interlace labels and values table = [ x for ( a , b ) in zip ( labels , values ) for x in ( a , b )] # Rm da_method label (but keep value) table . pop ( 0 ) # Transpose table = list ( map ( list , zip ( * table ))) # Tabulate table = utils . tab ( table , tablefmt = \"plain\" ) # Rm space between lbls/vals table = re . sub ( ': +' , ':' , table ) # Rm alignment if not tab : table = re . sub ( r ' +' , r ' ' , table ) return table . splitlines () def tabulate_avrgs ( self , * args , ** kwargs ): \"\"\"Pretty (tabulated) repr of xps & their avrgs. Similar to stats.tabulate_avrgs(), but for the entire list of xps.\"\"\" distinct , redundant , common = self . split_attrs () averages = dapper . stats . tabulate_avrgs ([ C . avrgs for C in self ], * args , ** kwargs ) columns = { ** distinct , '|' : [ '|' ] * len ( self ), ** averages } # merge return utils . tab ( columns , headers = \"keys\" , showindex = True ) . replace ( '\u2423' , ' ' ) def launch ( self , HMM , save_as = \"noname\" , mp = False , setup = seed_and_simulate , fail_gently = None , ** kwargs ): \"\"\"For each xp in self: run_experiment(xp, ...). The results are saved in ``rc.dirs['data']/save_as.stem``, unless ``save_as`` is False/None. Depending on ``mp``, run_experiment() is delegated to one of: - caller process (no parallelisation) - multiprocessing on this host - GCP (Google Cloud Computing) with HTCondor If ``setup == None``: use ``seed_and_simulate()``. The kwargs are forwarded to run_experiment(). See ``example_2.py`` and ``example_3.py`` for example use. \"\"\" # TODO 2: doc files and code options in mp, e.g # `files` get added to PYTHONPATH and have dir-structure preserved. # Setup: Experiment initialisation. Default: seed_and_simulate(). # Enables setting experiment variables that are not parameters of a da_method. # Collect common args forwarded to run_experiment kwargs [ 'HMM' ] = HMM kwargs [ \"setup\" ] = setup # Parse mp option if not mp : mp = False elif mp in [ True , \"MP\" ]: mp = dict ( server = \"local\" ) elif isinstance ( mp , int ): mp = dict ( server = \"local\" , NPROC = mp ) elif mp in [ \"GCP\" , \"Google\" ]: mp = dict ( server = \"GCP\" , files = [], code = \"\" ) assert isinstance ( mp , dict ) # Parse fail_gently if fail_gently is None : if isinstance ( mp , dict ) and mp [ \"server\" ] == \"GCP\" : fail_gently = False # coz cloud processing is entirely de-coupled anyways else : fail_gently = True # True unless otherwise requested kwargs [ \"fail_gently\" ] = fail_gently # Parse save_as if save_as in [ None , False ]: assert not mp , \"Multiprocessing requires saving data.\" # Parallelization w/o storing is possible, especially w/ threads. # But it involves more complicated communication set-up. def xpi_dir ( * args ): return None else : save_as = rc . dirs . data / Path ( save_as ) . stem save_as /= \"run_\" + datetime . now () . strftime ( \"%Y-%m- %d __%H:%M:%S\" ) os . makedirs ( save_as ) print ( f \"Experiment stored at { save_as } \" ) def xpi_dir ( i ): path = save_as / str ( i ) os . mkdir ( path ) return path # No parallelization if not mp : for ixp , ( xp , label ) in enumerate ( zip ( self , self . gen_names ())): run_experiment ( xp , label , xpi_dir ( ixp ), ** kwargs ) # Local multiprocessing elif mp [ \"server\" ] . lower () == \"local\" : def run_with_fixed_args ( arg ): xp , ixp = arg run_experiment ( xp , None , xpi_dir ( ixp ), ** kwargs ) args = zip ( self , range ( len ( self ))) utils . disable_progbar = True utils . disable_user_interaction = True NPROC = mp . get ( \"NPROC\" , None ) # None => mp.cpu_count() from dapper.tools.multiprocessing import mpd # will fail on GCP with mpd . Pool ( NPROC ) as pool : list ( utils . tqdm . tqdm ( pool . imap ( run_with_fixed_args , args ), total = len ( self ), desc = \"Parallel experim's\" , smoothing = 0.1 )) utils . disable_progbar = False utils . disable_user_interaction = False # Google cloud platform, multiprocessing elif mp [ \"server\" ] == \"GCP\" : for ixp , xp in enumerate ( self ): with open ( xpi_dir ( ixp ) / \"xp.var\" , \"wb\" ) as f : dill . dump ( dict ( xp = xp ), f ) with open ( save_as / \"xp.com\" , \"wb\" ) as f : dill . dump ( kwargs , f ) # mkdir extra_files extra_files = save_as / \"extra_files\" os . mkdir ( extra_files ) # Default files: .py files in sys.path[0] (main script's path) if not mp . get ( \"files\" , []): # Todo 4: also intersect(..., sys.modules). # Todo 4: use git ls-tree instead? ff = os . listdir ( sys . path [ 0 ]) mp [ \"files\" ] = [ f for f in ff if f . endswith ( \".py\" )] # Copy files into extra_files for f in mp [ \"files\" ]: if isinstance ( f , ( str , Path )): # Example: f = \"A.py\" path = Path ( sys . path [ 0 ]) / f dst = f else : # instance of tuple(path, root) # Example: f = (\"~/E/G/A.py\", \"G\") path , root = f dst = Path ( path ) . relative_to ( root ) dst = extra_files / dst os . makedirs ( dst . parent , exist_ok = True ) try : shutil . copytree ( path , dst ) # dir -r except OSError : shutil . copy2 ( path , dst ) # file # Loads PWD/xp_{var,com} and calls run_experiment() with open ( extra_files / \"load_and_run.py\" , \"w\" ) as f : f . write ( dedent ( \"\"\" \\ import dill from dapper.admin import run_experiment # Load with open(\"xp.com\", \"rb\") as f: com = dill.load(f) with open(\"xp.var\", \"rb\") as f: var = dill.load(f) # User-defined code %s # Run result = run_experiment(var['xp'], None, \".\", **com) \"\"\" ) % dedent ( mp [ \"code\" ])) with open ( extra_files / \"dpr_config.yaml\" , \"w\" ) as f : f . write ( \" \\n \" . join ([ \"data_root: '$cwd'\" , \"liveplotting: no\" , \"welcome_message: no\" ])) submit_job_GCP ( save_as ) return save_as def get_param_setter ( param_dict , ** glob_dict ): \"\"\"Mass creation of xp's by combining the value lists in the parameter dicts. The parameters are trimmed to the ones available for the given method. This is a good deal more efficient than relying on xpList's unique=True. Beware! If, eg., [infl,rot] are in the param_dict, aimed at the EnKF, but you forget that they are also attributes some method where you don't actually want to use them (eg. SVGDF), then you'll create many more than you intend. \"\"\" def for_params ( method , ** fixed_params ): dc_fields = [ f . name for f in dcs . fields ( method )] params = dict_tools . intersect ( param_dict , dc_fields ) params = dict_tools . complement ( params , fixed_params ) params = { ** glob_dict , ** params } # glob_dict 1st def xp1 ( dct ): xp = method ( ** dict_tools . intersect ( dct , dc_fields ), ** fixed_params ) for key , v in dict_tools . intersect ( dct , glob_dict ) . items (): setattr ( xp , key , v ) return xp return [ xp1 ( dct ) for dct in dict_tools . prodct ( params )] return for_params","title":"Module dapper.admin"},{"location":"reference/dapper/admin/#functions","text":"","title":"Functions"},{"location":"reference/dapper/admin/#da_method","text":"def da_method ( * default_dataclasses ) Make the decorator that makes the DA classes. Example: @da_method() class Sleeper(): \"Do nothing.\" seconds : int = 10 success : bool = True def assimilate(self, args, *kwargs): for k in utils.progbar(range(self.seconds)): time.sleep(1) if not self.success: raise RuntimeError(\"Sleep over. Failing as intended.\") Example: @dcs.dataclass class ens_defaults: infl : float = 1.0 rot : bool = False @da_method(ens_defaults) class EnKF: N : int upd_a : str = \"Sqrt\" def assimilate(self,HMM,xx,yy): ... View Source def da_method ( * default_dataclasses ) : \"\"\"Make the decorator that makes the DA classes. Example: >>> @da_method() >>> class Sleeper(): >>> \" Do nothing . \" >>> seconds : int = 10 >>> success : bool = True >>> def assimilate(self,*args,**kwargs): >>> for k in utils.progbar(range(self.seconds)): >>> time.sleep(1) >>> if not self.success: >>> raise RuntimeError(\" Sleep over . Failing as intended . \") Example: >>> @dcs.dataclass >>> class ens_defaults: >>> infl : float = 1.0 >>> rot : bool = False >>> >>> @da_method(ens_defaults) >>> class EnKF: >>> N : int >>> upd_a : str = \" Sqrt \" >>> >>> def assimilate(self,HMM,xx,yy): >>> ... \"\"\" def dataclass_with_defaults ( cls ) : \"\"\"Decorator based on dataclass. This adds __init__, __repr__, __eq__, ..., but also includes inherited defaults (see https://stackoverflow.com/a/58130805 ). Also: - Wraps assimilate() to provide gentle_fail functionality. - Initialises and writes the Stats object.\"\"\" # Default fields invovle : ( 1 ) annotations and ( 2 ) attributes . def set_field ( name , type , val ) : if not hasattr ( cls , '__annotations__' ) : cls . __annotations__ = {} cls . __annotations__ [ name ] = type if not isinstance ( val , dcs . Field ) : val = dcs . field ( default = val ) setattr ( cls , name , val ) # APPend default fields without overwriting . # Don ' t implement ( by PREpending ? ) non - default args -- to messy ! for D in default_dataclasses : # NB : Calling dataclass twice always makes repr = True , so avoid this . for F in dcs . fields ( dcs . dataclass ( D )) : if F . name not in cls . __annotations__ : set_field ( F . name , F . type , F ) # Create new class ( NB : old / new classes have same id ) cls = dcs . dataclass ( cls ) # Shortcut for self . __class__ . __name__ cls . da_method = cls . __name__ def assimilate ( self , HMM , xx , yy , desc = None , ** stat_kwargs ) : # Progressbar name pb_name_hook = self . da_method if desc is None else desc # noqa # Init stats self . stats = dapper . stats . Stats ( self , HMM , xx , yy , ** stat_kwargs ) # Assimilate time_start = time . time () old_assimilate ( self , HMM , xx , yy ) dapper . stats . register_stat ( self . stats , \"duration\" , time . time () - time_start ) old_assimilate = cls . assimilate cls . assimilate = functools . wraps ( old_assimilate )( assimilate ) return cls return dataclass_with_defaults","title":"da_method"},{"location":"reference/dapper/admin/#get_param_setter","text":"def get_param_setter ( param_dict , ** glob_dict ) Mass creation of xp's by combining the value lists in the parameter dicts. The parameters are trimmed to the ones available for the given method. This is a good deal more efficient than relying on xpList's unique=True. Beware! If, eg., [infl,rot] are in the param_dict, aimed at the EnKF, but you forget that they are also attributes some method where you don't actually want to use them (eg. SVGDF), then you'll create many more than you intend. View Source def get_param_setter ( param_dict , ** glob_dict ): \"\"\"Mass creation of xp's by combining the value lists in the parameter dicts. The parameters are trimmed to the ones available for the given method. This is a good deal more efficient than relying on xpList's unique=True. Beware! If, eg., [infl,rot] are in the param_dict, aimed at the EnKF, but you forget that they are also attributes some method where you don't actually want to use them (eg. SVGDF), then you'll create many more than you intend. \"\"\" def for_params ( method , ** fixed_params ): dc_fields = [ f . name for f in dcs . fields ( method )] params = dict_tools . intersect ( param_dict , dc_fields ) params = dict_tools . complement ( params , fixed_params ) params = { ** glob_dict , ** params } # glob_dict 1st def xp1 ( dct ): xp = method ( ** dict_tools . intersect ( dct , dc_fields ), ** fixed_params ) for key , v in dict_tools . intersect ( dct , glob_dict ) . items (): setattr ( xp , key , v ) return xp return [ xp1 ( dct ) for dct in dict_tools . prodct ( params )] return for_params","title":"get_param_setter"},{"location":"reference/dapper/admin/#run_experiment","text":"def run_experiment ( xp , label , savedir , HMM , setup = None , free = True , statkeys = False , fail_gently = False , ** stat_kwargs ) Used by xpList.launch() to run each single experiment. This involves steps similar to example_1.py , i.e.: setup() : Call function given by user. Should set params, eg HMM.Force, seed, and return (simulated/loaded) truth and obs series. xp.assimilate() : run DA, pass on exception if fail_gently xp.stats.average_in_time() : result averaging xp.avrgs.tabulate() : result printing dill.dump() : result storage View Source def run_experiment ( xp , label , savedir , HMM , setup = None , free = True , statkeys = False , fail_gently = False , ** stat_kwargs ): \"\"\"Used by xpList.launch() to run each single experiment. This involves steps similar to ``example_1.py``, i.e.: - setup() : Call function given by user. Should set params, eg HMM.Force, seed, and return (simulated/loaded) truth and obs series. - xp.assimilate() : run DA, pass on exception if fail_gently - xp.stats.average_in_time() : result averaging - xp.avrgs.tabulate() : result printing - dill.dump() : result storage \"\"\" # We should copy HMM so as not to cause any nasty surprises such as # expecting param=1 when param=2 (coz it's not been reset). # NB: won't copy implicitly ref'd obj's (like L96's core). => bug w/ MP? hmm = copy . deepcopy ( HMM ) # GENERATE TRUTH/OBS xx , yy = setup ( hmm , xp ) # ASSIMILATE try : xp . assimilate ( hmm , xx , yy , label , ** stat_kwargs ) except Exception as ERR : if fail_gently : xp . crashed = True if fail_gently not in [ \"silent\" , \"quiet\" ]: utils . print_cropped_traceback ( ERR ) else : raise ERR # AVERAGE xp . stats . average_in_time ( free = free ) # PRINT if statkeys : statkeys = () if statkeys is True else statkeys print ( xp . avrgs . tabulate ( statkeys )) # SAVE if savedir : with open ( Path ( savedir ) / \"xp\" , \"wb\" ) as FILE : dill . dump ({ 'xp' : xp }, FILE )","title":"run_experiment"},{"location":"reference/dapper/admin/#seed_and_simulate","text":"def seed_and_simulate ( HMM , xp ) Default experiment setup. Set seed and simulate truth and obs. Note: if there is no xp.seed then then the seed is not set. Thus, different experiments will produce different truth and obs. View Source def seed_and_simulate ( HMM , xp ): \"\"\"Default experiment setup. Set seed and simulate truth and obs. Note: if there is no ``xp.seed`` then then the seed is not set. Thus, different experiments will produce different truth and obs.\"\"\" set_seed ( getattr ( xp , 'seed' , False )) xx , yy = HMM . simulate () return xx , yy","title":"seed_and_simulate"},{"location":"reference/dapper/admin/#classes","text":"","title":"Classes"},{"location":"reference/dapper/admin/#hiddenmarkovmodel","text":"class HiddenMarkovModel ( Dyn , Obs , t , X0 , ** kwargs ) Container for attributes of a Hidden Markov Model (HMM). This container contains the specification of a \"twin experiment\", i.e. an \"OSSE (observing system simulation experiment)\". View Source class HiddenMarkovModel ( dict_tools . NicePrint ) : \"\"\"Container for attributes of a Hidden Markov Model (HMM). This container contains the specification of a \" twin experiment \", i.e. an \" OSSE ( observing system simulation experiment ) \". \"\"\" def __init__ ( self , Dyn , Obs , t , X0 , ** kwargs ) : # fmt : off self . Dyn = Dyn if isinstance ( Dyn , Operator ) else Operator ( ** Dyn ) # noqa self . Obs = Obs if isinstance ( Obs , Operator ) else Operator ( ** Obs ) # noqa self . t = t if isinstance ( t , Chronology ) else Chronology ( ** t ) # noqa self . X0 = X0 if isinstance ( X0 , RV ) else RV ( ** X0 ) # noqa # fmt : on # Name self . name = kwargs . pop ( \"name\" , \"\" ) if not self . name : name = inspect . getfile ( inspect . stack () [ 1 ][ 0 ] ) try : self . name = str ( Path ( name ). relative_to ( rc . dirs . dapper / 'mods' )) except ValueError : self . name = str ( Path ( name )) # Kwargs abbrevs = { 'LP' : 'liveplotters' } for key in kwargs : setattr ( self , abbrevs . get ( key , key ), kwargs [ key ] ) # Defaults if not hasattr ( self . Obs , \"localizer\" ) : self . Obs . localizer = no_localization ( self . Nx , self . Ny ) if not hasattr ( self , \"sectors\" ) : self . sectors = {} # Validation if self . Obs . noise . C == 0 or self . Obs . noise . C . rk != self . Obs . noise . C . M : raise ValueError ( \"Rank-deficient R not supported.\" ) # ndim shortcuts @property def Nx ( self ) : return self . Dyn . M @property def Ny ( self ) : return self . Obs . M printopts = { 'ordering' : [ 'Dyn', 'Obs', 't', 'X0' ] } def simulate ( self , desc = 'Truth & Obs' ) : \"\"\"Generate synthetic truth and observations.\"\"\" Dyn , Obs , chrono , X0 = self . Dyn , self . Obs , self . t , self . X0 # Init xx = np . zeros (( chrono . K + 1 , Dyn . M )) yy = np . zeros (( chrono . KObs + 1 , Obs . M )) xx [ 0 ] = X0 . sample ( 1 ) # Loop for k , kObs , t , dt in utils . progbar ( chrono . ticker , desc ) : xx [ k ] = Dyn ( xx [ k-1 ] , t - dt , dt ) + np . sqrt ( dt ) * Dyn . noise . sample ( 1 ) if kObs is not None : yy [ kObs ] = Obs ( xx [ k ] , t ) + Obs . noise . sample ( 1 ) return xx , yy","title":"HiddenMarkovModel"},{"location":"reference/dapper/admin/#ancestors-in-mro","text":"dapper.dict_tools.NicePrint","title":"Ancestors (in MRO)"},{"location":"reference/dapper/admin/#class-variables","text":"printopts","title":"Class variables"},{"location":"reference/dapper/admin/#instance-variables","text":"Nx Ny","title":"Instance variables"},{"location":"reference/dapper/admin/#methods","text":"","title":"Methods"},{"location":"reference/dapper/admin/#simulate","text":"def simulate ( self , desc = 'Truth & Obs' ) Generate synthetic truth and observations. View Source def simulate ( self , desc = 'Truth & Obs' ) : \"\"\"Generate synthetic truth and observations.\"\"\" Dyn , Obs , chrono , X0 = self . Dyn , self . Obs , self . t , self . X0 # Init xx = np . zeros (( chrono . K + 1 , Dyn . M )) yy = np . zeros (( chrono . KObs + 1 , Obs . M )) xx [ 0 ] = X0 . sample ( 1 ) # Loop for k , kObs , t , dt in utils . progbar ( chrono . ticker , desc ) : xx [ k ] = Dyn ( xx [ k-1 ] , t - dt , dt ) + np . sqrt ( dt ) * Dyn . noise . sample ( 1 ) if kObs is not None : yy [ kObs ] = Obs ( xx [ k ] , t ) + Obs . noise . sample ( 1 ) return xx , yy","title":"simulate"},{"location":"reference/dapper/admin/#operator","text":"class Operator ( M , model = None , noise = None , ** kwargs ) Container for operators (models). View Source class Operator ( dict_tools . NicePrint ): \"\"\"Container for operators (models).\"\"\" def __init__ ( self , M , model = None , noise = None , ** kwargs ): self . M = M # None => Identity model if model is None : model = Id_op () kwargs [ 'linear' ] = lambda x , t , dt : Id_mat ( M ) self . model = model # None/0 => No noise if isinstance ( noise , RV ): self . noise = noise else : if noise is None : noise = 0 if np . isscalar ( noise ): self . noise = GaussRV ( C = noise , M = M ) else : self . noise = GaussRV ( C = noise ) # Write attributes for key , value in kwargs . items (): setattr ( self , key , value ) def __call__ ( self , * args , ** kwargs ): return self . model ( * args , ** kwargs ) printopts = { 'ordering' : [ 'M' , 'model' , 'noise' ]}","title":"Operator"},{"location":"reference/dapper/admin/#ancestors-in-mro_1","text":"dapper.dict_tools.NicePrint","title":"Ancestors (in MRO)"},{"location":"reference/dapper/admin/#class-variables_1","text":"printopts","title":"Class variables"},{"location":"reference/dapper/admin/#xplist","text":"class xpList ( * args , unique = False ) List, subclassed for holding experiment (\"xp\") objects. Main use: administrate experiment launches . Also see: xpSpace for experiment result presentation . Modifications to list : __iadd__ (append) also for single items; this is hackey, but convenience is king. append() supports unique to enable lazy xp declaration. __getitem__ supports lists. pretty printing (using common/distinct attrs). Add-ons: launch() print_averages() gen_names() inds() to search by kw-attrs. View Source class xpList ( list ) : \"\"\"List, subclassed for holding experiment (\" xp \") objects. Main use : administrate experiment ** launches ** . Also see : `` xpSpace `` for experiment ** result presentation ** . Modifications to `` list `` : - `` __iadd__ `` ( append ) also for single items ; this is hackey , but convenience is king . - `` append () `` supports `` unique `` to enable lazy xp declaration . - `` __getitem__ `` supports lists . - pretty printing ( using common / distinct attrs ). Add - ons : - `` launch () `` - `` print_averages () `` - `` gen_names () `` - `` inds () `` to search by kw - attrs . \"\"\" def __init__ ( self , * args , unique = False ) : \"\"\"Initialize without args, or with a list of configs. If `` unique `` : duplicates won ' t get appended . This makes `` append () `` ( and `` __iadd__ () `` ) relatively slow . Use `` extend () `` or `` __add__ () `` to bypass this validation . \"\"\" self . unique = unique super (). __init__ ( * args ) def __iadd__ ( self , xp ) : if not hasattr ( xp , ' __iter__ ' ) : xp = [ xp ] for item in xp : self . append ( item ) return self def append ( self , xp ) : \"\"\"Append if not unique & present.\"\"\" if not ( self . unique and xp in self ) : super (). append ( xp ) def __getitem__ ( self , keys ) : \"\"\"Indexing, also by a list\"\"\" try : B = [ self [ k ] for k in keys ] # if keys is list except TypeError : B = super (). __getitem__ ( keys ) # if keys is int , slice if hasattr ( B , ' __len__ ' ) : B = xpList ( B ) # Cast return B def inds ( self , strict = True , missingval = \"NONSENSE\" , ** kws ) : \"\"\"Find (all) indices of configs whose attributes match kws. If strict , then xp ' s lacking a requested attr will not match , unless the missingval ( e . g . None ) matches the required value . \"\"\" def match ( xp ) : def missing ( v ) : return missingval if strict else v matches = [ getattr ( xp , k , missing ( v )) == v for k , v in kws . items ()] return all ( matches ) return [ i for i , xp in enumerate ( self ) if match ( xp )] @ property def da_methods ( self ) : return [ xp . da_method for xp in self ] def split_attrs ( self , nomerge = ()) : \"\"\"Compile the attrs of all xps ; split as distinct , redundant , common . Insert None if an attribute is distinct but not in xp . \"\"\" def _aggregate_keys () : \"Aggregate keys from all xps\" if len ( self ) == 0 : return [] # Start with da_method aggregate = [ ' da_method ' ] # Aggregate all other keys for xp in self : # Get dataclass fields try : dc_fields = dcs . fields ( xp . __class__ ) dc_names = [ F . name for F in dc_fields ] keys = xp . __dict__ . keys () except TypeError : # Assume namedtuple dc_names = [] keys = xp . _fields # For all potential keys: for k in keys : # If not already present: if k not in aggregate : # If dataclass, check repr: if k in dc_names : if dc_fields [ dc_names . index ( k )]. repr : aggregate . append ( k ) # Else, just append else : aggregate . append ( k ) # Remove unwanted excluded = [ re . compile ( ' ^ _ ' ), ' avrgs ' , ' stats ' , ' HMM ' , ' duration ' ] aggregate = dict_tools . complement ( aggregate , excluded ) return aggregate distinct , redundant , common = {}, {}, {} for key in _aggregate_keys () : # Want to distinguish actual None's from empty (\"N/A\"). # => Don't use getattr(obj,key,None) vals = [ getattr ( xp , key , \"N/A\" ) for xp in self ] # Sort (assign dct) into distinct, redundant, common if dict_tools . flexcomp ( key , * nomerge ) : # nomerge => Distinct dct , vals = distinct , vals elif all ( vals [ 0 ] == v for v in vals ) : # all values equal => common dct , vals = common , vals [ 0 ] else : v0 = next ( v for v in vals if \"N/A\" != v ) if all ( v == \"N/A\" or v == v0 for v in vals ) : # all values equal or \"N/A\" => redundant dct , vals = redundant , v0 else : # otherwise => distinct dct , vals = distinct , vals # Replace \"N/A\" by None def sub ( v ) : return None if v == \"N/A\" else v if isinstance ( vals , str ) : vals = sub ( vals ) else : try : vals = [ sub ( v ) for v in vals ] except TypeError : vals = sub ( vals ) dct [ key ] = vals return distinct , redundant , common def __repr__ ( self ) : distinct , redundant , common = self . split_attrs () s = ' < xpList > of length %d with attributes : \\ n ' % len ( self ) s += utils . tab ( distinct , headers = \"keys\" , showindex = True ) s += \" \\n Other attributes: \\n \" s += str ( dict_tools . AlignedDict ({ ** redundant , ** common })) return s def gen_names ( self , abbrev = 6 , tab = False ) : \"\"\"Similiar to ``self.__repr__()``, but: - returns * list * of names - tabulation is optional - attaches ( abbreviated ) labels to each attribute \"\"\" distinct , redundant , common = self . split_attrs ( nomerge = [ \"da_method\" ]) labels = distinct . keys () values = distinct . values () # Label abbreviation labels = [ utils . collapse_str ( k , abbrev ) for k in labels ] # Make label columns: insert None or lbl+\":\", depending on value def column ( lbl , vals ) : return [ None if v is None else lbl + \":\" for v in vals ] labels = [ column ( lbl , vals ) for lbl , vals in zip ( labels , values )] # Interlace labels and values table = [ x for ( a , b ) in zip ( labels , values ) for x in ( a , b )] # Rm da_method label (but keep value) table . pop ( 0 ) # Transpose table = list ( map ( list , zip ( * table ))) # Tabulate table = utils . tab ( table , tablefmt = \"plain\" ) # Rm space between lbls/vals table = re . sub ( ' : + ' , ':' , table ) # Rm alignment if not tab : table = re . sub ( r ' + ' , r ' ' , table ) return table . splitlines () def tabulate_avrgs ( self , * args , ** kwargs ) : \"\"\"Pretty (tabulated) repr of xps & their avrgs. Similar to stats . tabulate_avrgs (), but for the entire list of xps . \"\"\" distinct , redundant , common = self . split_attrs () averages = dapper . stats . tabulate_avrgs ([ C . avrgs for C in self ], * args , ** kwargs ) columns = { ** distinct , '|' : [ '|' ] * len ( self ), ** averages } # merge return utils . tab ( columns , headers = \"keys\" , showindex = True ). replace ( '\u2423' , ' ' ) def launch ( self , HMM , save_as = \"noname\" , mp = False , setup = seed_and_simulate , fail_gently = None , ** kwargs ) : \"\"\"For each xp in self: run_experiment(xp, ...). The results are saved in `` rc . dirs [ ' data ' ] / save_as . stem `` , unless `` save_as `` is False / None . Depending on `` mp `` , run_experiment () is delegated to one of : - caller process ( no parallelisation ) - multiprocessing on this host - GCP ( Google Cloud Computing ) with HTCondor If `` setup == None `` : use `` seed_and_simulate () `` . The kwargs are forwarded to run_experiment (). See `` example_2 . py `` and `` example_3 . py `` for example use . \"\"\" # TODO 2: doc files and code options in mp, e.g # `files` get added to PYTHONPATH and have dir-structure preserved. # Setup: Experiment initialisation. Default: seed_and_simulate(). # Enables setting experiment variables that are not parameters of a da_method. # Collect common args forwarded to run_experiment kwargs [ ' HMM ' ] = HMM kwargs [ \"setup\" ] = setup # Parse mp option if not mp : mp = False elif mp in [ True , \"MP\" ] : mp = dict ( server = \"local\" ) elif isinstance ( mp , int ) : mp = dict ( server = \"local\" , NPROC = mp ) elif mp in [ \"GCP\" , \"Google\" ] : mp = dict ( server = \"GCP\" , files = [], code = \"\" ) assert isinstance ( mp , dict ) # Parse fail_gently if fail_gently is None : if isinstance ( mp , dict ) and mp [ \"server\" ] == \"GCP\" : fail_gently = False # coz cloud processing is entirely de-coupled anyways else : fail_gently = True # True unless otherwise requested kwargs [ \"fail_gently\" ] = fail_gently # Parse save_as if save_as in [ None , False ] : assert not mp , \"Multiprocessing requires saving data.\" # Parallelization w/o storing is possible, especially w/ threads. # But it involves more complicated communication set-up. def xpi_dir ( * args ) : return None else : save_as = rc . dirs . data / Path ( save_as ). stem save_as /= \"run_\" + datetime . now (). strftime ( \"%Y-%m-%d__%H:%M:%S\" ) os . makedirs ( save_as ) print ( f \"Experiment stored at {save_as}\" ) def xpi_dir ( i ) : path = save_as / str ( i ) os . mkdir ( path ) return path # No parallelization if not mp : for ixp , ( xp , label ) in enumerate ( zip ( self , self . gen_names ())) : run_experiment ( xp , label , xpi_dir ( ixp ), ** kwargs ) # Local multiprocessing elif mp [ \"server\" ]. lower () == \"local\" : def run_with_fixed_args ( arg ) : xp , ixp = arg run_experiment ( xp , None , xpi_dir ( ixp ), ** kwargs ) args = zip ( self , range ( len ( self ))) utils . disable_progbar = True utils . disable_user_interaction = True NPROC = mp . get ( \"NPROC\" , None ) # None => mp . cpu_count () from dapper . tools . multiprocessing import mpd # will fail on GCP with mpd . Pool ( NPROC ) as pool : list ( utils . tqdm . tqdm ( pool . imap ( run_with_fixed_args , args ), total = len ( self ), desc = \"Parallel experim's\" , smoothing = 0.1 )) utils . disable_progbar = False utils . disable_user_interaction = False # Google cloud platform, multiprocessing elif mp [ \"server\" ] == \"GCP\" : for ixp , xp in enumerate ( self ) : with open ( xpi_dir ( ixp ) / \"xp.var\" , \"wb\" ) as f : dill . dump ( dict ( xp = xp ), f ) with open ( save_as / \"xp.com\" , \"wb\" ) as f : dill . dump ( kwargs , f ) # mkdir extra_files extra_files = save_as / \"extra_files\" os . mkdir ( extra_files ) # Default files: .py files in sys.path[0] (main script's path) if not mp . get ( \"files\" , []) : # Todo 4: also intersect(..., sys.modules). # Todo 4: use git ls-tree instead? ff = os . listdir ( sys . path [ 0 ]) mp [ \"files\" ] = [ f for f in ff if f . endswith ( \".py\" )] # Copy files into extra_files for f in mp [ \"files\" ] : if isinstance ( f , ( str , Path )) : # Example: f = \"A.py\" path = Path ( sys . path [ 0 ]) / f dst = f else : # instance of tuple ( path , root ) # Example: f = (\"~/E/G/A.py\", \"G\") path , root = f dst = Path ( path ). relative_to ( root ) dst = extra_files / dst os . makedirs ( dst . parent , exist_ok = True ) try : shutil . copytree ( path , dst ) # dir - r except OSError : shutil . copy2 ( path , dst ) # file # Loads PWD/xp_{var,com} and calls run_experiment() with open ( extra_files / \"load_and_run.py\" , \"w\" ) as f : f . write ( dedent ( \"\"\"\\ import dill from dapper . admin import run_experiment # Load with open ( \"xp.com\" , \"rb\" ) as f : com = dill . load ( f ) with open ( \"xp.var\" , \"rb\" ) as f : var = dill . load ( f ) # User-defined code %s # Run result = run_experiment ( var [ ' xp ' ], None , \".\" , ** com ) \"\"\") % dedent(mp[\" code \"])) with open ( extra_files / \"dpr_config.yaml\" , \"w\" ) as f : f . write ( \" \\n \" . join ([ \"data_root: '$cwd'\" , \"liveplotting: no\" , \"welcome_message: no\" ])) submit_job_GCP ( save_as ) return save_as","title":"xpList"},{"location":"reference/dapper/admin/#ancestors-in-mro_2","text":"builtins.list","title":"Ancestors (in MRO)"},{"location":"reference/dapper/admin/#instance-variables_1","text":"da_methods","title":"Instance variables"},{"location":"reference/dapper/admin/#methods_1","text":"","title":"Methods"},{"location":"reference/dapper/admin/#append","text":"def append ( self , xp ) Append if not unique & present. View Source def append ( self , xp ): \"\"\"Append if not unique & present.\"\"\" if not ( self . unique and xp in self ): super (). append ( xp )","title":"append"},{"location":"reference/dapper/admin/#clear","text":"def clear ( self , / ) Remove all items from list.","title":"clear"},{"location":"reference/dapper/admin/#copy","text":"def copy ( self , / ) Return a shallow copy of the list.","title":"copy"},{"location":"reference/dapper/admin/#count","text":"def count ( self , value , / ) Return number of occurrences of value.","title":"count"},{"location":"reference/dapper/admin/#extend","text":"def extend ( self , iterable , / ) Extend list by appending elements from the iterable.","title":"extend"},{"location":"reference/dapper/admin/#gen_names","text":"def gen_names ( self , abbrev = 6 , tab = False ) Similiar to self.__repr__() , but: returns list of names tabulation is optional attaches (abbreviated) labels to each attribute View Source def gen_names ( self , abbrev = 6 , tab = False ): \"\"\"Similiar to ``self.__repr__()``, but: - returns *list* of names - tabulation is optional - attaches (abbreviated) labels to each attribute \"\"\" distinct , redundant , common = self . split_attrs ( nomerge = [ \"da_method\" ]) labels = distinct . keys () values = distinct . values () # Label abbreviation labels = [ utils . collapse_str ( k , abbrev ) for k in labels ] # Make label columns : insert None or lbl + \":\" , depending on value def column ( lbl , vals ): return [ None if v is None else lbl + \":\" for v in vals ] labels = [ column ( lbl , vals ) for lbl , vals in zip ( labels , values )] # Interlace labels and values table = [ x for ( a , b ) in zip ( labels , values ) for x in ( a , b )] # Rm da_method label ( but keep value ) table . pop ( 0 ) # Transpose table = list ( map ( list , zip ( * table ))) # Tabulate table = utils . tab ( table , tablefmt = \"plain\" ) # Rm space between lbls / vals table = re . sub ( ': +' , ':' , table ) # Rm alignment if not tab : table = re . sub ( r ' +' , r ' ' , table ) return table . splitlines ()","title":"gen_names"},{"location":"reference/dapper/admin/#index","text":"def index ( self , value , start = 0 , stop = 9223372036854775807 , / ) Return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/dapper/admin/#inds","text":"def inds ( self , strict = True , missingval = 'NONSENSE' , ** kws ) Find (all) indices of configs whose attributes match kws. If strict, then xp's lacking a requested attr will not match, unless the missingval (e.g. None) matches the required value. View Source def inds ( self , strict = True , missingval = \"NONSENSE\" , ** kws ): \"\"\"Find (all) indices of configs whose attributes match kws. If strict, then xp's lacking a requested attr will not match, unless the missingval (e.g. None) matches the required value. \"\"\" def match ( xp ): def missing ( v ): return missingval if strict else v matches = [ getattr ( xp , k , missing ( v )) == v for k , v in kws . items ()] return all ( matches ) return [ i for i , xp in enumerate ( self ) if match ( xp )]","title":"inds"},{"location":"reference/dapper/admin/#insert","text":"def insert ( self , index , object , / ) Insert object before index.","title":"insert"},{"location":"reference/dapper/admin/#launch","text":"def launch ( self , HMM , save_as = 'noname' , mp = False , setup =< function seed_and_simulate at 0x7f8a722404c0 > , fail_gently = None , ** kwargs ) For each xp in self: run_experiment(xp, ...). The results are saved in rc.dirs['data']/save_as.stem , unless save_as is False/None. Depending on mp , run_experiment() is delegated to one of: - caller process (no parallelisation) - multiprocessing on this host - GCP (Google Cloud Computing) with HTCondor If setup == None : use seed_and_simulate() . The kwargs are forwarded to run_experiment(). See example_2.py and example_3.py for example use. View Source def launch ( self , HMM , save_as = \"noname\" , mp = False , setup = seed_and_simulate , fail_gently = None , ** kwargs ) : \"\"\"For each xp in self: run_experiment(xp, ...). The results are saved in `` rc . dirs [ ' data ' ] / save_as . stem `` , unless `` save_as `` is False / None . Depending on `` mp `` , run_experiment () is delegated to one of : - caller process ( no parallelisation ) - multiprocessing on this host - GCP ( Google Cloud Computing ) with HTCondor If `` setup == None `` : use `` seed_and_simulate () `` . The kwargs are forwarded to run_experiment (). See `` example_2 . py `` and `` example_3 . py `` for example use . \"\"\" # TODO 2: doc files and code options in mp, e.g # `files` get added to PYTHONPATH and have dir-structure preserved. # Setup: Experiment initialisation. Default: seed_and_simulate(). # Enables setting experiment variables that are not parameters of a da_method. # Collect common args forwarded to run_experiment kwargs [ ' HMM ' ] = HMM kwargs [ \"setup\" ] = setup # Parse mp option if not mp : mp = False elif mp in [ True , \"MP\" ] : mp = dict ( server = \"local\" ) elif isinstance ( mp , int ) : mp = dict ( server = \"local\" , NPROC = mp ) elif mp in [ \"GCP\" , \"Google\" ] : mp = dict ( server = \"GCP\" , files = [], code = \"\" ) assert isinstance ( mp , dict ) # Parse fail_gently if fail_gently is None : if isinstance ( mp , dict ) and mp [ \"server\" ] == \"GCP\" : fail_gently = False # coz cloud processing is entirely de-coupled anyways else : fail_gently = True # True unless otherwise requested kwargs [ \"fail_gently\" ] = fail_gently # Parse save_as if save_as in [ None , False ] : assert not mp , \"Multiprocessing requires saving data.\" # Parallelization w/o storing is possible, especially w/ threads. # But it involves more complicated communication set-up. def xpi_dir ( * args ) : return None else : save_as = rc . dirs . data / Path ( save_as ). stem save_as /= \"run_\" + datetime . now (). strftime ( \"%Y-%m-%d__%H:%M:%S\" ) os . makedirs ( save_as ) print ( f \"Experiment stored at { save_as } \") def xpi_dir ( i ) : path = save_as / str ( i ) os . mkdir ( path ) return path # No parallelization if not mp : for ixp , ( xp , label ) in enumerate ( zip ( self , self . gen_names ())) : run_experiment ( xp , label , xpi_dir ( ixp ), ** kwargs ) # Local multiprocessing elif mp [ \"server\" ]. lower () == \"local\" : def run_with_fixed_args ( arg ) : xp , ixp = arg run_experiment ( xp , None , xpi_dir ( ixp ), ** kwargs ) args = zip ( self , range ( len ( self ))) utils . disable_progbar = True utils . disable_user_interaction = True NPROC = mp . get ( \"NPROC\" , None ) # None => mp . cpu_count () from dapper . tools . multiprocessing import mpd # will fail on GCP with mpd . Pool ( NPROC ) as pool : list ( utils . tqdm . tqdm ( pool . imap ( run_with_fixed_args , args ), total = len ( self ), desc = \"Parallel experim's\" , smoothing = 0.1 )) utils . disable_progbar = False utils . disable_user_interaction = False # Google cloud platform, multiprocessing elif mp [ \"server\" ] == \"GCP\" : for ixp , xp in enumerate ( self ) : with open ( xpi_dir ( ixp ) / \"xp.var\" , \"wb\" ) as f : dill . dump ( dict ( xp = xp ), f ) with open ( save_as / \"xp.com\" , \"wb\" ) as f : dill . dump ( kwargs , f ) # mkdir extra_files extra_files = save_as / \"extra_files\" os . mkdir ( extra_files ) # Default files: .py files in sys.path[0] (main script's path) if not mp . get ( \"files\" , []) : # Todo 4: also intersect(..., sys.modules). # Todo 4: use git ls-tree instead? ff = os . listdir ( sys . path [ 0 ]) mp [ \"files\" ] = [ f for f in ff if f . endswith ( \".py\" )] # Copy files into extra_files for f in mp [ \"files\" ] : if isinstance ( f , ( str , Path )) : # Example: f = \"A.py\" path = Path ( sys . path [ 0 ]) / f dst = f else : # instance of tuple ( path , root ) # Example: f = (\"~/E/G/A.py\", \"G\") path , root = f dst = Path ( path ). relative_to ( root ) dst = extra_files / dst os . makedirs ( dst . parent , exist_ok = True ) try : shutil . copytree ( path , dst ) # dir - r except OSError : shutil . copy2 ( path , dst ) # file # Loads PWD/xp_ { var , com } and calls run_experiment () with open ( extra_files / \"load_and_run.py\" , \"w\" ) as f : f . write ( dedent ( \"\"\"\\ import dill from dapper . admin import run_experiment # Load with open ( \"xp.com\" , \"rb\" ) as f : com = dill . load ( f ) with open ( \"xp.var\" , \"rb\" ) as f : var = dill . load ( f ) # User-defined code %s # Run result = run_experiment ( var [ ' xp ' ], None , \".\" , ** com ) \"\"\") % dedent(mp[\" code \"])) with open ( extra_files / \"dpr_config.yaml\" , \"w\" ) as f : f . write ( \" \\n \" . join ([ \"data_root: '$cwd'\" , \"liveplotting: no\" , \"welcome_message: no\" ])) submit_job_GCP ( save_as ) return save_as","title":"launch"},{"location":"reference/dapper/admin/#pop","text":"def pop ( self , index =- 1 , / ) Remove and return item at index (default last). Raises IndexError if list is empty or index is out of range.","title":"pop"},{"location":"reference/dapper/admin/#remove","text":"def remove ( self , value , / ) Remove first occurrence of value. Raises ValueError if the value is not present.","title":"remove"},{"location":"reference/dapper/admin/#reverse","text":"def reverse ( self , / ) Reverse IN PLACE .","title":"reverse"},{"location":"reference/dapper/admin/#sort","text":"def sort ( self , / , * , key = None , reverse = False ) Sort the list in ascending order and return None. The sort is in-place (i.e. the list itself is modified) and stable (i.e. the order of two equal elements is maintained). If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values. The reverse flag can be set to sort in descending order.","title":"sort"},{"location":"reference/dapper/admin/#split_attrs","text":"def split_attrs ( self , nomerge = () ) Compile the attrs of all xps; split as distinct, redundant, common. Insert None if an attribute is distinct but not in xp. View Source def split_attrs ( self , nomerge = ()) : \"\"\"Compile the attrs of all xps; split as distinct, redundant, common. Insert None if an attribute is distinct but not in xp.\"\"\" def _aggregate_keys () : \"Aggregate keys from all xps\" if len ( self ) == 0 : return [] # Start with da_method aggregate = [ 'da_method' ] # Aggregate all other keys for xp in self : # Get dataclass fields try : dc_fields = dcs . fields ( xp . __class__ ) dc_names = [ F.name for F in dc_fields ] keys = xp . __dict__ . keys () except TypeError : # Assume namedtuple dc_names = [] keys = xp . _fields # For all potential keys : for k in keys : # If not already present : if k not in aggregate : # If dataclass , check repr : if k in dc_names : if dc_fields [ dc_names.index(k) ] . repr : aggregate . append ( k ) # Else , just append else : aggregate . append ( k ) # Remove unwanted excluded = [ re.compile('^_'), 'avrgs', 'stats', 'HMM', 'duration' ] aggregate = dict_tools . complement ( aggregate , excluded ) return aggregate distinct , redundant , common = {} , {} , {} for key in _aggregate_keys () : # Want to distinguish actual None 's from empty (\"N/A\"). # => Don' t use getattr ( obj , key , None ) vals = [ getattr(xp, key, \"N/A\") for xp in self ] # Sort ( assign dct ) into distinct , redundant , common if dict_tools . flexcomp ( key , * nomerge ) : # nomerge => Distinct dct , vals = distinct , vals elif all ( vals [ 0 ] == v for v in vals ) : # all values equal => common dct , vals = common , vals [ 0 ] else : v0 = next ( v for v in vals if \"N/A\" != v ) if all ( v == \"N/A\" or v == v0 for v in vals ) : # all values equal or \"N/A\" => redundant dct , vals = redundant , v0 else : # otherwise => distinct dct , vals = distinct , vals # Replace \"N/A\" by None def sub ( v ) : return None if v == \"N/A\" else v if isinstance ( vals , str ) : vals = sub ( vals ) else : try : vals = [ sub(v) for v in vals ] except TypeError : vals = sub ( vals ) dct [ key ] = vals return distinct , redundant , common","title":"split_attrs"},{"location":"reference/dapper/admin/#tabulate_avrgs","text":"def tabulate_avrgs ( self , * args , ** kwargs ) Pretty (tabulated) repr of xps & their avrgs. Similar to stats.tabulate_avrgs(), but for the entire list of xps. View Source def tabulate_avrgs ( self , * args , ** kwargs ): \"\"\"Pretty (tabulated) repr of xps & their avrgs. Similar to stats.tabulate_avrgs(), but for the entire list of xps.\"\"\" distinct , redundant , common = self . split_attrs () averages = dapper . stats . tabulate_avrgs ([ C . avrgs for C in self ], * args , ** kwargs ) columns = { ** distinct , '|' : [ '|' ] * len ( self ), ** averages } # merge return utils . tab ( columns , headers = \"keys\" , showindex = True ). replace ( '\u2423' , ' ' )","title":"tabulate_avrgs"},{"location":"reference/dapper/data_management/","text":"Module dapper.data_management Define xpSpace (subclasses SparseSpace (subclasses dict)), which is handles the presentation of experiment (xp) results. View Source \"\"\"Define xpSpace (subclasses SparseSpace (subclasses dict)), which is handles the **presentation** of experiment (xp) results.\"\"\" from pathlib import Path import os import copy import warnings import collections import logging import shutil import numpy as np import matplotlib as mpl from matplotlib import cm , ticker import colorama import dill from dapper.tools.colors import color_text from dapper.tools.viz import axis_scale_by_array , freshfig from dapper.tools.series import UncertainQtty from dapper.stats import tabulate_column , unpack_uqs from dapper.admin import xpList import dapper.dict_tools as dict_tools import dapper.tools.remote.uplink as uplink import dapper.tools.utils as utils mpl_logger = logging . getLogger ( 'matplotlib' ) NO_KEY = ( \"da_method\" , \"Const\" , \"upd_a\" ) def make_label ( coord , no_key = NO_KEY , exclude = ()): dct = { a : v for a , v in coord . _asdict () . items () if v != None } lbl = '' for k , v in dct . items (): if k not in exclude : if any ( x in k for x in no_key ): lbl = lbl + f ' { v } ' else : lbl = lbl + f ' { utils . collapse_str ( k , 7 ) } : { v } ' return lbl [ 1 :] def default_styles ( coord , baseline_legends = False ): \"\"\"Quick and dirty (but somewhat robust) styling.\"\"\" style = dict_tools . DotDict ( ms = 8 ) style . label = make_label ( coord ) try : if coord . da_method == \"Climatology\" : style . ls = \":\" style . c = \"k\" if not baseline_legends : style . label = None elif coord . da_method == \"OptInterp\" : style . ls = \":\" style . c = . 7 * np . ones ( 3 ) style . label = \"Opt. Interp.\" if not baseline_legends : style . label = None elif coord . da_method == \"Var3D\" : style . ls = \":\" style . c = . 5 * np . ones ( 3 ) style . label = \"3D-Var\" if not baseline_legends : style . label = None elif coord . da_method == \"EnKF\" : style . marker = \"*\" style . c = \"C1\" elif coord . da_method == \"PartFilt\" : style . marker = \"X\" style . c = \"C2\" else : style . marker = \".\" except AttributeError : pass return style def rel_index ( elem , lst , default = None ): \"\"\"``lst.index(elem) / len(lst)`` with fallback.\"\"\" try : return lst . index ( elem ) / len ( lst ) except ValueError : if default == None : raise return default def discretize_cmap ( cmap , N , val0 = 0 , val1 = 1 , name = None ): \"\"\"Discretize cmap so that it partitions [0,1] into N segments. I.e. cmap(k/N) == cmap(k/N + eps). Also provide the ScalarMappable ``sm`` that maps range(N) to the segment centers, as will be reflected by ``cb = fig.colorbar(sm)``. You can then re-label the ticks using ``cb.set_ticks(np.arange(N)); cb.set_ticklabels([\"A\",\"B\",\"C\",...])``.\"\"\" # cmap(k/N) from_list = mpl . colors . LinearSegmentedColormap . from_list colors = cmap ( np . linspace ( val0 , val1 , N )) cmap = from_list ( name , colors , N ) # sm cNorm = mpl . colors . Normalize ( -. 5 , -. 5 + N ) sm = mpl . cm . ScalarMappable ( cNorm , cmap ) return cmap , sm def cm_bond ( cmap , xp_dict , axis , vmin = 0 , vmax = 0 ): \"\"\"Map cmap for coord.axis \u2208 [0, len(ticks)].\"\"\" def link ( coord ): \"\"\"Essentially: cmap(ticks.index(coord.axis))\"\"\" if hasattr ( coord , axis ): ticks = xp_dict . ticks [ axis ] cNorm = mpl . colors . Normalize ( vmin , vmax + len ( ticks )) ScMap = cm . ScalarMappable ( cNorm , cmap ) . to_rgba index = ticks . index ( getattr ( coord , axis )) return ScMap ( index ) else : return cmap ( 0.5 ) return link def in_idx ( coord , indices , xp_dict , axis ): \"\"\"Essentially: coord.axis in ticks[indices]\"\"\" if hasattr ( coord , axis ): ticks = np . array ( xp_dict . ticks [ axis ])[ indices ] return getattr ( coord , axis ) in ticks else : return True def load_HMM ( save_as ): save_as = Path ( save_as ) . expanduser () HMM = dill . load ( open ( save_as / \"xp.com\" , \"rb\" ))[ \"HMM\" ] return HMM def load_xps ( save_as ): \"\"\"Load ``xps`` (as a simple list) from dir.\"\"\" save_as = Path ( save_as ) . expanduser () files = [ d / \"xp\" for d in uplink . list_job_dirs ( save_as )] def load_any ( filepath ): \"\"\"Load any/all ``xp's`` from ``filepath``.\"\"\" with open ( filepath , \"rb\" ) as F : # If experiment crashed, then xp will be empty try : data = dill . load ( F ) except EOFError : return [] # Always return list try : return data [ \"xps\" ] except KeyError : return [ data [ \"xp\" ]] print ( \"Loading %d files from %s \" % ( len ( files ), save_as )) xps = [] # NB: progbar wont clean up properly w/ list compr. for f in utils . progbar ( files , desc = \"Loading\" ): xps . extend ( load_any ( f )) if len ( xps ) < len ( files ): print ( f \" { len ( files ) - len ( xps ) } files could not be loaded.\" ) return xps def save_xps ( xps , save_as , nDir = 100 ): \"\"\"Split xps and save in save_as/i for i in range(nDir). Example: rename attr n_iter to nIter: >>> proj_name = \"Stein\" >>> dd = dpr.rc.dirs.data / proj_name >>> save_as = dd / \"run_2020-09-22__19:36:13\" >>> >>> for save_as in os.listdir(dd): >>> save_as = dd / save_as >>> >>> xps = load_xps(save_as) >>> HMM = load_HMM(save_as) >>> >>> for xp in xps: >>> if hasattr(xp,\"n_iter\"): >>> xp.nIter = xp.n_iter >>> del xp.n_iter >>> >>> overwrite_xps(xps, save_as) \"\"\" save_as = Path ( save_as ) . expanduser () save_as . mkdir ( parents = False , exist_ok = False ) splitting = np . array_split ( xps , nDir ) for i , sub_xps in enumerate ( utils . tqdm . tqdm ( splitting , desc = \"Saving\" )): if len ( sub_xps ): iDir = save_as / str ( i ) os . mkdir ( iDir ) with open ( iDir / \"xp\" , \"wb\" ) as F : dill . dump ({ 'xps' : sub_xps }, F ) def overwrite_xps ( xps , save_as , nDir = 100 ): \"\"\"Save xps in save_as, but safely (by first saving to tmp).\"\"\" save_xps ( xps , save_as / \"tmp\" , nDir ) # Delete for d in utils . tqdm . tqdm ( uplink . list_job_dirs ( save_as ), desc = \"Deleting old\" ): shutil . rmtree ( d ) # Mv up from tmp/ -- goes quick, coz there are not many. for d in os . listdir ( save_as / \"tmp\" ): shutil . move ( save_as / \"tmp\" / d , save_as / d ) shutil . rmtree ( save_as / \"tmp\" ) def reduce_inodes ( save_as , nDir = 100 ): \"\"\"Reduce the number of ``xp`` dirs by packing multiple ``xp``s into lists (``xps``). This reduces the **number** of files (inodes) on the system, which limits storage capacity (along with **size**). It also deletes files \"xp.var\" and \"out\" (which tends to be relatively large coz of the progbar). This is probably also the reason that the loading time is sometimes reduced.\"\"\" overwrite_xps ( load_xps ( save_as ), save_as , nDir ) class SparseSpace ( dict ): \"\"\"Dict, subclassed enforce key conformity (to a coord. sys, i.e. a space). The coordinate system is specified by its \"axes\", which is used to produce self.Coord (a namedtuple class). As a normal dict, it can hold any type of objects. In normal use, this space is highly sparse, coz there are many coordinates with no matching experiment, eg. coord(da_method=Climatology, rot=True, ...). Indeed, operations across (potentially multiple simultaneous) axes, such as optimization or averaging, should be carried out by iterating -- not over the axis -- but over the the list of items. The most important method is ``nest()``, which is used (by xpSpace.table_tree) to separate tables/columns, and also to carry out the mean/optim operations. In addition, __getitem__() is very flexible, allowing accessing by: - The actual key, a self.Coord object. Returns single item. - A dict match against (part of) the coordinates. Returns subspace. - An int. Returns list(self)[key]. - A list of any of the above. Returns list. This flexibility can cause bugs, but it's probably still worth it). Also see __call__(), get_for(), and coords(), for further convenience. Inspired by https://stackoverflow.com/a/7728830 Also see https://stackoverflow.com/q/3387691 \"\"\" @property def axes ( self ): return self . Coord . _fields def __init__ ( self , axes , * args , ** kwargs ): # Define coordinate system self . Coord = collections . namedtuple ( 'Coord' , axes ) # Write dict self . update ( * args , ** kwargs ) # Add repr/str self . Coord . __repr__ = lambda c : \",\" . join ( f \" { k } = { v !r} \" for k , v in zip ( c . _fields , c )) self . Coord . __str__ = lambda c : \",\" . join ( str ( v ) for v in c ) def update ( self , * args , ** kwargs ): \"\"\"Update using custom __setitem__().\"\"\" # See https://stackoverflow.com/a/2588648 # and https://stackoverflow.com/a/2390997 for k , v in dict ( * args , ** kwargs ) . items (): self [ k ] = v def __setitem__ ( self , key , val ): \"\"\"Setitem ensuring coordinate conforms.\"\"\" try : key = self . Coord ( * key ) except TypeError : raise TypeError ( f \"The key { key !r} did not fit the coord. system \" f \"which has axes { self . axes } \" ) super () . __setitem__ ( key , val ) def __getitem__ ( self , key ): \"\"\"Flexible indexing.\"\"\" # List of items (by a list of indices). # Also see get_for(). if isinstance ( key , list ): return [ self [ k ] for k in key ] # Single (by integer) or list (by Slice) # Note: NOT validating np.int64 here catches quite a few bugs. elif isinstance ( key , int ) or isinstance ( key , slice ): return [ * self . values ()][ key ] # Subspace (by dict, ie. an informal, partial coordinate) elif isinstance ( key , dict ): outer = self . nest ( outer_axes = list ( key )) # nest coord = outer . Coord ( * key . values ()) # create coord inner = outer [ coord ] # chose subspace return inner # Single item (by Coord object, coz an integer (eg) # gets interpreted (above) as a list index) else : # NB: Dont't use isinstance(key, self.Coord) # coz it fails when the namedtuple (Coord) has been # instantiated in different places (but with equal params). # Also see bugs.python.org/issue7796 return super () . __getitem__ ( key ) def __getkey__ ( self , entry ): \"\"\"Inverse of dict.__getitem__(), but also works on coords. Note: This dunder method is not a \"builtin\" naming convention.\"\"\" coord = ( getattr ( entry , a , None ) for a in self . axes ) return self . Coord ( * coord ) def __call__ ( self , ** kwargs ): \"\"\"Convenience, that enables, eg.: >>> xp_dict(da_method=\"EnKF\", infl=1, seed=3) \"\"\" return self . __getitem__ ( kwargs ) def get_for ( self , ticks , default = None ): \"\"\"Almost ``[self.get(Coord(x)) for x in ticks]``. NB: using the \"naive\" thing: ``[self[x] for x in ticks]`` would probably be a BUG coz x gets interpreted as indices for the internal list.\"\"\" singleton = not hasattr ( ticks [ 0 ], \"__iter__\" ) def coord ( xyz ): return self . Coord ( xyz if singleton else xyz ) return [ self . get ( coord ( x ), default ) for x in ticks ] def coords ( self , ** kwargs ): \"\"\"Get all ``coord``s matching kwargs. Unlike ``__getitem__(kwargs)``, - A list is returned, not a subspace. - This list constains keys (coords), not values. - The coords refer to the original space, not the subspace. The last point is especially useful for label_xSection(). \"\"\" def embed ( coord ): return { ** kwargs , ** coord . _asdict ()} return [ self . Coord ( ** embed ( x )) for x in self [ kwargs ]] # Old implementation. # - I prefer the new version for its re-use of __getitem__'s # nesting, evidencing their mutual relationship) # - Note that unlike xpList.inds(): missingval shenanigans # are here unnecessary coz each coordinate is complete. # match = lambda x: all(getattr(x,k)==kwargs[k] for k in kwargs) # return [x for x in self if match(x)] def __repr__ ( self ): # Note: print(xpList(self)) produces more human-readable key listing, # but we don't want to implement it here, coz it requires split_attrs(), # which we don't really want to call again. L = 2 keys = [ str ( k ) for k in self ] if 2 * L < len ( keys ): keys = keys [: L ] + [ \"...\" ] + keys [ - L :] keys = \"[ \\n \" + \", \\n \" . join ( keys ) + \" \\n ]\" txt = f \"< { self . __class__ . __name__ } > with { len ( self ) } keys: { keys } \" # txt += \" befitting the coord. sys. with axes \" txt += \" \\n placed in a coord-sys with axes \" try : txt += \"(and ticks):\" + str ( dict_tools . AlignedDict ( self . ticks )) except AttributeError : txt += \": \\n \" + str ( self . axes ) return txt def nest ( self , inner_axes = None , outer_axes = None ): \"\"\"Return a new xpSpace with axes ``outer_axes``, obtained by projecting along the ``inner_axes``. The entries of this ``xpSpace`` are themselves ``xpSpace``s, with axes ``inner_axes``, each one regrouping the entries with the same (projected) coordinate. Note: is also called by ``__getitem__(key)`` if ``key`` is dict.\"\"\" # Default: a singleton outer space, # with everything contained in the inner (projection) space. if inner_axes is None and outer_axes is None : outer_axes = () # Validate axes if inner_axes is None : assert outer_axes is not None inner_axes = dict_tools . complement ( self . axes , outer_axes ) else : assert outer_axes is None outer_axes = dict_tools . complement ( self . axes , inner_axes ) # Fill spaces outer_space = self . __class__ ( outer_axes ) for coord , entry in self . items (): outer_coord = outer_space . __getkey__ ( coord ) try : inner_space = outer_space [ outer_coord ] except KeyError : inner_space = self . __class__ ( inner_axes ) outer_space [ outer_coord ] = inner_space inner_space [ inner_space . __getkey__ ( coord )] = entry return outer_space def add_axis ( self , axis ): self . __init__ ( self . axes + ( axis ,)) for coord in list ( self ): entry = self . pop ( coord ) self [ coord + ( None ,)] = entry def intersect_axes ( self , attrs ): \"\"\"Rm those a in attrs that are not in self.axes. This allows errors in the axes allotment, for ease-of-use.\"\"\" absent = dict_tools . complement ( attrs , self . axes ) if absent : print ( color_text ( \"Warning:\" , colorama . Fore . RED ), \"The requested attributes\" , color_text ( str ( absent ), colorama . Fore . RED ), ( \"were not found among the\" \" xpSpace axes (attrs. used as coordinates\" \" for the set of experiments).\" \" This may be no problem if the attr. is redundant\" \" for the coord-sys.\" \" However, if it is caused by confusion or mis-spelling,\" \" then it is likely to cause mis-interpretation\" \" of the shown results.\" )) attrs = dict_tools . complement ( attrs , absent ) return attrs def label_xSection ( self , label , * NoneAttrs , ** sub_coord ): \"\"\"Insert duplicate entries for the cross section whose ``coord``s match ``sub_coord``, adding the attr ``Const=label`` to their ``coord``, reflecting the \"constance/constraint/fixation\" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up in ``nest()``. If you wish, you can specify the ``NoneAttrs``, which are consequently set to None for the duplicated entries, preventing them from getting plotted in tuning panels. \"\"\" if \"Const\" not in self . axes : self . add_axis ( 'Const' ) for coord in self . coords ( ** self . intersect_axes ( sub_coord )): entry = copy . deepcopy ( self [ coord ]) coord = coord . _replace ( Const = label ) coord = coord . _replace ( ** { a : None for a in NoneAttrs }) self [ coord ] = entry AXES_ROLES = dict ( outer = None , inner = None , mean = None , optim = None ) class xpSpace ( SparseSpace ): \"\"\"Functionality to facilitate working with ``xps`` and their results. The function ``from_list()`` initializes a ``SparseSpace`` from a list of objects, typically experiments referred to as ``xps``, by (1) computing the relevant axes from the attributes, and (2) filling the dict by ``xps``. The main use of xpSpace is through its ``print()`` & ``plot()``, both of which call ``table_tree()`` to nest the axes of the SparseSpace. For custom plotting, you will likely want to start with ``table_tree()``. Using ``from_list(xps)`` creates a SparseSpace holding ``xps``. However, the nested ``xpSpace``s output by ``table_tree()``, will hold objects of type ``UncertainQtty``, coz ``table_tree()`` calls ``mean()`` calls ``field(statkey)``.\"\"\" @classmethod def from_list ( cls , xps ): \"\"\"Init xpSpace from xpList.\"\"\" def make_ticks ( axes , ordering = dict ( N = 'default' , seed = 'default' , infl = 'default' , loc_rad = 'default' , rot = 'as_found' , da_method = 'as_found' , )): \"\"\"Unique & sort, for each axis (individually) in axes.\"\"\" for ax_name , arr in axes . items (): ticks = set ( arr ) # unique (jumbles order) # Sort order = ordering . get ( ax_name , 'default' ) . lower () if hasattr ( order , '__call__' ): # eg. mylist.index ticks = sorted ( ticks , key = order ) elif 'as_found' in order : ticks = sorted ( ticks , key = arr . index ) else : # default sorting, with None placed at the end ticks = sorted ( ticks , key = lambda x : ( x is None , x )) if any ( x in order for x in [ 'rev' , 'inv' ]): ticks = ticks [:: - 1 ] axes [ ax_name ] = ticks # Define axes xp_list = xpList ( xps ) axes = xp_list . split_attrs ( nomerge = [ 'Const' ])[ 0 ] make_ticks ( axes ) self = cls ( axes . keys ()) # Note: this attr (ticks) will not be propagated through nest(). # That is fine. Otherwise we should have to prune the ticks # (if they are to be useful), which we don't want to do. self . ticks = axes # Fill self . update ({ self . __getkey__ ( xp ): xp for xp in xps }) return self def field ( self , statkey = \"rmse.a\" ): \"\"\"Extract ``statkey`` for each item in ``self``.\"\"\" # Init a new xpDict to hold field avrgs = self . __class__ ( self . axes ) found_anything = False for coord , xp in self . items (): val = getattr ( xp . avrgs , statkey , None ) avrgs [ coord ] = val found_anything = found_anything or ( val is not None ) if not found_anything : raise AttributeError ( f \"The stat. field ' { statkey } ' was not found\" \" among any of the xp's.\" ) return avrgs def mean ( self , axes = None ): # Note: The case ``axes=()`` should work w/o special treatment. if axes is None : return self nested = self . nest ( axes ) for coord , space in nested . items (): def getval ( uq ): return uq . val if isinstance ( uq , UncertainQtty ) else uq vals = [ getval ( uq ) for uq in space . values ()] # Don't use nanmean! It would give false impressions. mu = np . mean ( vals ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) # Don't print warnings caused by N=1. # It already correctly yield nan's. var = np . var ( vals , ddof = 1 ) N = len ( vals ) uq = UncertainQtty ( mu , np . sqrt ( var / N )) uq . nTotal = N uq . nFail = N - np . isfinite ( vals ) . sum () uq . nSuccess = N - uq . nFail nested [ coord ] = uq return nested def tune ( self , axes = None , costfun = None ): \"\"\"Get (compile/tabulate) a stat field optimised wrt. tuning params.\"\"\" # Define cost-function costfun = ( costfun or 'increasing' ) . lower () if 'increas' in costfun : costfun = ( lambda x : + x ) elif 'decreas' in costfun : costfun = ( lambda x : - x ) else : assert hasattr ( costfun , '__call__' ) # custom # Note: The case ``axes=()`` should work w/o special treatment. if axes is None : return self nested = self . nest ( axes ) for coord , space in nested . items (): # Find optimal value and coord within space MIN = np . inf for i , ( inner_coord , uq ) in enumerate ( space . items ()): cost = costfun ( uq . val ) if cost <= MIN : MIN = cost uq_opt = uq uq_opt . tuned_coord = inner_coord nested [ coord ] = uq_opt return nested def validate_axes ( self , axes ): \"\"\"Validate axes. Note: This does not convert None to (), allowing None to remain special. Use ``axis or ()`` wherever tuples are required. \"\"\" roles = {} # \"inv\" for role in set ( axes ) | set ( AXES_ROLES ): assert role in AXES_ROLES , f \"Invalid role { role !r} \" aa = axes . get ( role , AXES_ROLES [ role ]) if aa is None : pass # Purposely special else : # Ensure iterable if isinstance ( aa , str ) or not hasattr ( aa , \"__iter__\" ): aa = ( aa ,) aa = self . intersect_axes ( aa ) for axis in aa : # Ensure unique if axis in roles : raise TypeError ( f \"An axis (here { axis !r} ) cannot be assigned to 2\" f \" roles (here { role !r} and { roles [ axis ] !r} ).\" ) else : roles [ axis ] = role axes [ role ] = aa return axes def table_tree ( self , statkey , axes ): \"\"\"Hierarchical nest(): xp_dict>outer>inner>mean>optim. as specified by ``axes``. Returns this new xpSpace. - print_1d / plot_1d (respectively) separate tables / panel(row)s for ``axes['outer']``, and columns/ x-axis for ``axes['inner']``. - The ``axes['mean']`` and ``axes['optim']`` get eliminated by the mean()/tune() operations. Note: cannot support multiple statkeys because it's not (obviously) meaningful when optimizing over tuning_axes. \"\"\" axes = self . validate_axes ( axes ) def mean_tune ( xp_dict ): \"\"\"Take mean, then tune. Note: the SparseDict implementation should be sufficiently \"uncluttered\" that mean_tune() (or a few of its code lines) could be called anywhere above/between/below the ``nest()``ing of ``outer`` or ``inner``. These possibile call locations are commented in the code.\"\"\" uq_dict = xp_dict . field ( statkey ) uq_dict = uq_dict . mean ( axes [ 'mean' ]) uq_dict = uq_dict . tune ( axes [ 'optim' ]) return uq_dict self = mean_tune ( self ) # Prefer calling mean_tune() [also see its docstring] # before doing outer/inner nesting. This is because then the axes of # a row (xpSpace) should not include mean&optim, and thus: # - Column header/coords may be had directly as row.keys(), # without extraction by __getkey__() from (e.g.) row[0]. # - Don't need to propagate mean&optim axes down to the row level. # which would require defining rows by the nesting: # rows = table.nest(outer_axes=dict_tools.complement(table.axes, # *(axes['inner'] or ()), # *(axes['mean'] or ()), # *(axes['optim'] or ()) )) # - Each level of the output from table_tree # is a smaller (and more manageable) dict. tables = self . nest ( outer_axes = axes [ 'outer' ]) for table_coord , table in tables . items (): # table = mean_tune(table) # Should not be used (nesting as rows is more natural, # and is required for getting distinct/row_keys). # cols = table.nest(outer_axes=axes['inner']) rows = table . nest ( inner_axes = axes [ 'inner' ] or ()) # Overwrite table by its nesting as rows tables [ table_coord ] = rows # for row_coord, row in rows.items(): # rows[row_coord] = mean_tune(row) return axes , tables def tickz ( self , axis_name ): \"\"\"Axis ticks without None\"\"\" return [ x for x in self . ticks [ axis_name ] if x is not None ] def print ( self , statkey = \"rmse.a\" , axes = AXES_ROLES , subcols = True , decimals = None ): \"\"\"Print tables of results. - statkey: The statistical field from the experiments to report. - subcols: If True, then subcolumns are added to indicate the 1\u03c3 confidence interval, and potentially some other stuff. - axes: Allots (maps) each role to a set of axis of the xp_dict. Suggestion: >>> dict( >>> outer='da_method', inner='N', mean='seed', >>> optim=('infl','loc_rad')) Example: If ``mean`` is assigned to: - (\"seed\",): Experiments are averaged accross seeds, and the 1\u03c3 (sub)col is computed as sqrt(var(xps)/N), where xps is a set of experiments. - () : Experiments are averaged across nothing (i.e. this is an edge case). - None : Experiments are not averaged (i.e. the values are the same as above), and the 1\u03c3 (sub)col is computed from the time series of that single experiment. \"\"\" import pandas as pd def align_subcols ( rows , cc , subcols , h2 ): \"\"\"Subcolumns: align, justify, join.\"\"\" # Define subcol formats subc = dict () subc [ 'keys' ] = [ \"val\" , \"conf\" ] subc [ 'headers' ] = [ statkey , '1\u03c3' ] subc [ 'frmts' ] = [ None , None ] subc [ 'spaces' ] = [ ' \u00b1' , ] # last one gets appended below. subc [ 'aligns' ] = [ '>' , '<' ] # 4 header -- matter gets decimal-aligned. if axes [ 'optim' ] is not None : subc [ 'keys' ] += [ \"tuned_coord\" ] subc [ 'headers' ] += [ axes [ 'optim' ]] subc [ 'frmts' ] += [ lambda x : tuple ( a for a in x )] subc [ 'spaces' ] += [ ' *' ] subc [ 'aligns' ] += [ '<' ] elif axes [ 'mean' ] is not None : subc [ 'keys' ] += [ \"nFail\" , \"nSuccess\" ] subc [ 'headers' ] += [ '\u2620' , '\u2713' ] # use width-1 symbols! subc [ 'frmts' ] += [ None , None ] subc [ 'spaces' ] += [ ' ' , ' ' ] subc [ 'aligns' ] += [ '>' , '>' ] subc [ 'spaces' ] . append ( '' ) # no space after last subcol template = ' {} ' + ' {} ' . join ( subc [ 'spaces' ]) # Transpose columns = [ list ( x ) for x in zip ( * rows )] # Iterate over columns. for j , ( col_coord , column ) in enumerate ( zip ( cc , columns )): # Tabulate columns if subcols : column = unpack_uqs ( column , decimals , subc [ \"keys\" ]) # Tabulate subcolumns subheaders = [] for key , header , frmt , _ , align in zip ( * subc . values ()): column [ key ] = tabulate_column ( column [ key ], header , frmt = frmt )[ 1 :] L = len ( column [ - 1 ][ key ]) if align == '<' : subheaders += [ str ( header ) . ljust ( L )] else : subheaders += [ str ( header ) . rjust ( L )] # Join subcolumns: matter = [ template . format ( * [ row [ k ] for k in subc [ 'keys' ]]) for row in column ] header = template . format ( * subheaders ) else : column = unpack_uqs ( column , decimals )[ \"val\" ] column = tabulate_column ( column , statkey ) header , matter = column [ 0 ], column [ 1 :] if h2 : # Do super_header if j : super_header = str ( col_coord ) else : super_header = repr ( col_coord ) width = len ( header ) # += 1 if using unicode chars like \u2714\ufe0f super_header = super_header . center ( width , \"_\" ) header = super_header + \" \\n \" + header columns [ j ] = [ header ] + matter # Un-transpose rows = [ list ( x ) for x in zip ( * columns )] return rows # Inform axes[\"mean\"] if axes . get ( 'mean' , None ): print ( f \"Averages (in time and) over { axes [ 'mean' ] } .\" ) else : print ( \"Averages in time only\" \" (=> the 1\u03c3 estimates may be unreliable).\" ) axes , tables = self . table_tree ( statkey , axes ) for table_coord , table in tables . items (): # Get this table's column coords (cc). Use dict for sorted&unique. # cc = xp_dict.ticks[axes[\"inner\"]] # May be larger than needed. # cc = table[0].keys() # May be too small a set. cc = { c : None for row in table . values () for c in row } # Convert table (rows) into rows (lists) of equal length rows = [[ row . get ( c , None ) for c in cc ] for row in table . values ()] if False : # ****************** Simple (for debugging) table for i , ( row_coord , row ) in enumerate ( zip ( table , rows )): row_key = \", \" . join ( str ( v ) for v in row_coord ) rows [ i ] = [ row_key ] + row rows . insert ( 0 , [ f \" { table . axes } \" ] + [ repr ( c ) for c in cc ]) else : # ********************** Elegant table. h2 = \" \\n \" if len ( cc ) > 1 else \"\" # do column-super-header rows = align_subcols ( rows , cc , subcols , h2 ) # Make and prepend left-side table # - It's prettier if row_keys don't have unnecessary cols. # For example, the table of Climatology should not have an # entire column repeatedly displaying \"infl=None\". # => split_attrs(). # - Why didn't we do this for the column attrs? # Coz there we have no ambition to split the attrs, # which would also require excessive processing: # nesting the table as cols, and then split_attrs() on cols. row_keys = xpList ( table . keys ()) . split_attrs ()[ 0 ] row_keys = pd . DataFrame . from_dict ( row_keys , dtype = \"O\" ) # allow storing None if len ( row_keys . columns ): # Header rows [ 0 ] = [ h2 + k for k in row_keys ] + [ h2 + '\u244a' ] + rows [ 0 ] # Matter for row , ( i , key ) in zip ( rows [ 1 :], row_keys . iterrows ()): rows [ i + 1 ] = [ * key ] + [ '|' ] + row # Print print ( \" \\n \" , end = \"\" ) if axes [ 'outer' ]: table_title = \"Table for \" + repr ( table_coord ) print ( color_text ( table_title , colorama . Back . YELLOW )) headers , * rows = rows print ( utils . tab ( rows , headers ) . replace ( '\u2423' , ' ' )) def plot ( self , statkey = \"rmse.a\" , axes = AXES_ROLES , get_style = default_styles , fignum = None , figsize = None , panels = None , title2 = None , costfun = None , unique_labels = True ): \"\"\"Plot the avrgs of ``statkey`` as a function of ``axis[\"inner\"]``. Firs of all, though, mean and optimum computations are done for ``axis[\"mean\"]`` and ``axis[\"optim\"]``. The optimal parameters are plotted in panels below the main plot. This can be prevented by providing the figure axes in ``panels``. Optionally, the experiments can be grouped by ``axis[\"outer\"]``, producing a figure with columns of panels.\"\"\" def plot1 ( panelcol , row , style ): \"\"\"Plot a given line (row) in the main panel and the optim panels. Involves: Sort, insert None's, handle constant lines.\"\"\" # Make a full row (yy) of vals, whether is_constant or not. # row.is_constant = (len(row)==1 and next(iter(row))==row.Coord(None)) row . is_constant = all ( x == row . Coord ( None ) for x in row ) yy = [ row [ 0 ] if row . is_constant else y for y in row . get_for ( xticks )] # Plot main row . vals = [ getattr ( y , 'val' , None ) for y in yy ] row . handles = {} row . handles [ \"main_panel\" ] = panelcol [ 0 ] . plot ( xticks , row . vals , ** style )[ 0 ] # Plot tuning params row . tuned_coords = {} # Store ordered, \"transposed\" argmins argmins = [ getattr ( y , 'tuned_coord' , None ) for y in yy ] for a , panel in zip ( axes [ \"optim\" ], panelcol [ 1 :]): yy = [ getattr ( coord , a , None ) for coord in argmins ] row . tuned_coords [ a ] = yy # Plotting all None's sets axes units (like any plotting call) # which can cause trouble if the axes units were actually supposed # to be categorical (eg upd_a), but this is only revealed later. if not all ( y == None for y in yy ): row . handles [ a ] = panel . plot ( xticks , yy , ** style ) # Nest axes through table_tree() assert len ( axes [ \"inner\" ]) == 1 , \"You must chose the abscissa.\" axes , tables = self . table_tree ( statkey , axes ) xticks = self . tickz ( axes [ \"inner\" ][ 0 ]) # Figure panels if panels is None : nrows = len ( axes [ 'optim' ] or ()) + 1 ncols = len ( tables ) maxW = 12.7 # my mac screen figsize = figsize or ( min ( 5 * ncols , maxW ), 7 ) gs = dict ( height_ratios = [ 6 ] + [ 1 ] * ( nrows - 1 ), hspace = 0.05 , wspace = 0.05 , # eyeballed: left = 0.15 / ( 1 + np . log ( ncols )), right = 0.97 , bottom = 0.06 , top = 0.9 ) # Create _ , panels = freshfig ( num = fignum , figsize = figsize , nrows = nrows , sharex = True , ncols = ncols , sharey = 'row' , gridspec_kw = gs ) panels = np . ravel ( panels ) . reshape (( - 1 , ncols )) else : panels = np . atleast_2d ( panels ) # Title fig = panels [ 0 , 0 ] . figure fig_title = \"Average wrt. time\" if axes [ \"mean\" ] is not None : fig_title += f \" and { axes [ 'mean' ] } \" if title2 is not None : fig_title += \" \\n \" + str ( title2 ) fig . suptitle ( fig_title ) # Loop outer label_register = set () # mv inside loop to get legend on each panel for table_panels , ( table_coord , table ) in zip ( panels . T , tables . items ()): table . panels = table_panels title = '' if axes [ \"outer\" ] is None else repr ( table_coord ) # Plot for coord , row in table . items (): style = get_style ( coord ) # Rm duplicate labels (contrary to coords, labels can # be \"tampered\" with, and so can be duplicate) if unique_labels : if style . get ( \"label\" , None ) in label_register : del style [ \"label\" ] else : label_register . add ( style [ \"label\" ]) plot1 ( table . panels , row , style ) # Beautify panel0 = table . panels [ 0 ] panel0 . set_title ( title ) if panel0 . is_first_col (): panel0 . set_ylabel ( statkey ) with utils . set_tmp ( mpl_logger , 'level' , 99 ): # silence \"no label\" msg panel0 . legend () table . panels [ - 1 ] . set_xlabel ( axes [ \"inner\" ][ 0 ]) # Tuning panels: for a , panel in zip ( axes [ \"optim\" ] or (), table . panels [ 1 :]): if panel . is_first_col (): panel . set_ylabel ( f \"Optim. \\n { a } \" ) tables . fig = fig tables . xp_dict = self tables . axes_roles = axes return tables def default_fig_adjustments ( tables ): \"\"\"Beautify. These settings do not generalize well.\"\"\" # Get axs as 2d-array axs = np . array ([ table . panels for table in tables . values ()]) . T # Main panels (top row) only: sensible_f = ticker . FormatStrFormatter ( ' %g ' ) for ax in axs [ 0 , :]: for direction , nPanel in zip ([ 'y' , 'x' ], axs . shape ): if nPanel < 6 : eval ( f \"ax.set_ { direction } scale('log')\" ) eval ( f \"ax. { direction } axis\" ) . set_minor_formatter ( sensible_f ) eval ( f \"ax. { direction } axis\" ) . set_major_formatter ( sensible_f ) # Tuning panels only table = tables [ 0 ] for a , panel in zip ( tables . axes_roles [ \"optim\" ] or (), table . panels [ 1 :]): yy = tables . xp_dict . tickz ( a ) axis_scale_by_array ( panel , yy , \"y\" ) # set_ymargin doesn't work for wonky scales. Do so manually: alpha = len ( yy ) / 10 y0 , y1 , y2 , y3 = yy [ 0 ], yy [ 1 ], yy [ - 2 ], yy [ - 1 ] panel . set_ylim ( y0 - alpha * ( y1 - y0 ), y3 + alpha * ( y3 - y2 )) # All panels for ax in axs . ravel (): for direction , nPanel in zip ([ 'y' , 'x' ], axs . shape ): if nPanel < 6 : ax . grid ( True , which = \"minor\" , axis = direction ) # Not strictly compatible with gridspec height_ratios, # (throws warning), but still works ok. with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = UserWarning ) axs [ 0 , 0 ] . figure . tight_layout () Variables AXES_ROLES NO_KEY mpl_logger Functions cm_bond def cm_bond ( cmap , xp_dict , axis , vmin = 0 , vmax = 0 ) Map cmap for coord.axis \u2208 [0, len(ticks)]. View Source def cm_bond ( cmap , xp_dict , axis , vmin = 0 , vmax = 0 ) : \"\"\"Map cmap for coord.axis \u2208 [0, len(ticks)].\"\"\" def link ( coord ) : \"\"\"Essentially: cmap(ticks.index(coord.axis))\"\"\" if hasattr ( coord , axis ) : ticks = xp_dict . ticks [ axis ] cNorm = mpl . colors . Normalize ( vmin , vmax + len ( ticks )) ScMap = cm . ScalarMappable ( cNorm , cmap ). to_rgba index = ticks . index ( getattr ( coord , axis )) return ScMap ( index ) else : return cmap ( 0.5 ) return link default_fig_adjustments def default_fig_adjustments ( tables ) Beautify. These settings do not generalize well. View Source def default_fig_adjustments ( tables ): \"\"\"Beautify. These settings do not generalize well.\"\"\" # Get axs as 2 d - array axs = np . array ([ table . panels for table in tables . values ()]). T # Main panels ( top row ) only : sensible_f = ticker . FormatStrFormatter ( '%g' ) for ax in axs [ 0 , :]: for direction , nPanel in zip ([ 'y' , 'x' ], axs . shape ): if nPanel < 6 : eval ( f \"ax.set_{direction}scale('log')\" ) eval ( f \"ax.{direction}axis\" ). set_minor_formatter ( sensible_f ) eval ( f \"ax.{direction}axis\" ). set_major_formatter ( sensible_f ) # Tuning panels only table = tables [ 0 ] for a , panel in zip ( tables . axes_roles [ \"optim\" ] or (), table . panels [ 1 :]): yy = tables . xp_dict . tickz ( a ) axis_scale_by_array ( panel , yy , \"y\" ) # set_ymargin doesn 't work for wonky scales. Do so manually: alpha = len(yy)/10 y0, y1, y2, y3 = yy[0], yy[1], yy[-2], yy[-1] panel.set_ylim(y0-alpha*(y1-y0), y3+alpha*(y3-y2)) # All panels for ax in axs.ravel(): for direction, nPanel in zip([' y ', ' x ' ], axs . shape ): if nPanel < 6 : ax . grid ( True , which = \"minor\" , axis = direction ) # Not strictly compatible with gridspec height_ratios , # ( throws warning ), but still works ok . with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = UserWarning ) axs [ 0 , 0 ]. figure . tight_layout () default_styles def default_styles ( coord , baseline_legends = False ) Quick and dirty (but somewhat robust) styling. View Source def default_styles ( coord , baseline_legends = False ): \"\"\"Quick and dirty (but somewhat robust) styling.\"\"\" style = dict_tools . DotDict ( ms = 8 ) style . label = make_label ( coord ) try : if coord . da_method == \"Climatology\" : style . ls = \":\" style . c = \"k\" if not baseline_legends : style . label = None elif coord . da_method == \"OptInterp\" : style . ls = \":\" style . c = . 7 * np . ones ( 3 ) style . label = \"Opt. Interp.\" if not baseline_legends : style . label = None elif coord . da_method == \"Var3D\" : style . ls = \":\" style . c = . 5 * np . ones ( 3 ) style . label = \"3D-Var\" if not baseline_legends : style . label = None elif coord . da_method == \"EnKF\" : style . marker = \"*\" style . c = \"C1\" elif coord . da_method == \"PartFilt\" : style . marker = \"X\" style . c = \"C2\" else : style . marker = \".\" except AttributeError : pass return style discretize_cmap def discretize_cmap ( cmap , N , val0 = 0 , val1 = 1 , name = None ) Discretize cmap so that it partitions [0,1] into N segments. I.e. cmap(k/N) == cmap(k/N + eps). Also provide the ScalarMappable sm that maps range(N) to the segment centers, as will be reflected by cb = fig.colorbar(sm) . You can then re-label the ticks using cb.set_ticks(np.arange(N)); cb.set_ticklabels([\"A\",\"B\",\"C\",...]) . View Source def discretize_cmap ( cmap , N , val0 = 0 , val1 = 1 , name = None ) : \" \"\" Discretize cmap so that it partitions [0,1] into N segments. I.e. cmap(k/N) == cmap(k/N + eps). Also provide the ScalarMappable ``sm`` that maps range(N) to the segment centers, as will be reflected by ``cb = fig.colorbar(sm)``. You can then re-label the ticks using ``cb.set_ticks(np.arange(N)); cb.set_ticklabels([\" A \",\" B \",\" C \",...])``. \"\" \" # cmap(k/N) from_list = mpl . colors . LinearSegmentedColormap . from_list colors = cmap ( np . linspace ( val0 , val1 , N )) cmap = from_list ( name , colors , N ) # sm cNorm = mpl . colors . Normalize ( - .5 , - .5 + N ) sm = mpl . cm . ScalarMappable ( cNorm , cmap ) return cmap , sm in_idx def in_idx ( coord , indices , xp_dict , axis ) Essentially: coord.axis in ticks[indices] View Source def in_idx ( coord , indices , xp_dict , axis ) : \"\"\"Essentially: coord.axis in ticks[indices]\"\"\" if hasattr ( coord , axis ) : ticks = np . array ( xp_dict . ticks [ axis ] ) [ indices ] return getattr ( coord , axis ) in ticks else : return True load_HMM def load_HMM ( save_as ) View Source def load_HMM ( save_as ): save_as = Path ( save_as ) . expanduser () HMM = dill . load ( open ( save_as / \"xp.com\" , \"rb\" ))[ \"HMM\" ] return HMM load_xps def load_xps ( save_as ) Load xps (as a simple list) from dir. View Source def load_xps ( save_as ) : \" \"\" Load ``xps`` (as a simple list) from dir. \"\" \" save_as = Path ( save_as ). expanduser () files = [ d / \"xp\" for d in uplink . list_job_dirs ( save_as ) ] def load_any ( filepath ) : \" \"\" Load any/all ``xp's`` from ``filepath``. \"\" \" with open ( filepath , \"rb\" ) as F : # If experiment crashed, then xp will be empty try : data = dill . load ( F ) except EOFError : return [] # Always return list try : return data [ \"xps\" ] except KeyError : return [ data [ \"xp\" ]] print ( \"Loading %d files from %s\" % ( len ( files ), save_as )) xps = [] # NB: progbar wont clean up properly w/ list compr. for f in utils . progbar ( files , desc = \"Loading\" ) : xps . extend ( load_any ( f )) if len ( xps ) < len ( files ) : print ( f \"{len(files)-len(xps)} files could not be loaded.\" ) return xps make_label def make_label ( coord , no_key = ( 'da_method' , 'Const' , 'upd_a' ), exclude = () ) View Source def make_label ( coord , no_key = NO_KEY , exclude = ()): dct = { a : v for a , v in coord . _asdict (). items () if v != None } lbl = '' for k , v in dct . items (): if k not in exclude : if any ( x in k for x in no_key ): lbl = lbl + f ' {v}' else : lbl = lbl + f ' {utils.collapse_str(k,7)}:{v}' return lbl [ 1 :] overwrite_xps def overwrite_xps ( xps , save_as , nDir = 100 ) Save xps in save_as, but safely (by first saving to tmp). View Source def overwrite_xps ( xps , save_as , nDir = 100 ): \"\"\"Save xps in save_as, but safely (by first saving to tmp).\"\"\" save_xps ( xps , save_as / \"tmp\" , nDir ) # Delete for d in utils . tqdm . tqdm ( uplink . list_job_dirs ( save_as ), desc = \"Deleting old\" ): shutil . rmtree ( d ) # Mv up from tmp / -- goes quick, coz there are not many. for d in os . listdir ( save_as / \"tmp\" ): shutil . move ( save_as / \"tmp\" / d , save_as / d ) shutil . rmtree ( save_as / \"tmp\" ) reduce_inodes def reduce_inodes ( save_as , nDir = 100 ) Reduce the number of xp dirs by packing multiple xp s into lists ( xps ). This reduces the number of files (inodes) on the system, which limits storage capacity (along with size ). It also deletes files \"xp.var\" and \"out\" (which tends to be relatively large coz of the progbar). This is probably also the reason that the loading time is sometimes reduced. View Source def reduce_inodes ( save_as , nDir = 100 ): \"\"\"Reduce the number of ``xp`` dirs by packing multiple ``xp``s into lists (``xps``). This reduces the **number** of files (inodes) on the system, which limits storage capacity (along with **size**). It also deletes files \"xp.var\" and \"out\" (which tends to be relatively large coz of the progbar). This is probably also the reason that the loading time is sometimes reduced.\"\"\" overwrite_xps ( load_xps ( save_as ), save_as , nDir ) rel_index def rel_index ( elem , lst , default = None ) lst.index(elem) / len(lst) with fallback. View Source def rel_index ( elem , lst , default = None ): \"\"\"``lst.index(elem) / len(lst)`` with fallback.\"\"\" try : return lst . index ( elem ) / len ( lst ) except ValueError : if default == None : raise return default save_xps def save_xps ( xps , save_as , nDir = 100 ) Split xps and save in save_as/i for i in range(nDir). Example: rename attr n_iter to nIter: proj_name = \"Stein\" dd = dpr.rc.dirs.data / proj_name save_as = dd / \"run_2020-09-22__19:36:13\" for save_as in os.listdir(dd): save_as = dd / save_as xps = load_xps ( save_as ) HMM = load_HMM ( save_as ) for xp in xps : if hasattr ( xp , \"n_iter\" ): xp . nIter = xp . n_iter del xp . n_iter overwrite_xps ( xps , save_as ) View Source def save_xps ( xps , save_as , nDir = 100 ): \"\"\"Split xps and save in save_as/i for i in range(nDir). Example: rename attr n_iter to nIter: >>> proj_name = \"Stein\" >>> dd = dpr.rc.dirs.data / proj_name >>> save_as = dd / \"run_2020-09-22__19:36:13\" >>> >>> for save_as in os.listdir(dd): >>> save_as = dd / save_as >>> >>> xps = load_xps(save_as) >>> HMM = load_HMM(save_as) >>> >>> for xp in xps: >>> if hasattr(xp,\"n_iter\"): >>> xp.nIter = xp.n_iter >>> del xp.n_iter >>> >>> overwrite_xps(xps, save_as) \"\"\" save_as = Path ( save_as ) . expanduser () save_as . mkdir ( parents = False , exist_ok = False ) splitting = np . array_split ( xps , nDir ) for i , sub_xps in enumerate ( utils . tqdm . tqdm ( splitting , desc = \"Saving\" )): if len ( sub_xps ): iDir = save_as / str ( i ) os . mkdir ( iDir ) with open ( iDir / \"xp\" , \"wb\" ) as F : dill . dump ({ 'xps' : sub_xps }, F ) Classes SparseSpace class SparseSpace ( axes , * args , ** kwargs ) Dict, subclassed enforce key conformity (to a coord. sys, i.e. a space). The coordinate system is specified by its \"axes\", which is used to produce self.Coord (a namedtuple class). As a normal dict, it can hold any type of objects. In normal use, this space is highly sparse, coz there are many coordinates with no matching experiment, eg. coord(da_method=Climatology, rot=True, ...). Indeed, operations across (potentially multiple simultaneous) axes, such as optimization or averaging, should be carried out by iterating -- not over the axis -- but over the the list of items. The most important method is nest() , which is used (by xpSpace.table_tree) to separate tables/columns, and also to carry out the mean/optim operations. In addition, getitem () is very flexible, allowing accessing by: - The actual key, a self.Coord object. Returns single item. - A dict match against (part of) the coordinates. Returns subspace. - An int. Returns list(self)[key]. - A list of any of the above. Returns list. This flexibility can cause bugs, but it's probably still worth it). Also see call (), get_for(), and coords(), for further convenience. Inspired by https://stackoverflow.com/a/7728830 Also see https://stackoverflow.com/q/3387691 View Source class SparseSpace ( dict ) : \"\"\"Dict, subclassed enforce key conformity (to a coord. sys, i.e. a space). The coordinate system is specified by its \" axes \", which is used to produce self.Coord (a namedtuple class). As a normal dict, it can hold any type of objects. In normal use, this space is highly sparse, coz there are many coordinates with no matching experiment, eg. coord(da_method=Climatology, rot=True, ...). Indeed, operations across (potentially multiple simultaneous) axes, such as optimization or averaging, should be carried out by iterating -- not over the axis -- but over the the list of items. The most important method is ``nest()``, which is used (by xpSpace.table_tree) to separate tables/columns, and also to carry out the mean/optim operations. In addition, __getitem__() is very flexible, allowing accessing by: - The actual key, a self.Coord object. Returns single item. - A dict match against (part of) the coordinates. Returns subspace. - An int. Returns list(self)[key]. - A list of any of the above. Returns list. This flexibility can cause bugs, but it's probably still worth it). Also see __call__(), get_for(), and coords(), for further convenience. Inspired by https://stackoverflow.com/a/7728830 Also see https://stackoverflow.com/q/3387691 \"\"\" @property def axes ( self ) : return self . Coord . _fields def __init__ ( self , axes , * args , ** kwargs ) : # Define coordinate system self . Coord = collections . namedtuple ( 'Coord' , axes ) # Write dict self . update ( * args , ** kwargs ) # Add repr / str self . Coord . __repr__ = lambda c : \",\" . join ( f \"{k}={v!r}\" for k , v in zip ( c . _fields , c )) self . Coord . __str__ = lambda c : \",\" . join ( str ( v ) for v in c ) def update ( self , * args , ** kwargs ) : \"\"\"Update using custom __setitem__().\"\"\" # See https : // stackoverflow . com / a / 2588648 # and https : // stackoverflow . com / a / 2390997 for k , v in dict ( * args , ** kwargs ). items () : self [ k ] = v def __setitem__ ( self , key , val ) : \"\"\"Setitem ensuring coordinate conforms.\"\"\" try : key = self . Coord ( * key ) except TypeError : raise TypeError ( f \"The key {key!r} did not fit the coord. system \" f \"which has axes {self.axes}\" ) super (). __setitem__ ( key , val ) def __getitem__ ( self , key ) : \"\"\"Flexible indexing.\"\"\" # List of items ( by a list of indices ). # Also see get_for (). if isinstance ( key , list ) : return [ self[k ] for k in key ] # Single ( by integer ) or list ( by Slice ) # Note : NOT validating np . int64 here catches quite a few bugs . elif isinstance ( key , int ) or isinstance ( key , slice ) : return [ *self.values() ][ key ] # Subspace ( by dict , ie . an informal , partial coordinate ) elif isinstance ( key , dict ) : outer = self . nest ( outer_axes = list ( key )) # nest coord = outer . Coord ( * key . values ()) # create coord inner = outer [ coord ] # chose subspace return inner # Single item ( by Coord object , coz an integer ( eg ) # gets interpreted ( above ) as a list index ) else : # NB : Dont 't use isinstance(key, self.Coord) # coz it fails when the namedtuple (Coord) has been # instantiated in different places (but with equal params). # Also see bugs.python.org/issue7796 return super().__getitem__(key) def __getkey__(self, entry): \"\"\"Inverse of dict.__getitem__(), but also works on coords. Note: This dunder method is not a \"builtin\" naming convention.\"\"\" coord = (getattr(entry, a, None) for a in self.axes) return self.Coord(*coord) def __call__(self, **kwargs): \"\"\"Convenience, that enables, eg.: >>> xp_dict(da_method=\"EnKF\", infl=1, seed=3) \"\"\" return self.__getitem__(kwargs) def get_for(self, ticks, default=None): \"\"\"Almost ``[self.get(Coord(x)) for x in ticks]``. NB: using the \"naive\" thing: ``[self[x] for x in ticks]`` would probably be a BUG coz x gets interpreted as indices for the internal list.\"\"\" singleton = not hasattr(ticks[0], \"__iter__\") def coord(xyz): return self.Coord(xyz if singleton else xyz) return [self.get(coord(x), default) for x in ticks] def coords(self, **kwargs): \"\"\"Get all ``coord``s matching kwargs. Unlike ``__getitem__(kwargs)``, - A list is returned, not a subspace. - This list constains keys (coords), not values. - The coords refer to the original space, not the subspace. The last point is especially useful for label_xSection(). \"\"\" def embed(coord): return {**kwargs, **coord._asdict()} return [self.Coord(**embed(x)) for x in self[kwargs]] # Old implementation. # - I prefer the new version for its re-use of __getitem__' s # nesting , evidencing their mutual relationship ) # - Note that unlike xpList . inds () : missingval shenanigans # are here unnecessary coz each coordinate is complete . # match = lambda x : all ( getattr ( x , k ) == kwargs [ k ] for k in kwargs ) # return [ x for x in self if match(x) ] def __repr__ ( self ) : # Note : print ( xpList ( self )) produces more human - readable key listing , # but we don 't want to implement it here, coz it requires split_attrs(), # which we don' t really want to call again . L = 2 keys = [ str(k) for k in self ] if 2 * L < len ( keys ) : keys = keys [ :L ] + [ \"...\" ] + keys [ -L: ] keys = \"[\\n \" + \",\\n \" . join ( keys ) + \"\\n]\" txt = f \"<{self.__class__.__name__}> with {len(self)} keys: {keys}\" # txt += \" befitting the coord. sys. with axes \" txt += \"\\nplaced in a coord-sys with axes \" try : txt += \"(and ticks):\" + str ( dict_tools . AlignedDict ( self . ticks )) except AttributeError : txt += \":\\n\" + str ( self . axes ) return txt def nest ( self , inner_axes = None , outer_axes = None ) : \"\"\"Return a new xpSpace with axes ``outer_axes``, obtained by projecting along the ``inner_axes``. The entries of this ``xpSpace`` are themselves ``xpSpace``s, with axes ``inner_axes``, each one regrouping the entries with the same (projected) coordinate. Note: is also called by ``__getitem__(key)`` if ``key`` is dict.\"\"\" # Default : a singleton outer space , # with everything contained in the inner ( projection ) space . if inner_axes is None and outer_axes is None : outer_axes = () # Validate axes if inner_axes is None : assert outer_axes is not None inner_axes = dict_tools . complement ( self . axes , outer_axes ) else : assert outer_axes is None outer_axes = dict_tools . complement ( self . axes , inner_axes ) # Fill spaces outer_space = self . __class__ ( outer_axes ) for coord , entry in self . items () : outer_coord = outer_space . __getkey__ ( coord ) try : inner_space = outer_space [ outer_coord ] except KeyError : inner_space = self . __class__ ( inner_axes ) outer_space [ outer_coord ] = inner_space inner_space [ inner_space.__getkey__(coord) ] = entry return outer_space def add_axis ( self , axis ) : self . __init__ ( self . axes + ( axis ,)) for coord in list ( self ) : entry = self . pop ( coord ) self [ coord + (None,) ] = entry def intersect_axes ( self , attrs ) : \"\"\"Rm those a in attrs that are not in self.axes. This allows errors in the axes allotment, for ease-of-use.\"\"\" absent = dict_tools . complement ( attrs , self . axes ) if absent : print ( color_text ( \"Warning:\" , colorama . Fore . RED ), \"The requested attributes\" , color_text ( str ( absent ), colorama . Fore . RED ), ( \"were not found among the\" \" xpSpace axes (attrs. used as coordinates\" \" for the set of experiments).\" \" This may be no problem if the attr. is redundant\" \" for the coord-sys.\" \" However, if it is caused by confusion or mis-spelling,\" \" then it is likely to cause mis-interpretation\" \" of the shown results.\" )) attrs = dict_tools . complement ( attrs , absent ) return attrs def label_xSection ( self , label , * NoneAttrs , ** sub_coord ) : \"\"\"Insert duplicate entries for the cross section whose ``coord``s match ``sub_coord``, adding the attr ``Const=label`` to their ``coord``, reflecting the \" constance / constraint / fixation \" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up in ``nest()``. If you wish, you can specify the ``NoneAttrs``, which are consequently set to None for the duplicated entries, preventing them from getting plotted in tuning panels. \"\"\" if \"Const\" not in self . axes : self . add_axis ( 'Const' ) for coord in self . coords ( ** self . intersect_axes ( sub_coord )) : entry = copy . deepcopy ( self [ coord ] ) coord = coord . _replace ( Const = label ) coord = coord . _replace ( ** { a : None for a in NoneAttrs } ) self [ coord ] = entry Ancestors (in MRO) builtins.dict Descendants dapper.data_management.xpSpace Instance variables axes Methods add_axis def add_axis ( self , axis ) View Source def add_axis ( self , axis ): self . __init__ ( self . axes + ( axis ,)) for coord in list ( self ): entry = self . pop ( coord ) self [ coord + ( None ,)] = entry clear def clear ( ... ) D.clear() -> None. Remove all items from D. coords def coords ( self , ** kwargs ) Get all coord s matching kwargs. Unlike __getitem__(kwargs) , - A list is returned, not a subspace. - This list constains keys (coords), not values. - The coords refer to the original space, not the subspace. The last point is especially useful for label_xSection(). View Source def coords ( self , ** kwargs ): \"\"\"Get all ``coord``s matching kwargs. Unlike ``__getitem__(kwargs)``, - A list is returned, not a subspace. - This list constains keys (coords), not values. - The coords refer to the original space, not the subspace. The last point is especially useful for label_xSection(). \"\"\" def embed ( coord ): return { ** kwargs , ** coord . _asdict ()} return [ self . Coord ( ** embed ( x )) for x in self [ kwargs ]] copy def copy ( ... ) D.copy() -> a shallow copy of D fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. get_for def get_for ( self , ticks , default = None ) Almost [self.get(Coord(x)) for x in ticks] . NB: using the \"naive\" thing: [self[x] for x in ticks] would probably be a BUG coz x gets interpreted as indices for the internal list. View Source def get_for ( self , ticks , default = None ) : \"\"\"Almost ``[self.get(Coord(x)) for x in ticks]``. NB: using the \" naive \" thing: ``[self[x] for x in ticks]`` would probably be a BUG coz x gets interpreted as indices for the internal list.\"\"\" singleton = not hasattr ( ticks [ 0 ] , \"__iter__\" ) def coord ( xyz ) : return self . Coord ( xyz if singleton else xyz ) return [ self.get(coord(x), default) for x in ticks ] intersect_axes def intersect_axes ( self , attrs ) Rm those a in attrs that are not in self.axes. This allows errors in the axes allotment, for ease-of-use. View Source def intersect_axes ( self , attrs ): \"\"\"Rm those a in attrs that are not in self.axes. This allows errors in the axes allotment, for ease-of-use.\"\"\" absent = dict_tools . complement ( attrs , self . axes ) if absent : print ( color_text ( \"Warning:\" , colorama . Fore . RED ), \"The requested attributes\" , color_text ( str ( absent ), colorama . Fore . RED ), ( \"were not found among the\" \" xpSpace axes (attrs. used as coordinates\" \" for the set of experiments).\" \" This may be no problem if the attr. is redundant\" \" for the coord-sys.\" \" However, if it is caused by confusion or mis-spelling,\" \" then it is likely to cause mis-interpretation\" \" of the shown results.\" )) attrs = dict_tools . complement ( attrs , absent ) return attrs items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys label_xSection def label_xSection ( self , label , * NoneAttrs , ** sub_coord ) Insert duplicate entries for the cross section whose coord s match sub_coord , adding the attr Const=label to their coord , reflecting the \"constance/constraint/fixation\" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up in nest() . If you wish, you can specify the NoneAttrs , which are consequently set to None for the duplicated entries, preventing them from getting plotted in tuning panels. View Source def label_xSection ( self , label , * NoneAttrs , ** sub_coord ) : \" \"\" Insert duplicate entries for the cross section whose ``coord``s match ``sub_coord``, adding the attr ``Const=label`` to their ``coord``, reflecting the \" constance / constraint / fixation \" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up in ``nest()``. If you wish, you can specify the ``NoneAttrs``, which are consequently set to None for the duplicated entries, preventing them from getting plotted in tuning panels. \"\" \" if \"Const\" not in self . axes : self . add_axis ( 'Const' ) for coord in self . coords ( ** self . intersect_axes ( sub_coord )) : entry = copy . deepcopy ( self [ coord ] ) coord = coord . _replace ( Const = label ) coord = coord . _replace ( ** { a : None for a in NoneAttrs } ) self [ coord ] = entry nest def nest ( self , inner_axes = None , outer_axes = None ) Return a new xpSpace with axes outer_axes , obtained by projecting along the inner_axes . The entries of this xpSpace are themselves xpSpace s, with axes inner_axes , each one regrouping the entries with the same (projected) coordinate. Note: is also called by __getitem__(key) if key is dict. View Source def nest ( self , inner_axes = None , outer_axes = None ) : \" \"\" Return a new xpSpace with axes ``outer_axes``, obtained by projecting along the ``inner_axes``. The entries of this ``xpSpace`` are themselves ``xpSpace``s, with axes ``inner_axes``, each one regrouping the entries with the same (projected) coordinate. Note: is also called by ``__getitem__(key)`` if ``key`` is dict. \"\" \" # Default: a singleton outer space, # with everything contained in the inner (projection) space. if inner_axes is None and outer_axes is None : outer_axes = () # Validate axes if inner_axes is None : assert outer_axes is not None inner_axes = dict_tools . complement ( self . axes , outer_axes ) else : assert outer_axes is None outer_axes = dict_tools . complement ( self . axes , inner_axes ) # Fill spaces outer_space = self . __class__ ( outer_axes ) for coord , entry in self . items () : outer_coord = outer_space . __getkey__ ( coord ) try : inner_space = outer_space [ outer_coord ] except KeyError : inner_space = self . __class__ ( inner_axes ) outer_space [ outer_coord ] = inner_space inner_space [ inner_space . __getkey__ ( coord ) ] = entry return outer_space pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. update def update ( self , * args , ** kwargs ) Update using custom setitem (). View Source def update ( self , * args , ** kwargs ) : \"\"\"Update using custom __setitem__().\"\"\" # See https : // stackoverflow . com / a / 2588648 # and https : // stackoverflow . com / a / 2390997 for k , v in dict ( * args , ** kwargs ). items () : self [ k ] = v values def values ( ... ) D.values() -> an object providing a view on D's values xpSpace class xpSpace ( axes , * args , ** kwargs ) Functionality to facilitate working with xps and their results. The function from_list() initializes a SparseSpace from a list of objects, typically experiments referred to as xps , by (1) computing the relevant axes from the attributes, and (2) filling the dict by xps . The main use of xpSpace is through its print() & plot() , both of which call table_tree() to nest the axes of the SparseSpace. For custom plotting, you will likely want to start with table_tree() . Using from_list(xps) creates a SparseSpace holding xps . However, the nested xpSpace s output by table_tree() , will hold objects of type UncertainQtty , coz table_tree() calls mean() calls field(statkey) . View Source class xpSpace ( SparseSpace ) : \"\"\"Functionality to facilitate working with ``xps`` and their results. The function ``from_list()`` initializes a ``SparseSpace`` from a list of objects, typically experiments referred to as ``xps``, by (1) computing the relevant axes from the attributes, and (2) filling the dict by ``xps``. The main use of xpSpace is through its ``print()`` & ``plot()``, both of which call ``table_tree()`` to nest the axes of the SparseSpace. For custom plotting, you will likely want to start with ``table_tree()``. Using ``from_list(xps)`` creates a SparseSpace holding ``xps``. However, the nested ``xpSpace``s output by ``table_tree()``, will hold objects of type ``UncertainQtty``, coz ``table_tree()`` calls ``mean()`` calls ``field(statkey)``.\"\"\" @classmethod def from_list ( cls , xps ) : \"\"\"Init xpSpace from xpList.\"\"\" def make_ticks ( axes , ordering = dict ( N = 'default' , seed = 'default' , infl = 'default' , loc_rad = 'default' , rot = 'as_found' , da_method = 'as_found' , )) : \"\"\"Unique & sort, for each axis (individually) in axes.\"\"\" for ax_name , arr in axes . items () : ticks = set ( arr ) # unique ( jumbles order ) # Sort order = ordering . get ( ax_name , 'default' ). lower () if hasattr ( order , '__call__' ) : # eg . mylist . index ticks = sorted ( ticks , key = order ) elif 'as_found' in order : ticks = sorted ( ticks , key = arr . index ) else : # default sorting , with None placed at the end ticks = sorted ( ticks , key = lambda x : ( x is None , x )) if any ( x in order for x in [ 'rev' , 'inv' ]) : ticks = ticks [ ::- 1 ] axes [ ax_name ] = ticks # Define axes xp_list = xpList ( xps ) axes = xp_list . split_attrs ( nomerge = [ 'Const' ])[ 0 ] make_ticks ( axes ) self = cls ( axes . keys ()) # Note : this attr ( ticks ) will not be propagated through nest (). # That is fine . Otherwise we should have to prune the ticks # ( if they are to be useful ), which we don't want to do. self.ticks = axes # Fill self.update({self.__getkey__(xp): xp for xp in xps}) return self def field(self, statkey=\"rmse.a\"): \"\"\"Extract ``statkey`` for each item in ``self``.\"\"\" # Init a new xpDict to hold field avrgs = self.__class__(self.axes) found_anything = False for coord, xp in self.items(): val = getattr(xp.avrgs, statkey, None) avrgs[coord] = val found_anything = found_anything or (val is not None) if not found_anything: raise AttributeError( f\"The stat. field ' { statkey } ' was not found\" \" among any of the xp's . \") return avrgs def mean(self, axes=None): # Note: The case ``axes=()`` should work w/o special treatment. if axes is None: return self nested = self.nest(axes) for coord, space in nested.items(): def getval(uq): return uq.val if isinstance(uq, UncertainQtty) else uq vals = [getval(uq) for uq in space.values()] # Don't use nanmean! It would give false impressions. mu = np.mean(vals) with warnings.catch_warnings(): warnings.simplefilter(\" ignore \", category=RuntimeWarning) # Don't print warnings caused by N=1. # It already correctly yield nan's. var = np.var(vals, ddof=1) N = len(vals) uq = UncertainQtty(mu, np.sqrt(var/N)) uq.nTotal = N uq.nFail = N - np.isfinite(vals).sum() uq.nSuccess = N - uq.nFail nested[coord] = uq return nested def tune(self, axes=None, costfun=None): \"\"\" Get ( compile / tabulate ) a stat field optimised wrt . tuning params . \"\"\" # Define cost-function costfun = (costfun or 'increasing').lower() if 'increas' in costfun: costfun = (lambda x: +x) elif 'decreas' in costfun: costfun = (lambda x: -x) else: assert hasattr(costfun, '__call__') # custom # Note: The case ``axes=()`` should work w/o special treatment. if axes is None: return self nested = self.nest(axes) for coord, space in nested.items(): # Find optimal value and coord within space MIN = np.inf for i, (inner_coord, uq) in enumerate(space.items()): cost = costfun(uq.val) if cost <= MIN: MIN = cost uq_opt = uq uq_opt.tuned_coord = inner_coord nested[coord] = uq_opt return nested def validate_axes(self, axes): \"\"\" Validate axes . Note : This does not convert None to (), allowing None to remain special . Use `` axis or () `` wherever tuples are required . \"\"\" roles = {} # \" inv \" for role in set(axes) | set(AXES_ROLES): assert role in AXES_ROLES, f\" Invalid role { role ! r } \" aa = axes.get(role, AXES_ROLES[role]) if aa is None: pass # Purposely special else: # Ensure iterable if isinstance(aa, str) or not hasattr(aa, \" __ iter__ \"): aa = (aa,) aa = self.intersect_axes(aa) for axis in aa: # Ensure unique if axis in roles: raise TypeError( f\" An axis ( here { axis ! r }) cannot be assigned to 2 \" f\" roles ( here { role ! r } and { roles [ axis ]! r }). \") else: roles[axis] = role axes[role] = aa return axes def table_tree(self, statkey, axes): \"\"\" Hierarchical nest () : xp_dict > outer > inner > mean > optim . as specified by `` axes `` . Returns this new xpSpace . - print_1d / plot_1d ( respectively ) separate tables / panel ( row ) s for `` axes [ 'outer' ] `` , and columns / x - axis for `` axes [ 'inner' ] `` . - The `` axes [ 'mean' ] `` and `` axes [ 'optim' ] `` get eliminated by the mean () / tune () operations . Note : cannot support multiple statkeys because it's not (obviously) meaningful when optimizing over tuning_axes. \"\"\" axes = self.validate_axes(axes) def mean_tune(xp_dict): \"\"\"Take mean, then tune. Note: the SparseDict implementation should be sufficiently \"uncluttered\" that mean_tune() (or a few of its code lines) could be called anywhere above/between/below the ``nest()``ing of ``outer`` or ``inner``. These possibile call locations are commented in the code.\"\"\" uq_dict = xp_dict.field(statkey) uq_dict = uq_dict.mean(axes['mean']) uq_dict = uq_dict.tune(axes['optim']) return uq_dict self = mean_tune(self) # Prefer calling mean_tune() [also see its docstring] # before doing outer/inner nesting. This is because then the axes of # a row (xpSpace) should not include mean&optim, and thus: # - Column header/coords may be had directly as row.keys(), # without extraction by __getkey__() from (e.g.) row[0]. # - Don't need to propagate mean&optim axes down to the row level . # which would require defining rows by the nesting : # rows = table . nest ( outer_axes = dict_tools . complement ( table . axes , # * ( axes [ 'inner' ] or ()), # * ( axes [ 'mean' ] or ()), # * ( axes [ 'optim' ] or ()) )) # - Each level of the output from table_tree # is a smaller ( and more manageable ) dict . tables = self . nest ( outer_axes = axes [ 'outer' ]) for table_coord , table in tables . items () : # table = mean_tune ( table ) # Should not be used ( nesting as rows is more natural , # and is required for getting distinct / row_keys ). # cols = table . nest ( outer_axes = axes [ 'inner' ]) rows = table . nest ( inner_axes = axes [ 'inner' ] or ()) # Overwrite table by its nesting as rows tables [ table_coord ] = rows # for row_coord , row in rows . items () : # rows [ row_coord ] = mean_tune ( row ) return axes , tables def tickz ( self , axis_name ) : \"\"\"Axis ticks without None\"\"\" return [ x for x in self . ticks [ axis_name ] if x is not None ] def print ( self , statkey= \"rmse.a\" , axes = AXES_ROLES , subcols = True , decimals = None ) : \"\"\"Print tables of results. - statkey: The statistical field from the experiments to report. - subcols: If True, then subcolumns are added to indicate the 1\u03c3 confidence interval, and potentially some other stuff. - axes: Allots (maps) each role to a set of axis of the xp_dict. Suggestion: >>> dict( >>> outer='da_method', inner='N', mean='seed', >>> optim=('infl','loc_rad')) Example: If ``mean`` is assigned to: - (\" seed \",): Experiments are averaged accross seeds, and the 1\u03c3 (sub)col is computed as sqrt(var(xps)/N), where xps is a set of experiments. - () : Experiments are averaged across nothing (i.e. this is an edge case). - None : Experiments are not averaged (i.e. the values are the same as above), and the 1\u03c3 (sub)col is computed from the time series of that single experiment. \"\"\" import pandas as pd def align_subcols ( rows , cc , subcols , h2 ) : \"\"\"Subcolumns: align, justify, join.\"\"\" # Define subcol formats subc = dict () subc [ 'keys' ] = [ \"val\" , \"conf\" ] subc [ 'headers' ] = [ statkey , '1\u03c3' ] subc [ 'frmts' ] = [ None , None ] subc [ 'spaces' ] = [ ' \u00b1' , ] # last one gets appended below . subc [ 'aligns' ] = [ '>' , '<' ] # 4 header -- matter gets decimal - aligned . if axes [ 'optim' ] is not None : subc [ 'keys' ] += [ \"tuned_coord\" ] subc [ 'headers' ] += [ axes [ 'optim' ]] subc [ 'frmts' ] += [ lambda x : tuple ( a for a in x )] subc [ 'spaces' ] += [ ' *' ] subc [ 'aligns' ] += [ '<' ] elif axes [ 'mean' ] is not None : subc [ 'keys' ] += [ \"nFail\" , \"nSuccess\" ] subc [ 'headers' ] += [ '\u2620' , '\u2713' ] # use width - 1 symbols ! subc [ 'frmts' ] += [ None , None ] subc [ 'spaces' ] += [ ' ' , ' ' ] subc [ 'aligns' ] += [ '>' , '>' ] subc [ 'spaces' ]. append ( '' ) # no space after last subcol template = '{}' + '{}' . join ( subc [ 'spaces' ]) # Transpose columns = [ list ( x ) for x in zip ( * rows )] # Iterate over columns . for j , ( col_coord , column ) in enumerate ( zip ( cc , columns )) : # Tabulate columns if subcols : column = unpack_uqs ( column , decimals , subc [ \"keys\" ]) # Tabulate subcolumns subheaders = [] for key , header , frmt , _ , align in zip ( * subc . values ()) : column [ key ] = tabulate_column ( column [ key ], header , frmt = frmt )[ 1 : ] L = len ( column [ - 1 ][ key ]) if align == '<': subheaders += [ str ( header ). ljust ( L )] else : subheaders += [ str ( header ). rjust ( L )] # Join subcolumns : matter = [ template . format ( * [ row [ k ] for k in subc [ 'keys' ]]) for row in column ] header = template . format ( * subheaders ) else : column = unpack_uqs ( column , decimals )[ \"val\" ] column = tabulate_column ( column , statkey ) header , matter = column [ 0 ], column [ 1 : ] if h2: # Do super_header if j : super_header = str ( col_coord ) else : super_header = repr ( col_coord ) width = len ( header ) # += 1 if using unicode chars like \u2714\ufe0f super_header = super_header . center ( width , \"_\" ) header = super_header + \"\\n\" + header columns [ j ] = [ header ] + matter # Un - transpose rows = [ list ( x ) for x in zip ( * columns )] return rows # Inform axes [ \"mean\" ] if axes . get ( 'mean' , None ) : print ( f \"Averages (in time and) over {axes['mean']}.\" ) else : print ( \"Averages in time only\" \" (=> the 1\u03c3 estimates may be unreliable).\" ) axes , tables = self . table_tree ( statkey , axes ) for table_coord , table in tables . items () : # Get this table's column coords (cc). Use dict for sorted&unique. # cc = xp_dict.ticks[axes[\"inner\"]] # May be larger than needed. # cc = table[0].keys() # May be too small a set. cc = {c: None for row in table.values() for c in row} # Convert table (rows) into rows (lists) of equal length rows = [[row.get(c, None) for c in cc] for row in table.values()] if False: # ****************** Simple (for debugging) table for i, (row_coord, row) in enumerate(zip(table, rows)): row_key = \", \".join(str(v) for v in row_coord) rows[i] = [row_key] + row rows.insert(0, [f\"{table.axes}\"] + [repr(c) for c in cc]) else: # ********************** Elegant table. h2 = \"\\n\" if len(cc) > 1 else \"\" # do column-super-header rows = align_subcols(rows, cc, subcols, h2) # Make and prepend left-side table # - It's prettier if row_keys don't have unnecessary cols. # For example, the table of Climatology should not have an # entire column repeatedly displaying \"infl=None\". # => split_attrs(). # - Why didn't we do this for the column attrs? # Coz there we have no ambition to split the attrs , # which would also require excessive processing : # nesting the table as cols , and then split_attrs () on cols . row_keys = xpList ( table . keys ()). split_attrs ()[ 0 ] row_keys = pd . DataFrame . from_dict ( row_keys , dtype= \"O\" ) # allow storing None if len ( row_keys . columns ) : # Header rows [ 0 ] = [ h2 + k for k in row_keys ] + [ h2+'\u244a' ] + rows [ 0 ] # Matter for row , ( i , key ) in zip ( rows [ 1 : ], row_keys . iterrows ()) : rows [ i + 1 ] = [ * key ] + [ '|' ] + row # Print print ( \"\\n\" , end= \"\" ) if axes [ 'outer' ] : table_title = \"Table for \" + repr ( table_coord ) print ( color_text ( table_title , colorama . Back . YELLOW )) headers , * rows = rows print ( utils . tab ( rows , headers ). replace ( '\u2423' , ' ' )) def plot ( self , statkey= \"rmse.a\" , axes = AXES_ROLES , get_style = default_styles , fignum = None , figsize = None , panels = None , title2 = None , costfun = None , unique_labels = True ) : \"\"\"Plot the avrgs of ``statkey`` as a function of ``axis[\" inner \"]``. Firs of all, though, mean and optimum computations are done for ``axis[\" mean \"]`` and ``axis[\" optim \"]``. The optimal parameters are plotted in panels below the main plot. This can be prevented by providing the figure axes in ``panels``. Optionally, the experiments can be grouped by ``axis[\" outer \"]``, producing a figure with columns of panels.\"\"\" def plot1 ( panelcol , row , style ) : \"\"\"Plot a given line (row) in the main panel and the optim panels. Involves: Sort, insert None's, handle constant lines.\"\"\" # Make a full row ( yy ) of vals , whether is_constant or not . # row . is_constant = ( len ( row ) == 1 and next ( iter ( row )) == row . Coord ( None )) row . is_constant = all ( x == row . Coord ( None ) for x in row ) yy = [ row [ 0 ] if row . is_constant else y for y in row . get_for ( xticks )] # Plot main row . vals = [ getattr ( y , 'val' , None ) for y in yy ] row . handles = {} row . handles [ \"main_panel\" ] = panelcol [ 0 ]. plot ( xticks , row . vals , **style )[ 0 ] # Plot tuning params row . tuned_coords = {} # Store ordered , \"transposed\" argmins argmins = [ getattr ( y , 'tuned_coord' , None ) for y in yy ] for a , panel in zip ( axes [ \"optim\" ], panelcol [ 1 : ]) : yy = [ getattr ( coord , a , None ) for coord in argmins ] row . tuned_coords [ a ] = yy # Plotting all None 's sets axes units (like any plotting call) # which can cause trouble if the axes units were actually supposed # to be categorical (eg upd_a), but this is only revealed later. if not all(y == None for y in yy): row.handles[a] = panel.plot(xticks, yy, **style) # Nest axes through table_tree() assert len(axes[\"inner\"]) == 1, \"You must chose the abscissa.\" axes, tables = self.table_tree(statkey, axes) xticks = self.tickz(axes[\"inner\"][0]) # Figure panels if panels is None: nrows = len(axes['optim'] or ()) + 1 ncols = len(tables) maxW = 12.7 # my mac screen figsize = figsize or (min(5*ncols, maxW), 7) gs = dict( height_ratios=[6]+[1]*(nrows-1), hspace=0.05, wspace=0.05, # eyeballed: left=0.15/(1+np.log(ncols)), right=0.97, bottom=0.06, top=0.9) # Create _, panels = freshfig(num=fignum, figsize=figsize, nrows=nrows, sharex=True, ncols=ncols, sharey='row', gridspec_kw=gs) panels = np.ravel(panels).reshape((-1, ncols)) else: panels = np.atleast_2d(panels) # Title fig = panels[0, 0].figure fig_title = \"Average wrt. time\" if axes[\"mean\"] is not None: fig_title += f\" and {axes['mean']}\" if title2 is not None: fig_title += \"\\n\" + str(title2) fig.suptitle(fig_title) # Loop outer label_register = set() # mv inside loop to get legend on each panel for table_panels, (table_coord, table) in zip(panels.T, tables.items()): table.panels = table_panels title = '' if axes[\"outer\"] is None else repr(table_coord) # Plot for coord, row in table.items(): style = get_style(coord) # Rm duplicate labels (contrary to coords, labels can # be \"tampered\" with, and so can be duplicate) if unique_labels: if style.get(\"label\", None) in label_register: del style[\"label\"] else: label_register.add(style[\"label\"]) plot1(table.panels, row, style) # Beautify panel0 = table.panels[0] panel0.set_title(title) if panel0.is_first_col(): panel0.set_ylabel(statkey) with utils.set_tmp(mpl_logger, 'level ' , 99 ) : # silence \"no label\" msg panel0 . legend () table . panels [ - 1 ]. set_xlabel ( axes [ \"inner\" ][ 0 ]) # Tuning panels : for a , panel in zip ( axes [ \"optim\" ] or (), table . panels [ 1 : ]) : if panel . is_first_col () : panel . set_ylabel ( f \"Optim.\\n{a}\" ) tables . fig = fig tables . xp_dict = self tables . axes_roles = axes return tables Ancestors (in MRO) dapper.data_management.SparseSpace builtins.dict Static methods from_list def from_list ( xps ) Init xpSpace from xpList. View Source @classmethod def from_list ( cls , xps ) : \"\"\"Init xpSpace from xpList.\"\"\" def make_ticks ( axes , ordering = dict ( N = 'default' , seed = 'default' , infl = 'default' , loc_rad = 'default' , rot = 'as_found' , da_method = 'as_found' , )) : \"\"\"Unique & sort, for each axis (individually) in axes.\"\"\" for ax_name , arr in axes . items () : ticks = set ( arr ) # unique ( jumbles order ) # Sort order = ordering . get ( ax_name , 'default' ). lower () if hasattr ( order , '__call__' ) : # eg . mylist . index ticks = sorted ( ticks , key = order ) elif 'as_found' in order : ticks = sorted ( ticks , key = arr . index ) else : # default sorting , with None placed at the end ticks = sorted ( ticks , key = lambda x : ( x is None , x )) if any ( x in order for x in [ 'rev' , 'inv' ]) : ticks = ticks [ ::- 1 ] axes [ ax_name ] = ticks # Define axes xp_list = xpList ( xps ) axes = xp_list . split_attrs ( nomerge = [ 'Const' ])[ 0 ] make_ticks ( axes ) self = cls ( axes . keys ()) # Note : this attr ( ticks ) will not be propagated through nest (). # That is fine . Otherwise we should have to prune the ticks # ( if they are to be useful ), which we don ' t want to do . self . ticks = axes # Fill self . update ({ self . __ getkey__ ( xp ) : xp for xp in xps }) return self Instance variables axes Methods add_axis def add_axis ( self , axis ) View Source def add_axis ( self , axis ): self . __init__ ( self . axes + ( axis ,)) for coord in list ( self ): entry = self . pop ( coord ) self [ coord + ( None ,)] = entry clear def clear ( ... ) D.clear() -> None. Remove all items from D. coords def coords ( self , ** kwargs ) Get all coord s matching kwargs. Unlike __getitem__(kwargs) , - A list is returned, not a subspace. - This list constains keys (coords), not values. - The coords refer to the original space, not the subspace. The last point is especially useful for label_xSection(). View Source def coords ( self , ** kwargs ): \"\"\"Get all ``coord``s matching kwargs. Unlike ``__getitem__(kwargs)``, - A list is returned, not a subspace. - This list constains keys (coords), not values. - The coords refer to the original space, not the subspace. The last point is especially useful for label_xSection(). \"\"\" def embed ( coord ): return { ** kwargs , ** coord . _asdict ()} return [ self . Coord ( ** embed ( x )) for x in self [ kwargs ]] copy def copy ( ... ) D.copy() -> a shallow copy of D field def field ( self , statkey = 'rmse.a' ) Extract statkey for each item in self . View Source def field ( self , statkey = \"rmse.a\" ) : \" \"\" Extract ``statkey`` for each item in ``self``. \"\" \" # Init a new xpDict to hold field avrgs = self . __class__ ( self . axes ) found_anything = False for coord , xp in self . items () : val = getattr ( xp . avrgs , statkey , None ) avrgs [ coord ] = val found_anything = found_anything or ( val is not None ) if not found_anything : raise AttributeError ( f \"The stat. field '{statkey}' was not found\" \" among any of the xp's.\" ) return avrgs fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. get_for def get_for ( self , ticks , default = None ) Almost [self.get(Coord(x)) for x in ticks] . NB: using the \"naive\" thing: [self[x] for x in ticks] would probably be a BUG coz x gets interpreted as indices for the internal list. View Source def get_for ( self , ticks , default = None ) : \"\"\"Almost ``[self.get(Coord(x)) for x in ticks]``. NB: using the \" naive \" thing: ``[self[x] for x in ticks]`` would probably be a BUG coz x gets interpreted as indices for the internal list.\"\"\" singleton = not hasattr ( ticks [ 0 ] , \"__iter__\" ) def coord ( xyz ) : return self . Coord ( xyz if singleton else xyz ) return [ self.get(coord(x), default) for x in ticks ] intersect_axes def intersect_axes ( self , attrs ) Rm those a in attrs that are not in self.axes. This allows errors in the axes allotment, for ease-of-use. View Source def intersect_axes ( self , attrs ): \"\"\"Rm those a in attrs that are not in self.axes. This allows errors in the axes allotment, for ease-of-use.\"\"\" absent = dict_tools . complement ( attrs , self . axes ) if absent : print ( color_text ( \"Warning:\" , colorama . Fore . RED ), \"The requested attributes\" , color_text ( str ( absent ), colorama . Fore . RED ), ( \"were not found among the\" \" xpSpace axes (attrs. used as coordinates\" \" for the set of experiments).\" \" This may be no problem if the attr. is redundant\" \" for the coord-sys.\" \" However, if it is caused by confusion or mis-spelling,\" \" then it is likely to cause mis-interpretation\" \" of the shown results.\" )) attrs = dict_tools . complement ( attrs , absent ) return attrs items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys label_xSection def label_xSection ( self , label , * NoneAttrs , ** sub_coord ) Insert duplicate entries for the cross section whose coord s match sub_coord , adding the attr Const=label to their coord , reflecting the \"constance/constraint/fixation\" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up in nest() . If you wish, you can specify the NoneAttrs , which are consequently set to None for the duplicated entries, preventing them from getting plotted in tuning panels. View Source def label_xSection ( self , label , * NoneAttrs , ** sub_coord ) : \" \"\" Insert duplicate entries for the cross section whose ``coord``s match ``sub_coord``, adding the attr ``Const=label`` to their ``coord``, reflecting the \" constance / constraint / fixation \" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up in ``nest()``. If you wish, you can specify the ``NoneAttrs``, which are consequently set to None for the duplicated entries, preventing them from getting plotted in tuning panels. \"\" \" if \"Const\" not in self . axes : self . add_axis ( 'Const' ) for coord in self . coords ( ** self . intersect_axes ( sub_coord )) : entry = copy . deepcopy ( self [ coord ] ) coord = coord . _replace ( Const = label ) coord = coord . _replace ( ** { a : None for a in NoneAttrs } ) self [ coord ] = entry mean def mean ( self , axes = None ) View Source def mean ( self , axes = None ) : # Note : The case `` axes = () `` should work w / o special treatment . if axes is None : return self nested = self . nest ( axes ) for coord , space in nested . items () : def getval ( uq ) : return uq . val if isinstance ( uq , UncertainQtty ) else uq vals = [ getval(uq) for uq in space.values() ] # Don 't use nanmean! It would give false impressions. mu = np.mean(vals) with warnings.catch_warnings(): warnings.simplefilter(\"ignore\", category=RuntimeWarning) # Don' t print warnings caused by N = 1. # It already correctly yield nan ' s . var = np . var ( vals , ddof = 1 ) N = len ( vals ) uq = UncertainQtty ( mu , np . sqrt ( var / N )) uq . nTotal = N uq . nFail = N - np . isfinite ( vals ). sum () uq . nSuccess = N - uq . nFail nested [ coord ] = uq return nested nest def nest ( self , inner_axes = None , outer_axes = None ) Return a new xpSpace with axes outer_axes , obtained by projecting along the inner_axes . The entries of this xpSpace are themselves xpSpace s, with axes inner_axes , each one regrouping the entries with the same (projected) coordinate. Note: is also called by __getitem__(key) if key is dict. View Source def nest ( self , inner_axes = None , outer_axes = None ) : \" \"\" Return a new xpSpace with axes ``outer_axes``, obtained by projecting along the ``inner_axes``. The entries of this ``xpSpace`` are themselves ``xpSpace``s, with axes ``inner_axes``, each one regrouping the entries with the same (projected) coordinate. Note: is also called by ``__getitem__(key)`` if ``key`` is dict. \"\" \" # Default: a singleton outer space, # with everything contained in the inner (projection) space. if inner_axes is None and outer_axes is None : outer_axes = () # Validate axes if inner_axes is None : assert outer_axes is not None inner_axes = dict_tools . complement ( self . axes , outer_axes ) else : assert outer_axes is None outer_axes = dict_tools . complement ( self . axes , inner_axes ) # Fill spaces outer_space = self . __class__ ( outer_axes ) for coord , entry in self . items () : outer_coord = outer_space . __getkey__ ( coord ) try : inner_space = outer_space [ outer_coord ] except KeyError : inner_space = self . __class__ ( inner_axes ) outer_space [ outer_coord ] = inner_space inner_space [ inner_space . __getkey__ ( coord ) ] = entry return outer_space plot def plot ( self , statkey = 'rmse.a' , axes = { 'outer' : None , 'inner' : None , 'mean' : None , 'optim' : None }, get_style =< function default_styles at 0x7f8a722ab790 > , fignum = None , figsize = None , panels = None , title2 = None , costfun = None , unique_labels = True ) Plot the avrgs of statkey as a function of axis[\"inner\"] . Firs of all, though, mean and optimum computations are done for axis[\"mean\"] and axis[\"optim\"] . The optimal parameters are plotted in panels below the main plot. This can be prevented by providing the figure axes in panels . Optionally, the experiments can be grouped by axis[\"outer\"] , producing a figure with columns of panels. View Source def plot ( self , statkey = \"rmse.a\" , axes = AXES_ROLES , get_style = default_styles , fignum = None , figsize = None , panels = None , title2 = None , costfun = None , unique_labels = True ): \"\"\"Plot the avrgs of ``statkey`` as a function of ``axis[\"inner\"]``. Firs of all, though, mean and optimum computations are done for ``axis[\"mean\"]`` and ``axis[\"optim\"]``. The optimal parameters are plotted in panels below the main plot. This can be prevented by providing the figure axes in ``panels``. Optionally, the experiments can be grouped by ``axis[\"outer\"]``, producing a figure with columns of panels.\"\"\" def plot1 ( panelcol , row , style ): \"\"\"Plot a given line (row) in the main panel and the optim panels. Involves: Sort, insert None's, handle constant lines.\"\"\" # Make a full row (yy) of vals, whether is_constant or not. # row.is_constant = (len(row)==1 and next(iter(row))==row.Coord(None)) row . is_constant = all ( x == row . Coord ( None ) for x in row ) yy = [ row [ 0 ] if row . is_constant else y for y in row . get_for ( xticks )] # Plot main row . vals = [ getattr ( y , 'val' , None ) for y in yy ] row . handles = {} row . handles [ \"main_panel\" ] = panelcol [ 0 ] . plot ( xticks , row . vals , ** style )[ 0 ] # Plot tuning params row . tuned_coords = {} # Store ordered, \"transposed\" argmins argmins = [ getattr ( y , 'tuned_coord' , None ) for y in yy ] for a , panel in zip ( axes [ \"optim\" ], panelcol [ 1 :]): yy = [ getattr ( coord , a , None ) for coord in argmins ] row . tuned_coords [ a ] = yy # Plotting all None's sets axes units (like any plotting call) # which can cause trouble if the axes units were actually supposed # to be categorical (eg upd_a), but this is only revealed later. if not all ( y == None for y in yy ): row . handles [ a ] = panel . plot ( xticks , yy , ** style ) # Nest axes through table_tree() assert len ( axes [ \"inner\" ]) == 1 , \"You must chose the abscissa.\" axes , tables = self . table_tree ( statkey , axes ) xticks = self . tickz ( axes [ \"inner\" ][ 0 ]) # Figure panels if panels is None : nrows = len ( axes [ 'optim' ] or ()) + 1 ncols = len ( tables ) maxW = 12.7 # my mac screen figsize = figsize or ( min ( 5 * ncols , maxW ), 7 ) gs = dict ( height_ratios = [ 6 ] + [ 1 ] * ( nrows - 1 ), hspace = 0.05 , wspace = 0.05 , # eyeballed: left = 0.15 / ( 1 + np . log ( ncols )), right = 0.97 , bottom = 0.06 , top = 0.9 ) # Create _ , panels = freshfig ( num = fignum , figsize = figsize , nrows = nrows , sharex = True , ncols = ncols , sharey = 'row' , gridspec_kw = gs ) panels = np . ravel ( panels ) . reshape (( - 1 , ncols )) else : panels = np . atleast_2d ( panels ) # Title fig = panels [ 0 , 0 ] . figure fig_title = \"Average wrt. time\" if axes [ \"mean\" ] is not None : fig_title += f \" and {axes['mean']}\" if title2 is not None : fig_title += \" \\n \" + str ( title2 ) fig . suptitle ( fig_title ) # Loop outer label_register = set () # mv inside loop to get legend on each panel for table_panels , ( table_coord , table ) in zip ( panels . T , tables . items ()): table . panels = table_panels title = '' if axes [ \"outer\" ] is None else repr ( table_coord ) # Plot for coord , row in table . items (): style = get_style ( coord ) # Rm duplicate labels (contrary to coords, labels can # be \"tampered\" with, and so can be duplicate) if unique_labels : if style . get ( \"label\" , None ) in label_register : del style [ \"label\" ] else : label_register . add ( style [ \"label\" ]) plot1 ( table . panels , row , style ) # Beautify panel0 = table . panels [ 0 ] panel0 . set_title ( title ) if panel0 . is_first_col (): panel0 . set_ylabel ( statkey ) with utils . set_tmp ( mpl_logger , 'level' , 99 ): # silence \"no label\" msg panel0 . legend () table . panels [ - 1 ] . set_xlabel ( axes [ \"inner\" ][ 0 ]) # Tuning panels: for a , panel in zip ( axes [ \"optim\" ] or (), table . panels [ 1 :]): if panel . is_first_col (): panel . set_ylabel ( f \"Optim. \\n {a}\" ) tables . fig = fig tables . xp_dict = self tables . axes_roles = axes return tables pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. print def print ( self , statkey = 'rmse.a' , axes = { 'outer' : None , 'inner' : None , 'mean' : None , 'optim' : None }, subcols = True , decimals = None ) Print tables of results. statkey: The statistical field from the experiments to report. subcols: If True, then subcolumns are added to indicate the 1\u03c3 confidence interval, and potentially some other stuff. axes: Allots (maps) each role to a set of axis of the xp_dict. Suggestion: dict( outer='da_method', inner='N', mean='seed', optim=('infl','loc_rad')) Example: If mean is assigned to: (\"seed\",): Experiments are averaged accross seeds, and the 1\u03c3 (sub)col is computed as sqrt(var(xps)/N), where xps is a set of experiments. () : Experiments are averaged across nothing (i.e. this is an edge case). None : Experiments are not averaged (i.e. the values are the same as above), and the 1\u03c3 (sub)col is computed from the time series of that single experiment. View Source def print ( self , statkey = \"rmse.a\" , axes = AXES_ROLES , subcols = True , decimals = None ) : \"\"\"Print tables of results. - statkey : The statistical field from the experiments to report . - subcols : If True , then subcolumns are added to indicate the 1 \u03c3 confidence interval , and potentially some other stuff . - axes : Allots ( maps ) each role to a set of axis of the xp_dict . Suggestion : >>> dict ( >>> outer = ' da_method ' , inner = 'N' , mean = ' seed ' , >>> optim = ( ' infl ',' loc_rad ' )) Example : If `` mean `` is assigned to : - ( \"seed\" ,) : Experiments are averaged accross seeds , and the 1 \u03c3 ( sub ) col is computed as sqrt ( var ( xps ) / N ), where xps is a set of experiments . - () : Experiments are averaged across nothing ( i . e . this is an edge case ). - None : Experiments are not averaged ( i . e . the values are the same as above ), and the 1 \u03c3 ( sub ) col is computed from the time series of that single experiment . \"\"\" import pandas as pd def align_subcols ( rows , cc , subcols , h2 ) : \"\"\"Subcolumns: align, justify, join.\"\"\" # Define subcol formats subc = dict () subc [ ' keys ' ] = [ \"val\" , \"conf\" ] subc [ ' headers ' ] = [ statkey , ' 1 \u03c3' ] subc [ ' frmts ' ] = [ None , None ] subc [ ' spaces ' ] = [ ' \u00b1' , ] # last one gets appended below . subc [ ' aligns ' ] = [ '>' , '<' ] # 4 header -- matter gets decimal - aligned . if axes [ ' optim ' ] is not None : subc [ ' keys ' ] += [ \"tuned_coord\" ] subc [ ' headers ' ] += [ axes [ ' optim ' ]] subc [ ' frmts ' ] += [ lambda x : tuple ( a for a in x )] subc [ ' spaces ' ] += [ ' * ' ] subc [ ' aligns ' ] += [ '<' ] elif axes [ ' mean ' ] is not None : subc [ ' keys ' ] += [ \"nFail\" , \"nSuccess\" ] subc [ ' headers ' ] += [ '\u2620' , '\u2713' ] # use width -1 symbols ! subc [ ' frmts ' ] += [ None , None ] subc [ ' spaces ' ] += [ ' ' , ' ' ] subc [ ' aligns ' ] += [ '>' , '>' ] subc [ ' spaces ' ]. append ( '' ) # no space after last subcol template = ' {} ' + ' {} ' . join ( subc [ ' spaces ' ]) # Transpose columns = [ list ( x ) for x in zip ( * rows )] # Iterate over columns. for j , ( col_coord , column ) in enumerate ( zip ( cc , columns )) : # Tabulate columns if subcols : column = unpack_uqs ( column , decimals , subc [ \"keys\" ]) # Tabulate subcolumns subheaders = [] for key , header , frmt , _ , align in zip ( * subc . values ()) : column [ key ] = tabulate_column ( column [ key ], header , frmt = frmt )[ 1 : ] L = len ( column [ -1 ][ key ]) if align == '<' : subheaders += [ str ( header ). ljust ( L )] else : subheaders += [ str ( header ). rjust ( L )] # Join subcolumns: matter = [ template . format ( * [ row [ k ] for k in subc [ ' keys ' ]]) for row in column ] header = template . format ( * subheaders ) else : column = unpack_uqs ( column , decimals )[ \"val\" ] column = tabulate_column ( column , statkey ) header , matter = column [ 0 ], column [ 1 : ] if h2 : # Do super_header if j : super_header = str ( col_coord ) else : super_header = repr ( col_coord ) width = len ( header ) # += 1 if using unicode chars like \u2714\ufe0f super_header = super_header . center ( width , \"_\" ) header = super_header + \" \\n \" + header columns [ j ] = [ header ] + matter # Un-transpose rows = [ list ( x ) for x in zip ( * columns )] return rows # Inform axes[\"mean\"] if axes . get ( ' mean ' , None ) : print ( f \"Averages (in time and) over {axes['mean']}.\" ) else : print ( \"Averages in time only\" \" (=> the 1\u03c3 estimates may be unreliable).\" ) axes , tables = self . table_tree ( statkey , axes ) for table_coord , table in tables . items () : # Get this table's column coords (cc). Use dict for sorted&unique. # cc = xp_dict.ticks[axes[\"inner\"]] # May be larger than needed. # cc = table[0].keys() # May be too small a set. cc = { c : None for row in table . values () for c in row } # Convert table (rows) into rows (lists) of equal length rows = [[ row . get ( c , None ) for c in cc ] for row in table . values ()] if False : # ****************** Simple ( for debugging ) table for i , ( row_coord , row ) in enumerate ( zip ( table , rows )) : row_key = \", \" . join ( str ( v ) for v in row_coord ) rows [ i ] = [ row_key ] + row rows . insert ( 0 , [ f \"{table.axes}\" ] + [ repr ( c ) for c in cc ]) else : # ********************** Elegant table . h2 = \" \\n \" if len ( cc ) > 1 else \"\" # do column - super - header rows = align_subcols ( rows , cc , subcols , h2 ) # Make and prepend left-side table # - It's prettier if row_keys don't have unnecessary cols. # For example, the table of Climatology should not have an # entire column repeatedly displaying \"infl=None\". # => split_attrs(). # - Why didn't we do this for the column attrs? # Coz there we have no ambition to split the attrs, # which would also require excessive processing: # nesting the table as cols, and then split_attrs() on cols. row_keys = xpList ( table . keys ()). split_attrs ()[ 0 ] row_keys = pd . DataFrame . from_dict ( row_keys , dtype = \"O\" ) # allow storing None if len ( row_keys . columns ) : # Header rows [ 0 ] = [ h2 + k for k in row_keys ] + [ h2 + '\u244a' ] + rows [ 0 ] # Matter for row , ( i , key ) in zip ( rows [ 1 : ], row_keys . iterrows ()) : rows [ i + 1 ] = [ * key ] + [ '|' ] + row # Print print ( \" \\n \" , end = \"\" ) if axes [ ' outer ' ] : table_title = \"Table for \" + repr ( table_coord ) print ( color_text ( table_title , colorama . Back . YELLOW )) headers , * rows = rows print ( utils . tab ( rows , headers ). replace ( '\u2423' , ' ' )) setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. table_tree def table_tree ( self , statkey , axes ) Hierarchical nest(): xp_dict>outer>inner>mean>optim. as specified by axes . Returns this new xpSpace. print_1d / plot_1d (respectively) separate tables / panel(row)s for axes['outer'] , and columns/ x-axis for axes['inner'] . The axes['mean'] and axes['optim'] get eliminated by the mean()/tune() operations. Note: cannot support multiple statkeys because it's not (obviously) meaningful when optimizing over tuning_axes. View Source def table_tree ( self , statkey , axes ): \"\"\"Hierarchical nest(): xp_dict>outer>inner>mean>optim. as specified by ``axes``. Returns this new xpSpace. - print_1d / plot_1d (respectively) separate tables / panel(row)s for ``axes['outer']``, and columns/ x-axis for ``axes['inner']``. - The ``axes['mean']`` and ``axes['optim']`` get eliminated by the mean()/tune() operations. Note: cannot support multiple statkeys because it's not (obviously) meaningful when optimizing over tuning_axes. \"\"\" axes = self . validate_axes ( axes ) def mean_tune ( xp_dict ): \"\"\"Take mean, then tune. Note: the SparseDict implementation should be sufficiently \"uncluttered\" that mean_tune() (or a few of its code lines) could be called anywhere above/between/below the ``nest()``ing of ``outer`` or ``inner``. These possibile call locations are commented in the code.\"\"\" uq_dict = xp_dict . field ( statkey ) uq_dict = uq_dict . mean ( axes [ 'mean' ]) uq_dict = uq_dict . tune ( axes [ 'optim' ]) return uq_dict self = mean_tune ( self ) # Prefer calling mean_tune() [also see its docstring] # before doing outer/inner nesting. This is because then the axes of # a row (xpSpace) should not include mean&optim, and thus: # - Column header/coords may be had directly as row.keys(), # without extraction by __getkey__() from (e.g.) row[0]. # - Don't need to propagate mean&optim axes down to the row level. # which would require defining rows by the nesting: # rows = table.nest(outer_axes=dict_tools.complement(table.axes, # *(axes['inner'] or ()), # *(axes['mean'] or ()), # *(axes['optim'] or ()) )) # - Each level of the output from table_tree # is a smaller (and more manageable) dict. tables = self . nest ( outer_axes = axes [ 'outer' ]) for table_coord , table in tables . items (): # table = mean_tune(table) # Should not be used (nesting as rows is more natural, # and is required for getting distinct/row_keys). # cols = table.nest(outer_axes=axes['inner']) rows = table . nest ( inner_axes = axes [ 'inner' ] or ()) # Overwrite table by its nesting as rows tables [ table_coord ] = rows # for row_coord, row in rows.items(): # rows[row_coord] = mean_tune(row) return axes , tables tickz def tickz ( self , axis_name ) Axis ticks without None View Source def tickz ( self , axis_name ) : \"\"\"Axis ticks without None\"\"\" return [ x for x in self.ticks[axis_name ] if x is not None ] tune def tune ( self , axes = None , costfun = None ) Get (compile/tabulate) a stat field optimised wrt. tuning params. View Source def tune ( self , axes = None , costfun = None ) : \"\"\"Get (compile/tabulate) a stat field optimised wrt. tuning params.\"\"\" # Define cost - function costfun = ( costfun or 'increasing' ). lower () if 'increas' in costfun : costfun = ( lambda x : + x ) elif 'decreas' in costfun : costfun = ( lambda x : - x ) else : assert hasattr ( costfun , '__call__' ) # custom # Note : The case `` axes = () `` should work w / o special treatment . if axes is None : return self nested = self . nest ( axes ) for coord , space in nested . items () : # Find optimal value and coord within space MIN = np . inf for i , ( inner_coord , uq ) in enumerate ( space . items ()) : cost = costfun ( uq . val ) if cost <= MIN : MIN = cost uq_opt = uq uq_opt . tuned_coord = inner_coord nested [ coord ] = uq_opt return nested update def update ( self , * args , ** kwargs ) Update using custom setitem (). View Source def update ( self , * args , ** kwargs ) : \"\"\"Update using custom __setitem__().\"\"\" # See https : // stackoverflow . com / a / 2588648 # and https : // stackoverflow . com / a / 2390997 for k , v in dict ( * args , ** kwargs ). items () : self [ k ] = v validate_axes def validate_axes ( self , axes ) Validate axes. Note: This does not convert None to (), allowing None to remain special. Use axis or () wherever tuples are required. View Source def validate_axes ( self , axes ) : \"\"\"Validate axes. Note: This does not convert None to (), allowing None to remain special. Use ``axis or ()`` wherever tuples are required. \"\"\" roles = {} # \"inv\" for role in set ( axes ) | set ( AXES_ROLES ) : assert role in AXES_ROLES , f \"Invalid role {role!r}\" aa = axes . get ( role , AXES_ROLES [ role ] ) if aa is None : pass # Purposely special else : # Ensure iterable if isinstance ( aa , str ) or not hasattr ( aa , \"__iter__\" ) : aa = ( aa ,) aa = self . intersect_axes ( aa ) for axis in aa : # Ensure unique if axis in roles : raise TypeError ( f \"An axis (here {axis!r}) cannot be assigned to 2\" f \" roles (here {role!r} and {roles[axis]!r}).\" ) else : roles [ axis ] = role axes [ role ] = aa return axes values def values ( ... ) D.values() -> an object providing a view on D's values","title":"Data Management"},{"location":"reference/dapper/data_management/#module-dapperdata_management","text":"Define xpSpace (subclasses SparseSpace (subclasses dict)), which is handles the presentation of experiment (xp) results. View Source \"\"\"Define xpSpace (subclasses SparseSpace (subclasses dict)), which is handles the **presentation** of experiment (xp) results.\"\"\" from pathlib import Path import os import copy import warnings import collections import logging import shutil import numpy as np import matplotlib as mpl from matplotlib import cm , ticker import colorama import dill from dapper.tools.colors import color_text from dapper.tools.viz import axis_scale_by_array , freshfig from dapper.tools.series import UncertainQtty from dapper.stats import tabulate_column , unpack_uqs from dapper.admin import xpList import dapper.dict_tools as dict_tools import dapper.tools.remote.uplink as uplink import dapper.tools.utils as utils mpl_logger = logging . getLogger ( 'matplotlib' ) NO_KEY = ( \"da_method\" , \"Const\" , \"upd_a\" ) def make_label ( coord , no_key = NO_KEY , exclude = ()): dct = { a : v for a , v in coord . _asdict () . items () if v != None } lbl = '' for k , v in dct . items (): if k not in exclude : if any ( x in k for x in no_key ): lbl = lbl + f ' { v } ' else : lbl = lbl + f ' { utils . collapse_str ( k , 7 ) } : { v } ' return lbl [ 1 :] def default_styles ( coord , baseline_legends = False ): \"\"\"Quick and dirty (but somewhat robust) styling.\"\"\" style = dict_tools . DotDict ( ms = 8 ) style . label = make_label ( coord ) try : if coord . da_method == \"Climatology\" : style . ls = \":\" style . c = \"k\" if not baseline_legends : style . label = None elif coord . da_method == \"OptInterp\" : style . ls = \":\" style . c = . 7 * np . ones ( 3 ) style . label = \"Opt. Interp.\" if not baseline_legends : style . label = None elif coord . da_method == \"Var3D\" : style . ls = \":\" style . c = . 5 * np . ones ( 3 ) style . label = \"3D-Var\" if not baseline_legends : style . label = None elif coord . da_method == \"EnKF\" : style . marker = \"*\" style . c = \"C1\" elif coord . da_method == \"PartFilt\" : style . marker = \"X\" style . c = \"C2\" else : style . marker = \".\" except AttributeError : pass return style def rel_index ( elem , lst , default = None ): \"\"\"``lst.index(elem) / len(lst)`` with fallback.\"\"\" try : return lst . index ( elem ) / len ( lst ) except ValueError : if default == None : raise return default def discretize_cmap ( cmap , N , val0 = 0 , val1 = 1 , name = None ): \"\"\"Discretize cmap so that it partitions [0,1] into N segments. I.e. cmap(k/N) == cmap(k/N + eps). Also provide the ScalarMappable ``sm`` that maps range(N) to the segment centers, as will be reflected by ``cb = fig.colorbar(sm)``. You can then re-label the ticks using ``cb.set_ticks(np.arange(N)); cb.set_ticklabels([\"A\",\"B\",\"C\",...])``.\"\"\" # cmap(k/N) from_list = mpl . colors . LinearSegmentedColormap . from_list colors = cmap ( np . linspace ( val0 , val1 , N )) cmap = from_list ( name , colors , N ) # sm cNorm = mpl . colors . Normalize ( -. 5 , -. 5 + N ) sm = mpl . cm . ScalarMappable ( cNorm , cmap ) return cmap , sm def cm_bond ( cmap , xp_dict , axis , vmin = 0 , vmax = 0 ): \"\"\"Map cmap for coord.axis \u2208 [0, len(ticks)].\"\"\" def link ( coord ): \"\"\"Essentially: cmap(ticks.index(coord.axis))\"\"\" if hasattr ( coord , axis ): ticks = xp_dict . ticks [ axis ] cNorm = mpl . colors . Normalize ( vmin , vmax + len ( ticks )) ScMap = cm . ScalarMappable ( cNorm , cmap ) . to_rgba index = ticks . index ( getattr ( coord , axis )) return ScMap ( index ) else : return cmap ( 0.5 ) return link def in_idx ( coord , indices , xp_dict , axis ): \"\"\"Essentially: coord.axis in ticks[indices]\"\"\" if hasattr ( coord , axis ): ticks = np . array ( xp_dict . ticks [ axis ])[ indices ] return getattr ( coord , axis ) in ticks else : return True def load_HMM ( save_as ): save_as = Path ( save_as ) . expanduser () HMM = dill . load ( open ( save_as / \"xp.com\" , \"rb\" ))[ \"HMM\" ] return HMM def load_xps ( save_as ): \"\"\"Load ``xps`` (as a simple list) from dir.\"\"\" save_as = Path ( save_as ) . expanduser () files = [ d / \"xp\" for d in uplink . list_job_dirs ( save_as )] def load_any ( filepath ): \"\"\"Load any/all ``xp's`` from ``filepath``.\"\"\" with open ( filepath , \"rb\" ) as F : # If experiment crashed, then xp will be empty try : data = dill . load ( F ) except EOFError : return [] # Always return list try : return data [ \"xps\" ] except KeyError : return [ data [ \"xp\" ]] print ( \"Loading %d files from %s \" % ( len ( files ), save_as )) xps = [] # NB: progbar wont clean up properly w/ list compr. for f in utils . progbar ( files , desc = \"Loading\" ): xps . extend ( load_any ( f )) if len ( xps ) < len ( files ): print ( f \" { len ( files ) - len ( xps ) } files could not be loaded.\" ) return xps def save_xps ( xps , save_as , nDir = 100 ): \"\"\"Split xps and save in save_as/i for i in range(nDir). Example: rename attr n_iter to nIter: >>> proj_name = \"Stein\" >>> dd = dpr.rc.dirs.data / proj_name >>> save_as = dd / \"run_2020-09-22__19:36:13\" >>> >>> for save_as in os.listdir(dd): >>> save_as = dd / save_as >>> >>> xps = load_xps(save_as) >>> HMM = load_HMM(save_as) >>> >>> for xp in xps: >>> if hasattr(xp,\"n_iter\"): >>> xp.nIter = xp.n_iter >>> del xp.n_iter >>> >>> overwrite_xps(xps, save_as) \"\"\" save_as = Path ( save_as ) . expanduser () save_as . mkdir ( parents = False , exist_ok = False ) splitting = np . array_split ( xps , nDir ) for i , sub_xps in enumerate ( utils . tqdm . tqdm ( splitting , desc = \"Saving\" )): if len ( sub_xps ): iDir = save_as / str ( i ) os . mkdir ( iDir ) with open ( iDir / \"xp\" , \"wb\" ) as F : dill . dump ({ 'xps' : sub_xps }, F ) def overwrite_xps ( xps , save_as , nDir = 100 ): \"\"\"Save xps in save_as, but safely (by first saving to tmp).\"\"\" save_xps ( xps , save_as / \"tmp\" , nDir ) # Delete for d in utils . tqdm . tqdm ( uplink . list_job_dirs ( save_as ), desc = \"Deleting old\" ): shutil . rmtree ( d ) # Mv up from tmp/ -- goes quick, coz there are not many. for d in os . listdir ( save_as / \"tmp\" ): shutil . move ( save_as / \"tmp\" / d , save_as / d ) shutil . rmtree ( save_as / \"tmp\" ) def reduce_inodes ( save_as , nDir = 100 ): \"\"\"Reduce the number of ``xp`` dirs by packing multiple ``xp``s into lists (``xps``). This reduces the **number** of files (inodes) on the system, which limits storage capacity (along with **size**). It also deletes files \"xp.var\" and \"out\" (which tends to be relatively large coz of the progbar). This is probably also the reason that the loading time is sometimes reduced.\"\"\" overwrite_xps ( load_xps ( save_as ), save_as , nDir ) class SparseSpace ( dict ): \"\"\"Dict, subclassed enforce key conformity (to a coord. sys, i.e. a space). The coordinate system is specified by its \"axes\", which is used to produce self.Coord (a namedtuple class). As a normal dict, it can hold any type of objects. In normal use, this space is highly sparse, coz there are many coordinates with no matching experiment, eg. coord(da_method=Climatology, rot=True, ...). Indeed, operations across (potentially multiple simultaneous) axes, such as optimization or averaging, should be carried out by iterating -- not over the axis -- but over the the list of items. The most important method is ``nest()``, which is used (by xpSpace.table_tree) to separate tables/columns, and also to carry out the mean/optim operations. In addition, __getitem__() is very flexible, allowing accessing by: - The actual key, a self.Coord object. Returns single item. - A dict match against (part of) the coordinates. Returns subspace. - An int. Returns list(self)[key]. - A list of any of the above. Returns list. This flexibility can cause bugs, but it's probably still worth it). Also see __call__(), get_for(), and coords(), for further convenience. Inspired by https://stackoverflow.com/a/7728830 Also see https://stackoverflow.com/q/3387691 \"\"\" @property def axes ( self ): return self . Coord . _fields def __init__ ( self , axes , * args , ** kwargs ): # Define coordinate system self . Coord = collections . namedtuple ( 'Coord' , axes ) # Write dict self . update ( * args , ** kwargs ) # Add repr/str self . Coord . __repr__ = lambda c : \",\" . join ( f \" { k } = { v !r} \" for k , v in zip ( c . _fields , c )) self . Coord . __str__ = lambda c : \",\" . join ( str ( v ) for v in c ) def update ( self , * args , ** kwargs ): \"\"\"Update using custom __setitem__().\"\"\" # See https://stackoverflow.com/a/2588648 # and https://stackoverflow.com/a/2390997 for k , v in dict ( * args , ** kwargs ) . items (): self [ k ] = v def __setitem__ ( self , key , val ): \"\"\"Setitem ensuring coordinate conforms.\"\"\" try : key = self . Coord ( * key ) except TypeError : raise TypeError ( f \"The key { key !r} did not fit the coord. system \" f \"which has axes { self . axes } \" ) super () . __setitem__ ( key , val ) def __getitem__ ( self , key ): \"\"\"Flexible indexing.\"\"\" # List of items (by a list of indices). # Also see get_for(). if isinstance ( key , list ): return [ self [ k ] for k in key ] # Single (by integer) or list (by Slice) # Note: NOT validating np.int64 here catches quite a few bugs. elif isinstance ( key , int ) or isinstance ( key , slice ): return [ * self . values ()][ key ] # Subspace (by dict, ie. an informal, partial coordinate) elif isinstance ( key , dict ): outer = self . nest ( outer_axes = list ( key )) # nest coord = outer . Coord ( * key . values ()) # create coord inner = outer [ coord ] # chose subspace return inner # Single item (by Coord object, coz an integer (eg) # gets interpreted (above) as a list index) else : # NB: Dont't use isinstance(key, self.Coord) # coz it fails when the namedtuple (Coord) has been # instantiated in different places (but with equal params). # Also see bugs.python.org/issue7796 return super () . __getitem__ ( key ) def __getkey__ ( self , entry ): \"\"\"Inverse of dict.__getitem__(), but also works on coords. Note: This dunder method is not a \"builtin\" naming convention.\"\"\" coord = ( getattr ( entry , a , None ) for a in self . axes ) return self . Coord ( * coord ) def __call__ ( self , ** kwargs ): \"\"\"Convenience, that enables, eg.: >>> xp_dict(da_method=\"EnKF\", infl=1, seed=3) \"\"\" return self . __getitem__ ( kwargs ) def get_for ( self , ticks , default = None ): \"\"\"Almost ``[self.get(Coord(x)) for x in ticks]``. NB: using the \"naive\" thing: ``[self[x] for x in ticks]`` would probably be a BUG coz x gets interpreted as indices for the internal list.\"\"\" singleton = not hasattr ( ticks [ 0 ], \"__iter__\" ) def coord ( xyz ): return self . Coord ( xyz if singleton else xyz ) return [ self . get ( coord ( x ), default ) for x in ticks ] def coords ( self , ** kwargs ): \"\"\"Get all ``coord``s matching kwargs. Unlike ``__getitem__(kwargs)``, - A list is returned, not a subspace. - This list constains keys (coords), not values. - The coords refer to the original space, not the subspace. The last point is especially useful for label_xSection(). \"\"\" def embed ( coord ): return { ** kwargs , ** coord . _asdict ()} return [ self . Coord ( ** embed ( x )) for x in self [ kwargs ]] # Old implementation. # - I prefer the new version for its re-use of __getitem__'s # nesting, evidencing their mutual relationship) # - Note that unlike xpList.inds(): missingval shenanigans # are here unnecessary coz each coordinate is complete. # match = lambda x: all(getattr(x,k)==kwargs[k] for k in kwargs) # return [x for x in self if match(x)] def __repr__ ( self ): # Note: print(xpList(self)) produces more human-readable key listing, # but we don't want to implement it here, coz it requires split_attrs(), # which we don't really want to call again. L = 2 keys = [ str ( k ) for k in self ] if 2 * L < len ( keys ): keys = keys [: L ] + [ \"...\" ] + keys [ - L :] keys = \"[ \\n \" + \", \\n \" . join ( keys ) + \" \\n ]\" txt = f \"< { self . __class__ . __name__ } > with { len ( self ) } keys: { keys } \" # txt += \" befitting the coord. sys. with axes \" txt += \" \\n placed in a coord-sys with axes \" try : txt += \"(and ticks):\" + str ( dict_tools . AlignedDict ( self . ticks )) except AttributeError : txt += \": \\n \" + str ( self . axes ) return txt def nest ( self , inner_axes = None , outer_axes = None ): \"\"\"Return a new xpSpace with axes ``outer_axes``, obtained by projecting along the ``inner_axes``. The entries of this ``xpSpace`` are themselves ``xpSpace``s, with axes ``inner_axes``, each one regrouping the entries with the same (projected) coordinate. Note: is also called by ``__getitem__(key)`` if ``key`` is dict.\"\"\" # Default: a singleton outer space, # with everything contained in the inner (projection) space. if inner_axes is None and outer_axes is None : outer_axes = () # Validate axes if inner_axes is None : assert outer_axes is not None inner_axes = dict_tools . complement ( self . axes , outer_axes ) else : assert outer_axes is None outer_axes = dict_tools . complement ( self . axes , inner_axes ) # Fill spaces outer_space = self . __class__ ( outer_axes ) for coord , entry in self . items (): outer_coord = outer_space . __getkey__ ( coord ) try : inner_space = outer_space [ outer_coord ] except KeyError : inner_space = self . __class__ ( inner_axes ) outer_space [ outer_coord ] = inner_space inner_space [ inner_space . __getkey__ ( coord )] = entry return outer_space def add_axis ( self , axis ): self . __init__ ( self . axes + ( axis ,)) for coord in list ( self ): entry = self . pop ( coord ) self [ coord + ( None ,)] = entry def intersect_axes ( self , attrs ): \"\"\"Rm those a in attrs that are not in self.axes. This allows errors in the axes allotment, for ease-of-use.\"\"\" absent = dict_tools . complement ( attrs , self . axes ) if absent : print ( color_text ( \"Warning:\" , colorama . Fore . RED ), \"The requested attributes\" , color_text ( str ( absent ), colorama . Fore . RED ), ( \"were not found among the\" \" xpSpace axes (attrs. used as coordinates\" \" for the set of experiments).\" \" This may be no problem if the attr. is redundant\" \" for the coord-sys.\" \" However, if it is caused by confusion or mis-spelling,\" \" then it is likely to cause mis-interpretation\" \" of the shown results.\" )) attrs = dict_tools . complement ( attrs , absent ) return attrs def label_xSection ( self , label , * NoneAttrs , ** sub_coord ): \"\"\"Insert duplicate entries for the cross section whose ``coord``s match ``sub_coord``, adding the attr ``Const=label`` to their ``coord``, reflecting the \"constance/constraint/fixation\" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up in ``nest()``. If you wish, you can specify the ``NoneAttrs``, which are consequently set to None for the duplicated entries, preventing them from getting plotted in tuning panels. \"\"\" if \"Const\" not in self . axes : self . add_axis ( 'Const' ) for coord in self . coords ( ** self . intersect_axes ( sub_coord )): entry = copy . deepcopy ( self [ coord ]) coord = coord . _replace ( Const = label ) coord = coord . _replace ( ** { a : None for a in NoneAttrs }) self [ coord ] = entry AXES_ROLES = dict ( outer = None , inner = None , mean = None , optim = None ) class xpSpace ( SparseSpace ): \"\"\"Functionality to facilitate working with ``xps`` and their results. The function ``from_list()`` initializes a ``SparseSpace`` from a list of objects, typically experiments referred to as ``xps``, by (1) computing the relevant axes from the attributes, and (2) filling the dict by ``xps``. The main use of xpSpace is through its ``print()`` & ``plot()``, both of which call ``table_tree()`` to nest the axes of the SparseSpace. For custom plotting, you will likely want to start with ``table_tree()``. Using ``from_list(xps)`` creates a SparseSpace holding ``xps``. However, the nested ``xpSpace``s output by ``table_tree()``, will hold objects of type ``UncertainQtty``, coz ``table_tree()`` calls ``mean()`` calls ``field(statkey)``.\"\"\" @classmethod def from_list ( cls , xps ): \"\"\"Init xpSpace from xpList.\"\"\" def make_ticks ( axes , ordering = dict ( N = 'default' , seed = 'default' , infl = 'default' , loc_rad = 'default' , rot = 'as_found' , da_method = 'as_found' , )): \"\"\"Unique & sort, for each axis (individually) in axes.\"\"\" for ax_name , arr in axes . items (): ticks = set ( arr ) # unique (jumbles order) # Sort order = ordering . get ( ax_name , 'default' ) . lower () if hasattr ( order , '__call__' ): # eg. mylist.index ticks = sorted ( ticks , key = order ) elif 'as_found' in order : ticks = sorted ( ticks , key = arr . index ) else : # default sorting, with None placed at the end ticks = sorted ( ticks , key = lambda x : ( x is None , x )) if any ( x in order for x in [ 'rev' , 'inv' ]): ticks = ticks [:: - 1 ] axes [ ax_name ] = ticks # Define axes xp_list = xpList ( xps ) axes = xp_list . split_attrs ( nomerge = [ 'Const' ])[ 0 ] make_ticks ( axes ) self = cls ( axes . keys ()) # Note: this attr (ticks) will not be propagated through nest(). # That is fine. Otherwise we should have to prune the ticks # (if they are to be useful), which we don't want to do. self . ticks = axes # Fill self . update ({ self . __getkey__ ( xp ): xp for xp in xps }) return self def field ( self , statkey = \"rmse.a\" ): \"\"\"Extract ``statkey`` for each item in ``self``.\"\"\" # Init a new xpDict to hold field avrgs = self . __class__ ( self . axes ) found_anything = False for coord , xp in self . items (): val = getattr ( xp . avrgs , statkey , None ) avrgs [ coord ] = val found_anything = found_anything or ( val is not None ) if not found_anything : raise AttributeError ( f \"The stat. field ' { statkey } ' was not found\" \" among any of the xp's.\" ) return avrgs def mean ( self , axes = None ): # Note: The case ``axes=()`` should work w/o special treatment. if axes is None : return self nested = self . nest ( axes ) for coord , space in nested . items (): def getval ( uq ): return uq . val if isinstance ( uq , UncertainQtty ) else uq vals = [ getval ( uq ) for uq in space . values ()] # Don't use nanmean! It would give false impressions. mu = np . mean ( vals ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) # Don't print warnings caused by N=1. # It already correctly yield nan's. var = np . var ( vals , ddof = 1 ) N = len ( vals ) uq = UncertainQtty ( mu , np . sqrt ( var / N )) uq . nTotal = N uq . nFail = N - np . isfinite ( vals ) . sum () uq . nSuccess = N - uq . nFail nested [ coord ] = uq return nested def tune ( self , axes = None , costfun = None ): \"\"\"Get (compile/tabulate) a stat field optimised wrt. tuning params.\"\"\" # Define cost-function costfun = ( costfun or 'increasing' ) . lower () if 'increas' in costfun : costfun = ( lambda x : + x ) elif 'decreas' in costfun : costfun = ( lambda x : - x ) else : assert hasattr ( costfun , '__call__' ) # custom # Note: The case ``axes=()`` should work w/o special treatment. if axes is None : return self nested = self . nest ( axes ) for coord , space in nested . items (): # Find optimal value and coord within space MIN = np . inf for i , ( inner_coord , uq ) in enumerate ( space . items ()): cost = costfun ( uq . val ) if cost <= MIN : MIN = cost uq_opt = uq uq_opt . tuned_coord = inner_coord nested [ coord ] = uq_opt return nested def validate_axes ( self , axes ): \"\"\"Validate axes. Note: This does not convert None to (), allowing None to remain special. Use ``axis or ()`` wherever tuples are required. \"\"\" roles = {} # \"inv\" for role in set ( axes ) | set ( AXES_ROLES ): assert role in AXES_ROLES , f \"Invalid role { role !r} \" aa = axes . get ( role , AXES_ROLES [ role ]) if aa is None : pass # Purposely special else : # Ensure iterable if isinstance ( aa , str ) or not hasattr ( aa , \"__iter__\" ): aa = ( aa ,) aa = self . intersect_axes ( aa ) for axis in aa : # Ensure unique if axis in roles : raise TypeError ( f \"An axis (here { axis !r} ) cannot be assigned to 2\" f \" roles (here { role !r} and { roles [ axis ] !r} ).\" ) else : roles [ axis ] = role axes [ role ] = aa return axes def table_tree ( self , statkey , axes ): \"\"\"Hierarchical nest(): xp_dict>outer>inner>mean>optim. as specified by ``axes``. Returns this new xpSpace. - print_1d / plot_1d (respectively) separate tables / panel(row)s for ``axes['outer']``, and columns/ x-axis for ``axes['inner']``. - The ``axes['mean']`` and ``axes['optim']`` get eliminated by the mean()/tune() operations. Note: cannot support multiple statkeys because it's not (obviously) meaningful when optimizing over tuning_axes. \"\"\" axes = self . validate_axes ( axes ) def mean_tune ( xp_dict ): \"\"\"Take mean, then tune. Note: the SparseDict implementation should be sufficiently \"uncluttered\" that mean_tune() (or a few of its code lines) could be called anywhere above/between/below the ``nest()``ing of ``outer`` or ``inner``. These possibile call locations are commented in the code.\"\"\" uq_dict = xp_dict . field ( statkey ) uq_dict = uq_dict . mean ( axes [ 'mean' ]) uq_dict = uq_dict . tune ( axes [ 'optim' ]) return uq_dict self = mean_tune ( self ) # Prefer calling mean_tune() [also see its docstring] # before doing outer/inner nesting. This is because then the axes of # a row (xpSpace) should not include mean&optim, and thus: # - Column header/coords may be had directly as row.keys(), # without extraction by __getkey__() from (e.g.) row[0]. # - Don't need to propagate mean&optim axes down to the row level. # which would require defining rows by the nesting: # rows = table.nest(outer_axes=dict_tools.complement(table.axes, # *(axes['inner'] or ()), # *(axes['mean'] or ()), # *(axes['optim'] or ()) )) # - Each level of the output from table_tree # is a smaller (and more manageable) dict. tables = self . nest ( outer_axes = axes [ 'outer' ]) for table_coord , table in tables . items (): # table = mean_tune(table) # Should not be used (nesting as rows is more natural, # and is required for getting distinct/row_keys). # cols = table.nest(outer_axes=axes['inner']) rows = table . nest ( inner_axes = axes [ 'inner' ] or ()) # Overwrite table by its nesting as rows tables [ table_coord ] = rows # for row_coord, row in rows.items(): # rows[row_coord] = mean_tune(row) return axes , tables def tickz ( self , axis_name ): \"\"\"Axis ticks without None\"\"\" return [ x for x in self . ticks [ axis_name ] if x is not None ] def print ( self , statkey = \"rmse.a\" , axes = AXES_ROLES , subcols = True , decimals = None ): \"\"\"Print tables of results. - statkey: The statistical field from the experiments to report. - subcols: If True, then subcolumns are added to indicate the 1\u03c3 confidence interval, and potentially some other stuff. - axes: Allots (maps) each role to a set of axis of the xp_dict. Suggestion: >>> dict( >>> outer='da_method', inner='N', mean='seed', >>> optim=('infl','loc_rad')) Example: If ``mean`` is assigned to: - (\"seed\",): Experiments are averaged accross seeds, and the 1\u03c3 (sub)col is computed as sqrt(var(xps)/N), where xps is a set of experiments. - () : Experiments are averaged across nothing (i.e. this is an edge case). - None : Experiments are not averaged (i.e. the values are the same as above), and the 1\u03c3 (sub)col is computed from the time series of that single experiment. \"\"\" import pandas as pd def align_subcols ( rows , cc , subcols , h2 ): \"\"\"Subcolumns: align, justify, join.\"\"\" # Define subcol formats subc = dict () subc [ 'keys' ] = [ \"val\" , \"conf\" ] subc [ 'headers' ] = [ statkey , '1\u03c3' ] subc [ 'frmts' ] = [ None , None ] subc [ 'spaces' ] = [ ' \u00b1' , ] # last one gets appended below. subc [ 'aligns' ] = [ '>' , '<' ] # 4 header -- matter gets decimal-aligned. if axes [ 'optim' ] is not None : subc [ 'keys' ] += [ \"tuned_coord\" ] subc [ 'headers' ] += [ axes [ 'optim' ]] subc [ 'frmts' ] += [ lambda x : tuple ( a for a in x )] subc [ 'spaces' ] += [ ' *' ] subc [ 'aligns' ] += [ '<' ] elif axes [ 'mean' ] is not None : subc [ 'keys' ] += [ \"nFail\" , \"nSuccess\" ] subc [ 'headers' ] += [ '\u2620' , '\u2713' ] # use width-1 symbols! subc [ 'frmts' ] += [ None , None ] subc [ 'spaces' ] += [ ' ' , ' ' ] subc [ 'aligns' ] += [ '>' , '>' ] subc [ 'spaces' ] . append ( '' ) # no space after last subcol template = ' {} ' + ' {} ' . join ( subc [ 'spaces' ]) # Transpose columns = [ list ( x ) for x in zip ( * rows )] # Iterate over columns. for j , ( col_coord , column ) in enumerate ( zip ( cc , columns )): # Tabulate columns if subcols : column = unpack_uqs ( column , decimals , subc [ \"keys\" ]) # Tabulate subcolumns subheaders = [] for key , header , frmt , _ , align in zip ( * subc . values ()): column [ key ] = tabulate_column ( column [ key ], header , frmt = frmt )[ 1 :] L = len ( column [ - 1 ][ key ]) if align == '<' : subheaders += [ str ( header ) . ljust ( L )] else : subheaders += [ str ( header ) . rjust ( L )] # Join subcolumns: matter = [ template . format ( * [ row [ k ] for k in subc [ 'keys' ]]) for row in column ] header = template . format ( * subheaders ) else : column = unpack_uqs ( column , decimals )[ \"val\" ] column = tabulate_column ( column , statkey ) header , matter = column [ 0 ], column [ 1 :] if h2 : # Do super_header if j : super_header = str ( col_coord ) else : super_header = repr ( col_coord ) width = len ( header ) # += 1 if using unicode chars like \u2714\ufe0f super_header = super_header . center ( width , \"_\" ) header = super_header + \" \\n \" + header columns [ j ] = [ header ] + matter # Un-transpose rows = [ list ( x ) for x in zip ( * columns )] return rows # Inform axes[\"mean\"] if axes . get ( 'mean' , None ): print ( f \"Averages (in time and) over { axes [ 'mean' ] } .\" ) else : print ( \"Averages in time only\" \" (=> the 1\u03c3 estimates may be unreliable).\" ) axes , tables = self . table_tree ( statkey , axes ) for table_coord , table in tables . items (): # Get this table's column coords (cc). Use dict for sorted&unique. # cc = xp_dict.ticks[axes[\"inner\"]] # May be larger than needed. # cc = table[0].keys() # May be too small a set. cc = { c : None for row in table . values () for c in row } # Convert table (rows) into rows (lists) of equal length rows = [[ row . get ( c , None ) for c in cc ] for row in table . values ()] if False : # ****************** Simple (for debugging) table for i , ( row_coord , row ) in enumerate ( zip ( table , rows )): row_key = \", \" . join ( str ( v ) for v in row_coord ) rows [ i ] = [ row_key ] + row rows . insert ( 0 , [ f \" { table . axes } \" ] + [ repr ( c ) for c in cc ]) else : # ********************** Elegant table. h2 = \" \\n \" if len ( cc ) > 1 else \"\" # do column-super-header rows = align_subcols ( rows , cc , subcols , h2 ) # Make and prepend left-side table # - It's prettier if row_keys don't have unnecessary cols. # For example, the table of Climatology should not have an # entire column repeatedly displaying \"infl=None\". # => split_attrs(). # - Why didn't we do this for the column attrs? # Coz there we have no ambition to split the attrs, # which would also require excessive processing: # nesting the table as cols, and then split_attrs() on cols. row_keys = xpList ( table . keys ()) . split_attrs ()[ 0 ] row_keys = pd . DataFrame . from_dict ( row_keys , dtype = \"O\" ) # allow storing None if len ( row_keys . columns ): # Header rows [ 0 ] = [ h2 + k for k in row_keys ] + [ h2 + '\u244a' ] + rows [ 0 ] # Matter for row , ( i , key ) in zip ( rows [ 1 :], row_keys . iterrows ()): rows [ i + 1 ] = [ * key ] + [ '|' ] + row # Print print ( \" \\n \" , end = \"\" ) if axes [ 'outer' ]: table_title = \"Table for \" + repr ( table_coord ) print ( color_text ( table_title , colorama . Back . YELLOW )) headers , * rows = rows print ( utils . tab ( rows , headers ) . replace ( '\u2423' , ' ' )) def plot ( self , statkey = \"rmse.a\" , axes = AXES_ROLES , get_style = default_styles , fignum = None , figsize = None , panels = None , title2 = None , costfun = None , unique_labels = True ): \"\"\"Plot the avrgs of ``statkey`` as a function of ``axis[\"inner\"]``. Firs of all, though, mean and optimum computations are done for ``axis[\"mean\"]`` and ``axis[\"optim\"]``. The optimal parameters are plotted in panels below the main plot. This can be prevented by providing the figure axes in ``panels``. Optionally, the experiments can be grouped by ``axis[\"outer\"]``, producing a figure with columns of panels.\"\"\" def plot1 ( panelcol , row , style ): \"\"\"Plot a given line (row) in the main panel and the optim panels. Involves: Sort, insert None's, handle constant lines.\"\"\" # Make a full row (yy) of vals, whether is_constant or not. # row.is_constant = (len(row)==1 and next(iter(row))==row.Coord(None)) row . is_constant = all ( x == row . Coord ( None ) for x in row ) yy = [ row [ 0 ] if row . is_constant else y for y in row . get_for ( xticks )] # Plot main row . vals = [ getattr ( y , 'val' , None ) for y in yy ] row . handles = {} row . handles [ \"main_panel\" ] = panelcol [ 0 ] . plot ( xticks , row . vals , ** style )[ 0 ] # Plot tuning params row . tuned_coords = {} # Store ordered, \"transposed\" argmins argmins = [ getattr ( y , 'tuned_coord' , None ) for y in yy ] for a , panel in zip ( axes [ \"optim\" ], panelcol [ 1 :]): yy = [ getattr ( coord , a , None ) for coord in argmins ] row . tuned_coords [ a ] = yy # Plotting all None's sets axes units (like any plotting call) # which can cause trouble if the axes units were actually supposed # to be categorical (eg upd_a), but this is only revealed later. if not all ( y == None for y in yy ): row . handles [ a ] = panel . plot ( xticks , yy , ** style ) # Nest axes through table_tree() assert len ( axes [ \"inner\" ]) == 1 , \"You must chose the abscissa.\" axes , tables = self . table_tree ( statkey , axes ) xticks = self . tickz ( axes [ \"inner\" ][ 0 ]) # Figure panels if panels is None : nrows = len ( axes [ 'optim' ] or ()) + 1 ncols = len ( tables ) maxW = 12.7 # my mac screen figsize = figsize or ( min ( 5 * ncols , maxW ), 7 ) gs = dict ( height_ratios = [ 6 ] + [ 1 ] * ( nrows - 1 ), hspace = 0.05 , wspace = 0.05 , # eyeballed: left = 0.15 / ( 1 + np . log ( ncols )), right = 0.97 , bottom = 0.06 , top = 0.9 ) # Create _ , panels = freshfig ( num = fignum , figsize = figsize , nrows = nrows , sharex = True , ncols = ncols , sharey = 'row' , gridspec_kw = gs ) panels = np . ravel ( panels ) . reshape (( - 1 , ncols )) else : panels = np . atleast_2d ( panels ) # Title fig = panels [ 0 , 0 ] . figure fig_title = \"Average wrt. time\" if axes [ \"mean\" ] is not None : fig_title += f \" and { axes [ 'mean' ] } \" if title2 is not None : fig_title += \" \\n \" + str ( title2 ) fig . suptitle ( fig_title ) # Loop outer label_register = set () # mv inside loop to get legend on each panel for table_panels , ( table_coord , table ) in zip ( panels . T , tables . items ()): table . panels = table_panels title = '' if axes [ \"outer\" ] is None else repr ( table_coord ) # Plot for coord , row in table . items (): style = get_style ( coord ) # Rm duplicate labels (contrary to coords, labels can # be \"tampered\" with, and so can be duplicate) if unique_labels : if style . get ( \"label\" , None ) in label_register : del style [ \"label\" ] else : label_register . add ( style [ \"label\" ]) plot1 ( table . panels , row , style ) # Beautify panel0 = table . panels [ 0 ] panel0 . set_title ( title ) if panel0 . is_first_col (): panel0 . set_ylabel ( statkey ) with utils . set_tmp ( mpl_logger , 'level' , 99 ): # silence \"no label\" msg panel0 . legend () table . panels [ - 1 ] . set_xlabel ( axes [ \"inner\" ][ 0 ]) # Tuning panels: for a , panel in zip ( axes [ \"optim\" ] or (), table . panels [ 1 :]): if panel . is_first_col (): panel . set_ylabel ( f \"Optim. \\n { a } \" ) tables . fig = fig tables . xp_dict = self tables . axes_roles = axes return tables def default_fig_adjustments ( tables ): \"\"\"Beautify. These settings do not generalize well.\"\"\" # Get axs as 2d-array axs = np . array ([ table . panels for table in tables . values ()]) . T # Main panels (top row) only: sensible_f = ticker . FormatStrFormatter ( ' %g ' ) for ax in axs [ 0 , :]: for direction , nPanel in zip ([ 'y' , 'x' ], axs . shape ): if nPanel < 6 : eval ( f \"ax.set_ { direction } scale('log')\" ) eval ( f \"ax. { direction } axis\" ) . set_minor_formatter ( sensible_f ) eval ( f \"ax. { direction } axis\" ) . set_major_formatter ( sensible_f ) # Tuning panels only table = tables [ 0 ] for a , panel in zip ( tables . axes_roles [ \"optim\" ] or (), table . panels [ 1 :]): yy = tables . xp_dict . tickz ( a ) axis_scale_by_array ( panel , yy , \"y\" ) # set_ymargin doesn't work for wonky scales. Do so manually: alpha = len ( yy ) / 10 y0 , y1 , y2 , y3 = yy [ 0 ], yy [ 1 ], yy [ - 2 ], yy [ - 1 ] panel . set_ylim ( y0 - alpha * ( y1 - y0 ), y3 + alpha * ( y3 - y2 )) # All panels for ax in axs . ravel (): for direction , nPanel in zip ([ 'y' , 'x' ], axs . shape ): if nPanel < 6 : ax . grid ( True , which = \"minor\" , axis = direction ) # Not strictly compatible with gridspec height_ratios, # (throws warning), but still works ok. with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = UserWarning ) axs [ 0 , 0 ] . figure . tight_layout ()","title":"Module dapper.data_management"},{"location":"reference/dapper/data_management/#variables","text":"AXES_ROLES NO_KEY mpl_logger","title":"Variables"},{"location":"reference/dapper/data_management/#functions","text":"","title":"Functions"},{"location":"reference/dapper/data_management/#cm_bond","text":"def cm_bond ( cmap , xp_dict , axis , vmin = 0 , vmax = 0 ) Map cmap for coord.axis \u2208 [0, len(ticks)]. View Source def cm_bond ( cmap , xp_dict , axis , vmin = 0 , vmax = 0 ) : \"\"\"Map cmap for coord.axis \u2208 [0, len(ticks)].\"\"\" def link ( coord ) : \"\"\"Essentially: cmap(ticks.index(coord.axis))\"\"\" if hasattr ( coord , axis ) : ticks = xp_dict . ticks [ axis ] cNorm = mpl . colors . Normalize ( vmin , vmax + len ( ticks )) ScMap = cm . ScalarMappable ( cNorm , cmap ). to_rgba index = ticks . index ( getattr ( coord , axis )) return ScMap ( index ) else : return cmap ( 0.5 ) return link","title":"cm_bond"},{"location":"reference/dapper/data_management/#default_fig_adjustments","text":"def default_fig_adjustments ( tables ) Beautify. These settings do not generalize well. View Source def default_fig_adjustments ( tables ): \"\"\"Beautify. These settings do not generalize well.\"\"\" # Get axs as 2 d - array axs = np . array ([ table . panels for table in tables . values ()]). T # Main panels ( top row ) only : sensible_f = ticker . FormatStrFormatter ( '%g' ) for ax in axs [ 0 , :]: for direction , nPanel in zip ([ 'y' , 'x' ], axs . shape ): if nPanel < 6 : eval ( f \"ax.set_{direction}scale('log')\" ) eval ( f \"ax.{direction}axis\" ). set_minor_formatter ( sensible_f ) eval ( f \"ax.{direction}axis\" ). set_major_formatter ( sensible_f ) # Tuning panels only table = tables [ 0 ] for a , panel in zip ( tables . axes_roles [ \"optim\" ] or (), table . panels [ 1 :]): yy = tables . xp_dict . tickz ( a ) axis_scale_by_array ( panel , yy , \"y\" ) # set_ymargin doesn 't work for wonky scales. Do so manually: alpha = len(yy)/10 y0, y1, y2, y3 = yy[0], yy[1], yy[-2], yy[-1] panel.set_ylim(y0-alpha*(y1-y0), y3+alpha*(y3-y2)) # All panels for ax in axs.ravel(): for direction, nPanel in zip([' y ', ' x ' ], axs . shape ): if nPanel < 6 : ax . grid ( True , which = \"minor\" , axis = direction ) # Not strictly compatible with gridspec height_ratios , # ( throws warning ), but still works ok . with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = UserWarning ) axs [ 0 , 0 ]. figure . tight_layout ()","title":"default_fig_adjustments"},{"location":"reference/dapper/data_management/#default_styles","text":"def default_styles ( coord , baseline_legends = False ) Quick and dirty (but somewhat robust) styling. View Source def default_styles ( coord , baseline_legends = False ): \"\"\"Quick and dirty (but somewhat robust) styling.\"\"\" style = dict_tools . DotDict ( ms = 8 ) style . label = make_label ( coord ) try : if coord . da_method == \"Climatology\" : style . ls = \":\" style . c = \"k\" if not baseline_legends : style . label = None elif coord . da_method == \"OptInterp\" : style . ls = \":\" style . c = . 7 * np . ones ( 3 ) style . label = \"Opt. Interp.\" if not baseline_legends : style . label = None elif coord . da_method == \"Var3D\" : style . ls = \":\" style . c = . 5 * np . ones ( 3 ) style . label = \"3D-Var\" if not baseline_legends : style . label = None elif coord . da_method == \"EnKF\" : style . marker = \"*\" style . c = \"C1\" elif coord . da_method == \"PartFilt\" : style . marker = \"X\" style . c = \"C2\" else : style . marker = \".\" except AttributeError : pass return style","title":"default_styles"},{"location":"reference/dapper/data_management/#discretize_cmap","text":"def discretize_cmap ( cmap , N , val0 = 0 , val1 = 1 , name = None ) Discretize cmap so that it partitions [0,1] into N segments. I.e. cmap(k/N) == cmap(k/N + eps). Also provide the ScalarMappable sm that maps range(N) to the segment centers, as will be reflected by cb = fig.colorbar(sm) . You can then re-label the ticks using cb.set_ticks(np.arange(N)); cb.set_ticklabels([\"A\",\"B\",\"C\",...]) . View Source def discretize_cmap ( cmap , N , val0 = 0 , val1 = 1 , name = None ) : \" \"\" Discretize cmap so that it partitions [0,1] into N segments. I.e. cmap(k/N) == cmap(k/N + eps). Also provide the ScalarMappable ``sm`` that maps range(N) to the segment centers, as will be reflected by ``cb = fig.colorbar(sm)``. You can then re-label the ticks using ``cb.set_ticks(np.arange(N)); cb.set_ticklabels([\" A \",\" B \",\" C \",...])``. \"\" \" # cmap(k/N) from_list = mpl . colors . LinearSegmentedColormap . from_list colors = cmap ( np . linspace ( val0 , val1 , N )) cmap = from_list ( name , colors , N ) # sm cNorm = mpl . colors . Normalize ( - .5 , - .5 + N ) sm = mpl . cm . ScalarMappable ( cNorm , cmap ) return cmap , sm","title":"discretize_cmap"},{"location":"reference/dapper/data_management/#in_idx","text":"def in_idx ( coord , indices , xp_dict , axis ) Essentially: coord.axis in ticks[indices] View Source def in_idx ( coord , indices , xp_dict , axis ) : \"\"\"Essentially: coord.axis in ticks[indices]\"\"\" if hasattr ( coord , axis ) : ticks = np . array ( xp_dict . ticks [ axis ] ) [ indices ] return getattr ( coord , axis ) in ticks else : return True","title":"in_idx"},{"location":"reference/dapper/data_management/#load_hmm","text":"def load_HMM ( save_as ) View Source def load_HMM ( save_as ): save_as = Path ( save_as ) . expanduser () HMM = dill . load ( open ( save_as / \"xp.com\" , \"rb\" ))[ \"HMM\" ] return HMM","title":"load_HMM"},{"location":"reference/dapper/data_management/#load_xps","text":"def load_xps ( save_as ) Load xps (as a simple list) from dir. View Source def load_xps ( save_as ) : \" \"\" Load ``xps`` (as a simple list) from dir. \"\" \" save_as = Path ( save_as ). expanduser () files = [ d / \"xp\" for d in uplink . list_job_dirs ( save_as ) ] def load_any ( filepath ) : \" \"\" Load any/all ``xp's`` from ``filepath``. \"\" \" with open ( filepath , \"rb\" ) as F : # If experiment crashed, then xp will be empty try : data = dill . load ( F ) except EOFError : return [] # Always return list try : return data [ \"xps\" ] except KeyError : return [ data [ \"xp\" ]] print ( \"Loading %d files from %s\" % ( len ( files ), save_as )) xps = [] # NB: progbar wont clean up properly w/ list compr. for f in utils . progbar ( files , desc = \"Loading\" ) : xps . extend ( load_any ( f )) if len ( xps ) < len ( files ) : print ( f \"{len(files)-len(xps)} files could not be loaded.\" ) return xps","title":"load_xps"},{"location":"reference/dapper/data_management/#make_label","text":"def make_label ( coord , no_key = ( 'da_method' , 'Const' , 'upd_a' ), exclude = () ) View Source def make_label ( coord , no_key = NO_KEY , exclude = ()): dct = { a : v for a , v in coord . _asdict (). items () if v != None } lbl = '' for k , v in dct . items (): if k not in exclude : if any ( x in k for x in no_key ): lbl = lbl + f ' {v}' else : lbl = lbl + f ' {utils.collapse_str(k,7)}:{v}' return lbl [ 1 :]","title":"make_label"},{"location":"reference/dapper/data_management/#overwrite_xps","text":"def overwrite_xps ( xps , save_as , nDir = 100 ) Save xps in save_as, but safely (by first saving to tmp). View Source def overwrite_xps ( xps , save_as , nDir = 100 ): \"\"\"Save xps in save_as, but safely (by first saving to tmp).\"\"\" save_xps ( xps , save_as / \"tmp\" , nDir ) # Delete for d in utils . tqdm . tqdm ( uplink . list_job_dirs ( save_as ), desc = \"Deleting old\" ): shutil . rmtree ( d ) # Mv up from tmp / -- goes quick, coz there are not many. for d in os . listdir ( save_as / \"tmp\" ): shutil . move ( save_as / \"tmp\" / d , save_as / d ) shutil . rmtree ( save_as / \"tmp\" )","title":"overwrite_xps"},{"location":"reference/dapper/data_management/#reduce_inodes","text":"def reduce_inodes ( save_as , nDir = 100 ) Reduce the number of xp dirs by packing multiple xp s into lists ( xps ). This reduces the number of files (inodes) on the system, which limits storage capacity (along with size ). It also deletes files \"xp.var\" and \"out\" (which tends to be relatively large coz of the progbar). This is probably also the reason that the loading time is sometimes reduced. View Source def reduce_inodes ( save_as , nDir = 100 ): \"\"\"Reduce the number of ``xp`` dirs by packing multiple ``xp``s into lists (``xps``). This reduces the **number** of files (inodes) on the system, which limits storage capacity (along with **size**). It also deletes files \"xp.var\" and \"out\" (which tends to be relatively large coz of the progbar). This is probably also the reason that the loading time is sometimes reduced.\"\"\" overwrite_xps ( load_xps ( save_as ), save_as , nDir )","title":"reduce_inodes"},{"location":"reference/dapper/data_management/#rel_index","text":"def rel_index ( elem , lst , default = None ) lst.index(elem) / len(lst) with fallback. View Source def rel_index ( elem , lst , default = None ): \"\"\"``lst.index(elem) / len(lst)`` with fallback.\"\"\" try : return lst . index ( elem ) / len ( lst ) except ValueError : if default == None : raise return default","title":"rel_index"},{"location":"reference/dapper/data_management/#save_xps","text":"def save_xps ( xps , save_as , nDir = 100 ) Split xps and save in save_as/i for i in range(nDir). Example: rename attr n_iter to nIter: proj_name = \"Stein\" dd = dpr.rc.dirs.data / proj_name save_as = dd / \"run_2020-09-22__19:36:13\" for save_as in os.listdir(dd): save_as = dd / save_as xps = load_xps ( save_as ) HMM = load_HMM ( save_as ) for xp in xps : if hasattr ( xp , \"n_iter\" ): xp . nIter = xp . n_iter del xp . n_iter overwrite_xps ( xps , save_as ) View Source def save_xps ( xps , save_as , nDir = 100 ): \"\"\"Split xps and save in save_as/i for i in range(nDir). Example: rename attr n_iter to nIter: >>> proj_name = \"Stein\" >>> dd = dpr.rc.dirs.data / proj_name >>> save_as = dd / \"run_2020-09-22__19:36:13\" >>> >>> for save_as in os.listdir(dd): >>> save_as = dd / save_as >>> >>> xps = load_xps(save_as) >>> HMM = load_HMM(save_as) >>> >>> for xp in xps: >>> if hasattr(xp,\"n_iter\"): >>> xp.nIter = xp.n_iter >>> del xp.n_iter >>> >>> overwrite_xps(xps, save_as) \"\"\" save_as = Path ( save_as ) . expanduser () save_as . mkdir ( parents = False , exist_ok = False ) splitting = np . array_split ( xps , nDir ) for i , sub_xps in enumerate ( utils . tqdm . tqdm ( splitting , desc = \"Saving\" )): if len ( sub_xps ): iDir = save_as / str ( i ) os . mkdir ( iDir ) with open ( iDir / \"xp\" , \"wb\" ) as F : dill . dump ({ 'xps' : sub_xps }, F )","title":"save_xps"},{"location":"reference/dapper/data_management/#classes","text":"","title":"Classes"},{"location":"reference/dapper/data_management/#sparsespace","text":"class SparseSpace ( axes , * args , ** kwargs ) Dict, subclassed enforce key conformity (to a coord. sys, i.e. a space). The coordinate system is specified by its \"axes\", which is used to produce self.Coord (a namedtuple class). As a normal dict, it can hold any type of objects. In normal use, this space is highly sparse, coz there are many coordinates with no matching experiment, eg. coord(da_method=Climatology, rot=True, ...). Indeed, operations across (potentially multiple simultaneous) axes, such as optimization or averaging, should be carried out by iterating -- not over the axis -- but over the the list of items. The most important method is nest() , which is used (by xpSpace.table_tree) to separate tables/columns, and also to carry out the mean/optim operations. In addition, getitem () is very flexible, allowing accessing by: - The actual key, a self.Coord object. Returns single item. - A dict match against (part of) the coordinates. Returns subspace. - An int. Returns list(self)[key]. - A list of any of the above. Returns list. This flexibility can cause bugs, but it's probably still worth it). Also see call (), get_for(), and coords(), for further convenience. Inspired by https://stackoverflow.com/a/7728830 Also see https://stackoverflow.com/q/3387691 View Source class SparseSpace ( dict ) : \"\"\"Dict, subclassed enforce key conformity (to a coord. sys, i.e. a space). The coordinate system is specified by its \" axes \", which is used to produce self.Coord (a namedtuple class). As a normal dict, it can hold any type of objects. In normal use, this space is highly sparse, coz there are many coordinates with no matching experiment, eg. coord(da_method=Climatology, rot=True, ...). Indeed, operations across (potentially multiple simultaneous) axes, such as optimization or averaging, should be carried out by iterating -- not over the axis -- but over the the list of items. The most important method is ``nest()``, which is used (by xpSpace.table_tree) to separate tables/columns, and also to carry out the mean/optim operations. In addition, __getitem__() is very flexible, allowing accessing by: - The actual key, a self.Coord object. Returns single item. - A dict match against (part of) the coordinates. Returns subspace. - An int. Returns list(self)[key]. - A list of any of the above. Returns list. This flexibility can cause bugs, but it's probably still worth it). Also see __call__(), get_for(), and coords(), for further convenience. Inspired by https://stackoverflow.com/a/7728830 Also see https://stackoverflow.com/q/3387691 \"\"\" @property def axes ( self ) : return self . Coord . _fields def __init__ ( self , axes , * args , ** kwargs ) : # Define coordinate system self . Coord = collections . namedtuple ( 'Coord' , axes ) # Write dict self . update ( * args , ** kwargs ) # Add repr / str self . Coord . __repr__ = lambda c : \",\" . join ( f \"{k}={v!r}\" for k , v in zip ( c . _fields , c )) self . Coord . __str__ = lambda c : \",\" . join ( str ( v ) for v in c ) def update ( self , * args , ** kwargs ) : \"\"\"Update using custom __setitem__().\"\"\" # See https : // stackoverflow . com / a / 2588648 # and https : // stackoverflow . com / a / 2390997 for k , v in dict ( * args , ** kwargs ). items () : self [ k ] = v def __setitem__ ( self , key , val ) : \"\"\"Setitem ensuring coordinate conforms.\"\"\" try : key = self . Coord ( * key ) except TypeError : raise TypeError ( f \"The key {key!r} did not fit the coord. system \" f \"which has axes {self.axes}\" ) super (). __setitem__ ( key , val ) def __getitem__ ( self , key ) : \"\"\"Flexible indexing.\"\"\" # List of items ( by a list of indices ). # Also see get_for (). if isinstance ( key , list ) : return [ self[k ] for k in key ] # Single ( by integer ) or list ( by Slice ) # Note : NOT validating np . int64 here catches quite a few bugs . elif isinstance ( key , int ) or isinstance ( key , slice ) : return [ *self.values() ][ key ] # Subspace ( by dict , ie . an informal , partial coordinate ) elif isinstance ( key , dict ) : outer = self . nest ( outer_axes = list ( key )) # nest coord = outer . Coord ( * key . values ()) # create coord inner = outer [ coord ] # chose subspace return inner # Single item ( by Coord object , coz an integer ( eg ) # gets interpreted ( above ) as a list index ) else : # NB : Dont 't use isinstance(key, self.Coord) # coz it fails when the namedtuple (Coord) has been # instantiated in different places (but with equal params). # Also see bugs.python.org/issue7796 return super().__getitem__(key) def __getkey__(self, entry): \"\"\"Inverse of dict.__getitem__(), but also works on coords. Note: This dunder method is not a \"builtin\" naming convention.\"\"\" coord = (getattr(entry, a, None) for a in self.axes) return self.Coord(*coord) def __call__(self, **kwargs): \"\"\"Convenience, that enables, eg.: >>> xp_dict(da_method=\"EnKF\", infl=1, seed=3) \"\"\" return self.__getitem__(kwargs) def get_for(self, ticks, default=None): \"\"\"Almost ``[self.get(Coord(x)) for x in ticks]``. NB: using the \"naive\" thing: ``[self[x] for x in ticks]`` would probably be a BUG coz x gets interpreted as indices for the internal list.\"\"\" singleton = not hasattr(ticks[0], \"__iter__\") def coord(xyz): return self.Coord(xyz if singleton else xyz) return [self.get(coord(x), default) for x in ticks] def coords(self, **kwargs): \"\"\"Get all ``coord``s matching kwargs. Unlike ``__getitem__(kwargs)``, - A list is returned, not a subspace. - This list constains keys (coords), not values. - The coords refer to the original space, not the subspace. The last point is especially useful for label_xSection(). \"\"\" def embed(coord): return {**kwargs, **coord._asdict()} return [self.Coord(**embed(x)) for x in self[kwargs]] # Old implementation. # - I prefer the new version for its re-use of __getitem__' s # nesting , evidencing their mutual relationship ) # - Note that unlike xpList . inds () : missingval shenanigans # are here unnecessary coz each coordinate is complete . # match = lambda x : all ( getattr ( x , k ) == kwargs [ k ] for k in kwargs ) # return [ x for x in self if match(x) ] def __repr__ ( self ) : # Note : print ( xpList ( self )) produces more human - readable key listing , # but we don 't want to implement it here, coz it requires split_attrs(), # which we don' t really want to call again . L = 2 keys = [ str(k) for k in self ] if 2 * L < len ( keys ) : keys = keys [ :L ] + [ \"...\" ] + keys [ -L: ] keys = \"[\\n \" + \",\\n \" . join ( keys ) + \"\\n]\" txt = f \"<{self.__class__.__name__}> with {len(self)} keys: {keys}\" # txt += \" befitting the coord. sys. with axes \" txt += \"\\nplaced in a coord-sys with axes \" try : txt += \"(and ticks):\" + str ( dict_tools . AlignedDict ( self . ticks )) except AttributeError : txt += \":\\n\" + str ( self . axes ) return txt def nest ( self , inner_axes = None , outer_axes = None ) : \"\"\"Return a new xpSpace with axes ``outer_axes``, obtained by projecting along the ``inner_axes``. The entries of this ``xpSpace`` are themselves ``xpSpace``s, with axes ``inner_axes``, each one regrouping the entries with the same (projected) coordinate. Note: is also called by ``__getitem__(key)`` if ``key`` is dict.\"\"\" # Default : a singleton outer space , # with everything contained in the inner ( projection ) space . if inner_axes is None and outer_axes is None : outer_axes = () # Validate axes if inner_axes is None : assert outer_axes is not None inner_axes = dict_tools . complement ( self . axes , outer_axes ) else : assert outer_axes is None outer_axes = dict_tools . complement ( self . axes , inner_axes ) # Fill spaces outer_space = self . __class__ ( outer_axes ) for coord , entry in self . items () : outer_coord = outer_space . __getkey__ ( coord ) try : inner_space = outer_space [ outer_coord ] except KeyError : inner_space = self . __class__ ( inner_axes ) outer_space [ outer_coord ] = inner_space inner_space [ inner_space.__getkey__(coord) ] = entry return outer_space def add_axis ( self , axis ) : self . __init__ ( self . axes + ( axis ,)) for coord in list ( self ) : entry = self . pop ( coord ) self [ coord + (None,) ] = entry def intersect_axes ( self , attrs ) : \"\"\"Rm those a in attrs that are not in self.axes. This allows errors in the axes allotment, for ease-of-use.\"\"\" absent = dict_tools . complement ( attrs , self . axes ) if absent : print ( color_text ( \"Warning:\" , colorama . Fore . RED ), \"The requested attributes\" , color_text ( str ( absent ), colorama . Fore . RED ), ( \"were not found among the\" \" xpSpace axes (attrs. used as coordinates\" \" for the set of experiments).\" \" This may be no problem if the attr. is redundant\" \" for the coord-sys.\" \" However, if it is caused by confusion or mis-spelling,\" \" then it is likely to cause mis-interpretation\" \" of the shown results.\" )) attrs = dict_tools . complement ( attrs , absent ) return attrs def label_xSection ( self , label , * NoneAttrs , ** sub_coord ) : \"\"\"Insert duplicate entries for the cross section whose ``coord``s match ``sub_coord``, adding the attr ``Const=label`` to their ``coord``, reflecting the \" constance / constraint / fixation \" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up in ``nest()``. If you wish, you can specify the ``NoneAttrs``, which are consequently set to None for the duplicated entries, preventing them from getting plotted in tuning panels. \"\"\" if \"Const\" not in self . axes : self . add_axis ( 'Const' ) for coord in self . coords ( ** self . intersect_axes ( sub_coord )) : entry = copy . deepcopy ( self [ coord ] ) coord = coord . _replace ( Const = label ) coord = coord . _replace ( ** { a : None for a in NoneAttrs } ) self [ coord ] = entry","title":"SparseSpace"},{"location":"reference/dapper/data_management/#ancestors-in-mro","text":"builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/dapper/data_management/#descendants","text":"dapper.data_management.xpSpace","title":"Descendants"},{"location":"reference/dapper/data_management/#instance-variables","text":"axes","title":"Instance variables"},{"location":"reference/dapper/data_management/#methods","text":"","title":"Methods"},{"location":"reference/dapper/data_management/#add_axis","text":"def add_axis ( self , axis ) View Source def add_axis ( self , axis ): self . __init__ ( self . axes + ( axis ,)) for coord in list ( self ): entry = self . pop ( coord ) self [ coord + ( None ,)] = entry","title":"add_axis"},{"location":"reference/dapper/data_management/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/dapper/data_management/#coords","text":"def coords ( self , ** kwargs ) Get all coord s matching kwargs. Unlike __getitem__(kwargs) , - A list is returned, not a subspace. - This list constains keys (coords), not values. - The coords refer to the original space, not the subspace. The last point is especially useful for label_xSection(). View Source def coords ( self , ** kwargs ): \"\"\"Get all ``coord``s matching kwargs. Unlike ``__getitem__(kwargs)``, - A list is returned, not a subspace. - This list constains keys (coords), not values. - The coords refer to the original space, not the subspace. The last point is especially useful for label_xSection(). \"\"\" def embed ( coord ): return { ** kwargs , ** coord . _asdict ()} return [ self . Coord ( ** embed ( x )) for x in self [ kwargs ]]","title":"coords"},{"location":"reference/dapper/data_management/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/dapper/data_management/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/dapper/data_management/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/dapper/data_management/#get_for","text":"def get_for ( self , ticks , default = None ) Almost [self.get(Coord(x)) for x in ticks] . NB: using the \"naive\" thing: [self[x] for x in ticks] would probably be a BUG coz x gets interpreted as indices for the internal list. View Source def get_for ( self , ticks , default = None ) : \"\"\"Almost ``[self.get(Coord(x)) for x in ticks]``. NB: using the \" naive \" thing: ``[self[x] for x in ticks]`` would probably be a BUG coz x gets interpreted as indices for the internal list.\"\"\" singleton = not hasattr ( ticks [ 0 ] , \"__iter__\" ) def coord ( xyz ) : return self . Coord ( xyz if singleton else xyz ) return [ self.get(coord(x), default) for x in ticks ]","title":"get_for"},{"location":"reference/dapper/data_management/#intersect_axes","text":"def intersect_axes ( self , attrs ) Rm those a in attrs that are not in self.axes. This allows errors in the axes allotment, for ease-of-use. View Source def intersect_axes ( self , attrs ): \"\"\"Rm those a in attrs that are not in self.axes. This allows errors in the axes allotment, for ease-of-use.\"\"\" absent = dict_tools . complement ( attrs , self . axes ) if absent : print ( color_text ( \"Warning:\" , colorama . Fore . RED ), \"The requested attributes\" , color_text ( str ( absent ), colorama . Fore . RED ), ( \"were not found among the\" \" xpSpace axes (attrs. used as coordinates\" \" for the set of experiments).\" \" This may be no problem if the attr. is redundant\" \" for the coord-sys.\" \" However, if it is caused by confusion or mis-spelling,\" \" then it is likely to cause mis-interpretation\" \" of the shown results.\" )) attrs = dict_tools . complement ( attrs , absent ) return attrs","title":"intersect_axes"},{"location":"reference/dapper/data_management/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/dapper/data_management/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/dapper/data_management/#label_xsection","text":"def label_xSection ( self , label , * NoneAttrs , ** sub_coord ) Insert duplicate entries for the cross section whose coord s match sub_coord , adding the attr Const=label to their coord , reflecting the \"constance/constraint/fixation\" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up in nest() . If you wish, you can specify the NoneAttrs , which are consequently set to None for the duplicated entries, preventing them from getting plotted in tuning panels. View Source def label_xSection ( self , label , * NoneAttrs , ** sub_coord ) : \" \"\" Insert duplicate entries for the cross section whose ``coord``s match ``sub_coord``, adding the attr ``Const=label`` to their ``coord``, reflecting the \" constance / constraint / fixation \" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up in ``nest()``. If you wish, you can specify the ``NoneAttrs``, which are consequently set to None for the duplicated entries, preventing them from getting plotted in tuning panels. \"\" \" if \"Const\" not in self . axes : self . add_axis ( 'Const' ) for coord in self . coords ( ** self . intersect_axes ( sub_coord )) : entry = copy . deepcopy ( self [ coord ] ) coord = coord . _replace ( Const = label ) coord = coord . _replace ( ** { a : None for a in NoneAttrs } ) self [ coord ] = entry","title":"label_xSection"},{"location":"reference/dapper/data_management/#nest","text":"def nest ( self , inner_axes = None , outer_axes = None ) Return a new xpSpace with axes outer_axes , obtained by projecting along the inner_axes . The entries of this xpSpace are themselves xpSpace s, with axes inner_axes , each one regrouping the entries with the same (projected) coordinate. Note: is also called by __getitem__(key) if key is dict. View Source def nest ( self , inner_axes = None , outer_axes = None ) : \" \"\" Return a new xpSpace with axes ``outer_axes``, obtained by projecting along the ``inner_axes``. The entries of this ``xpSpace`` are themselves ``xpSpace``s, with axes ``inner_axes``, each one regrouping the entries with the same (projected) coordinate. Note: is also called by ``__getitem__(key)`` if ``key`` is dict. \"\" \" # Default: a singleton outer space, # with everything contained in the inner (projection) space. if inner_axes is None and outer_axes is None : outer_axes = () # Validate axes if inner_axes is None : assert outer_axes is not None inner_axes = dict_tools . complement ( self . axes , outer_axes ) else : assert outer_axes is None outer_axes = dict_tools . complement ( self . axes , inner_axes ) # Fill spaces outer_space = self . __class__ ( outer_axes ) for coord , entry in self . items () : outer_coord = outer_space . __getkey__ ( coord ) try : inner_space = outer_space [ outer_coord ] except KeyError : inner_space = self . __class__ ( inner_axes ) outer_space [ outer_coord ] = inner_space inner_space [ inner_space . __getkey__ ( coord ) ] = entry return outer_space","title":"nest"},{"location":"reference/dapper/data_management/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/dapper/data_management/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/dapper/data_management/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/dapper/data_management/#update","text":"def update ( self , * args , ** kwargs ) Update using custom setitem (). View Source def update ( self , * args , ** kwargs ) : \"\"\"Update using custom __setitem__().\"\"\" # See https : // stackoverflow . com / a / 2588648 # and https : // stackoverflow . com / a / 2390997 for k , v in dict ( * args , ** kwargs ). items () : self [ k ] = v","title":"update"},{"location":"reference/dapper/data_management/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/dapper/data_management/#xpspace","text":"class xpSpace ( axes , * args , ** kwargs ) Functionality to facilitate working with xps and their results. The function from_list() initializes a SparseSpace from a list of objects, typically experiments referred to as xps , by (1) computing the relevant axes from the attributes, and (2) filling the dict by xps . The main use of xpSpace is through its print() & plot() , both of which call table_tree() to nest the axes of the SparseSpace. For custom plotting, you will likely want to start with table_tree() . Using from_list(xps) creates a SparseSpace holding xps . However, the nested xpSpace s output by table_tree() , will hold objects of type UncertainQtty , coz table_tree() calls mean() calls field(statkey) . View Source class xpSpace ( SparseSpace ) : \"\"\"Functionality to facilitate working with ``xps`` and their results. The function ``from_list()`` initializes a ``SparseSpace`` from a list of objects, typically experiments referred to as ``xps``, by (1) computing the relevant axes from the attributes, and (2) filling the dict by ``xps``. The main use of xpSpace is through its ``print()`` & ``plot()``, both of which call ``table_tree()`` to nest the axes of the SparseSpace. For custom plotting, you will likely want to start with ``table_tree()``. Using ``from_list(xps)`` creates a SparseSpace holding ``xps``. However, the nested ``xpSpace``s output by ``table_tree()``, will hold objects of type ``UncertainQtty``, coz ``table_tree()`` calls ``mean()`` calls ``field(statkey)``.\"\"\" @classmethod def from_list ( cls , xps ) : \"\"\"Init xpSpace from xpList.\"\"\" def make_ticks ( axes , ordering = dict ( N = 'default' , seed = 'default' , infl = 'default' , loc_rad = 'default' , rot = 'as_found' , da_method = 'as_found' , )) : \"\"\"Unique & sort, for each axis (individually) in axes.\"\"\" for ax_name , arr in axes . items () : ticks = set ( arr ) # unique ( jumbles order ) # Sort order = ordering . get ( ax_name , 'default' ). lower () if hasattr ( order , '__call__' ) : # eg . mylist . index ticks = sorted ( ticks , key = order ) elif 'as_found' in order : ticks = sorted ( ticks , key = arr . index ) else : # default sorting , with None placed at the end ticks = sorted ( ticks , key = lambda x : ( x is None , x )) if any ( x in order for x in [ 'rev' , 'inv' ]) : ticks = ticks [ ::- 1 ] axes [ ax_name ] = ticks # Define axes xp_list = xpList ( xps ) axes = xp_list . split_attrs ( nomerge = [ 'Const' ])[ 0 ] make_ticks ( axes ) self = cls ( axes . keys ()) # Note : this attr ( ticks ) will not be propagated through nest (). # That is fine . Otherwise we should have to prune the ticks # ( if they are to be useful ), which we don't want to do. self.ticks = axes # Fill self.update({self.__getkey__(xp): xp for xp in xps}) return self def field(self, statkey=\"rmse.a\"): \"\"\"Extract ``statkey`` for each item in ``self``.\"\"\" # Init a new xpDict to hold field avrgs = self.__class__(self.axes) found_anything = False for coord, xp in self.items(): val = getattr(xp.avrgs, statkey, None) avrgs[coord] = val found_anything = found_anything or (val is not None) if not found_anything: raise AttributeError( f\"The stat. field ' { statkey } ' was not found\" \" among any of the xp's . \") return avrgs def mean(self, axes=None): # Note: The case ``axes=()`` should work w/o special treatment. if axes is None: return self nested = self.nest(axes) for coord, space in nested.items(): def getval(uq): return uq.val if isinstance(uq, UncertainQtty) else uq vals = [getval(uq) for uq in space.values()] # Don't use nanmean! It would give false impressions. mu = np.mean(vals) with warnings.catch_warnings(): warnings.simplefilter(\" ignore \", category=RuntimeWarning) # Don't print warnings caused by N=1. # It already correctly yield nan's. var = np.var(vals, ddof=1) N = len(vals) uq = UncertainQtty(mu, np.sqrt(var/N)) uq.nTotal = N uq.nFail = N - np.isfinite(vals).sum() uq.nSuccess = N - uq.nFail nested[coord] = uq return nested def tune(self, axes=None, costfun=None): \"\"\" Get ( compile / tabulate ) a stat field optimised wrt . tuning params . \"\"\" # Define cost-function costfun = (costfun or 'increasing').lower() if 'increas' in costfun: costfun = (lambda x: +x) elif 'decreas' in costfun: costfun = (lambda x: -x) else: assert hasattr(costfun, '__call__') # custom # Note: The case ``axes=()`` should work w/o special treatment. if axes is None: return self nested = self.nest(axes) for coord, space in nested.items(): # Find optimal value and coord within space MIN = np.inf for i, (inner_coord, uq) in enumerate(space.items()): cost = costfun(uq.val) if cost <= MIN: MIN = cost uq_opt = uq uq_opt.tuned_coord = inner_coord nested[coord] = uq_opt return nested def validate_axes(self, axes): \"\"\" Validate axes . Note : This does not convert None to (), allowing None to remain special . Use `` axis or () `` wherever tuples are required . \"\"\" roles = {} # \" inv \" for role in set(axes) | set(AXES_ROLES): assert role in AXES_ROLES, f\" Invalid role { role ! r } \" aa = axes.get(role, AXES_ROLES[role]) if aa is None: pass # Purposely special else: # Ensure iterable if isinstance(aa, str) or not hasattr(aa, \" __ iter__ \"): aa = (aa,) aa = self.intersect_axes(aa) for axis in aa: # Ensure unique if axis in roles: raise TypeError( f\" An axis ( here { axis ! r }) cannot be assigned to 2 \" f\" roles ( here { role ! r } and { roles [ axis ]! r }). \") else: roles[axis] = role axes[role] = aa return axes def table_tree(self, statkey, axes): \"\"\" Hierarchical nest () : xp_dict > outer > inner > mean > optim . as specified by `` axes `` . Returns this new xpSpace . - print_1d / plot_1d ( respectively ) separate tables / panel ( row ) s for `` axes [ 'outer' ] `` , and columns / x - axis for `` axes [ 'inner' ] `` . - The `` axes [ 'mean' ] `` and `` axes [ 'optim' ] `` get eliminated by the mean () / tune () operations . Note : cannot support multiple statkeys because it's not (obviously) meaningful when optimizing over tuning_axes. \"\"\" axes = self.validate_axes(axes) def mean_tune(xp_dict): \"\"\"Take mean, then tune. Note: the SparseDict implementation should be sufficiently \"uncluttered\" that mean_tune() (or a few of its code lines) could be called anywhere above/between/below the ``nest()``ing of ``outer`` or ``inner``. These possibile call locations are commented in the code.\"\"\" uq_dict = xp_dict.field(statkey) uq_dict = uq_dict.mean(axes['mean']) uq_dict = uq_dict.tune(axes['optim']) return uq_dict self = mean_tune(self) # Prefer calling mean_tune() [also see its docstring] # before doing outer/inner nesting. This is because then the axes of # a row (xpSpace) should not include mean&optim, and thus: # - Column header/coords may be had directly as row.keys(), # without extraction by __getkey__() from (e.g.) row[0]. # - Don't need to propagate mean&optim axes down to the row level . # which would require defining rows by the nesting : # rows = table . nest ( outer_axes = dict_tools . complement ( table . axes , # * ( axes [ 'inner' ] or ()), # * ( axes [ 'mean' ] or ()), # * ( axes [ 'optim' ] or ()) )) # - Each level of the output from table_tree # is a smaller ( and more manageable ) dict . tables = self . nest ( outer_axes = axes [ 'outer' ]) for table_coord , table in tables . items () : # table = mean_tune ( table ) # Should not be used ( nesting as rows is more natural , # and is required for getting distinct / row_keys ). # cols = table . nest ( outer_axes = axes [ 'inner' ]) rows = table . nest ( inner_axes = axes [ 'inner' ] or ()) # Overwrite table by its nesting as rows tables [ table_coord ] = rows # for row_coord , row in rows . items () : # rows [ row_coord ] = mean_tune ( row ) return axes , tables def tickz ( self , axis_name ) : \"\"\"Axis ticks without None\"\"\" return [ x for x in self . ticks [ axis_name ] if x is not None ] def print ( self , statkey= \"rmse.a\" , axes = AXES_ROLES , subcols = True , decimals = None ) : \"\"\"Print tables of results. - statkey: The statistical field from the experiments to report. - subcols: If True, then subcolumns are added to indicate the 1\u03c3 confidence interval, and potentially some other stuff. - axes: Allots (maps) each role to a set of axis of the xp_dict. Suggestion: >>> dict( >>> outer='da_method', inner='N', mean='seed', >>> optim=('infl','loc_rad')) Example: If ``mean`` is assigned to: - (\" seed \",): Experiments are averaged accross seeds, and the 1\u03c3 (sub)col is computed as sqrt(var(xps)/N), where xps is a set of experiments. - () : Experiments are averaged across nothing (i.e. this is an edge case). - None : Experiments are not averaged (i.e. the values are the same as above), and the 1\u03c3 (sub)col is computed from the time series of that single experiment. \"\"\" import pandas as pd def align_subcols ( rows , cc , subcols , h2 ) : \"\"\"Subcolumns: align, justify, join.\"\"\" # Define subcol formats subc = dict () subc [ 'keys' ] = [ \"val\" , \"conf\" ] subc [ 'headers' ] = [ statkey , '1\u03c3' ] subc [ 'frmts' ] = [ None , None ] subc [ 'spaces' ] = [ ' \u00b1' , ] # last one gets appended below . subc [ 'aligns' ] = [ '>' , '<' ] # 4 header -- matter gets decimal - aligned . if axes [ 'optim' ] is not None : subc [ 'keys' ] += [ \"tuned_coord\" ] subc [ 'headers' ] += [ axes [ 'optim' ]] subc [ 'frmts' ] += [ lambda x : tuple ( a for a in x )] subc [ 'spaces' ] += [ ' *' ] subc [ 'aligns' ] += [ '<' ] elif axes [ 'mean' ] is not None : subc [ 'keys' ] += [ \"nFail\" , \"nSuccess\" ] subc [ 'headers' ] += [ '\u2620' , '\u2713' ] # use width - 1 symbols ! subc [ 'frmts' ] += [ None , None ] subc [ 'spaces' ] += [ ' ' , ' ' ] subc [ 'aligns' ] += [ '>' , '>' ] subc [ 'spaces' ]. append ( '' ) # no space after last subcol template = '{}' + '{}' . join ( subc [ 'spaces' ]) # Transpose columns = [ list ( x ) for x in zip ( * rows )] # Iterate over columns . for j , ( col_coord , column ) in enumerate ( zip ( cc , columns )) : # Tabulate columns if subcols : column = unpack_uqs ( column , decimals , subc [ \"keys\" ]) # Tabulate subcolumns subheaders = [] for key , header , frmt , _ , align in zip ( * subc . values ()) : column [ key ] = tabulate_column ( column [ key ], header , frmt = frmt )[ 1 : ] L = len ( column [ - 1 ][ key ]) if align == '<': subheaders += [ str ( header ). ljust ( L )] else : subheaders += [ str ( header ). rjust ( L )] # Join subcolumns : matter = [ template . format ( * [ row [ k ] for k in subc [ 'keys' ]]) for row in column ] header = template . format ( * subheaders ) else : column = unpack_uqs ( column , decimals )[ \"val\" ] column = tabulate_column ( column , statkey ) header , matter = column [ 0 ], column [ 1 : ] if h2: # Do super_header if j : super_header = str ( col_coord ) else : super_header = repr ( col_coord ) width = len ( header ) # += 1 if using unicode chars like \u2714\ufe0f super_header = super_header . center ( width , \"_\" ) header = super_header + \"\\n\" + header columns [ j ] = [ header ] + matter # Un - transpose rows = [ list ( x ) for x in zip ( * columns )] return rows # Inform axes [ \"mean\" ] if axes . get ( 'mean' , None ) : print ( f \"Averages (in time and) over {axes['mean']}.\" ) else : print ( \"Averages in time only\" \" (=> the 1\u03c3 estimates may be unreliable).\" ) axes , tables = self . table_tree ( statkey , axes ) for table_coord , table in tables . items () : # Get this table's column coords (cc). Use dict for sorted&unique. # cc = xp_dict.ticks[axes[\"inner\"]] # May be larger than needed. # cc = table[0].keys() # May be too small a set. cc = {c: None for row in table.values() for c in row} # Convert table (rows) into rows (lists) of equal length rows = [[row.get(c, None) for c in cc] for row in table.values()] if False: # ****************** Simple (for debugging) table for i, (row_coord, row) in enumerate(zip(table, rows)): row_key = \", \".join(str(v) for v in row_coord) rows[i] = [row_key] + row rows.insert(0, [f\"{table.axes}\"] + [repr(c) for c in cc]) else: # ********************** Elegant table. h2 = \"\\n\" if len(cc) > 1 else \"\" # do column-super-header rows = align_subcols(rows, cc, subcols, h2) # Make and prepend left-side table # - It's prettier if row_keys don't have unnecessary cols. # For example, the table of Climatology should not have an # entire column repeatedly displaying \"infl=None\". # => split_attrs(). # - Why didn't we do this for the column attrs? # Coz there we have no ambition to split the attrs , # which would also require excessive processing : # nesting the table as cols , and then split_attrs () on cols . row_keys = xpList ( table . keys ()). split_attrs ()[ 0 ] row_keys = pd . DataFrame . from_dict ( row_keys , dtype= \"O\" ) # allow storing None if len ( row_keys . columns ) : # Header rows [ 0 ] = [ h2 + k for k in row_keys ] + [ h2+'\u244a' ] + rows [ 0 ] # Matter for row , ( i , key ) in zip ( rows [ 1 : ], row_keys . iterrows ()) : rows [ i + 1 ] = [ * key ] + [ '|' ] + row # Print print ( \"\\n\" , end= \"\" ) if axes [ 'outer' ] : table_title = \"Table for \" + repr ( table_coord ) print ( color_text ( table_title , colorama . Back . YELLOW )) headers , * rows = rows print ( utils . tab ( rows , headers ). replace ( '\u2423' , ' ' )) def plot ( self , statkey= \"rmse.a\" , axes = AXES_ROLES , get_style = default_styles , fignum = None , figsize = None , panels = None , title2 = None , costfun = None , unique_labels = True ) : \"\"\"Plot the avrgs of ``statkey`` as a function of ``axis[\" inner \"]``. Firs of all, though, mean and optimum computations are done for ``axis[\" mean \"]`` and ``axis[\" optim \"]``. The optimal parameters are plotted in panels below the main plot. This can be prevented by providing the figure axes in ``panels``. Optionally, the experiments can be grouped by ``axis[\" outer \"]``, producing a figure with columns of panels.\"\"\" def plot1 ( panelcol , row , style ) : \"\"\"Plot a given line (row) in the main panel and the optim panels. Involves: Sort, insert None's, handle constant lines.\"\"\" # Make a full row ( yy ) of vals , whether is_constant or not . # row . is_constant = ( len ( row ) == 1 and next ( iter ( row )) == row . Coord ( None )) row . is_constant = all ( x == row . Coord ( None ) for x in row ) yy = [ row [ 0 ] if row . is_constant else y for y in row . get_for ( xticks )] # Plot main row . vals = [ getattr ( y , 'val' , None ) for y in yy ] row . handles = {} row . handles [ \"main_panel\" ] = panelcol [ 0 ]. plot ( xticks , row . vals , **style )[ 0 ] # Plot tuning params row . tuned_coords = {} # Store ordered , \"transposed\" argmins argmins = [ getattr ( y , 'tuned_coord' , None ) for y in yy ] for a , panel in zip ( axes [ \"optim\" ], panelcol [ 1 : ]) : yy = [ getattr ( coord , a , None ) for coord in argmins ] row . tuned_coords [ a ] = yy # Plotting all None 's sets axes units (like any plotting call) # which can cause trouble if the axes units were actually supposed # to be categorical (eg upd_a), but this is only revealed later. if not all(y == None for y in yy): row.handles[a] = panel.plot(xticks, yy, **style) # Nest axes through table_tree() assert len(axes[\"inner\"]) == 1, \"You must chose the abscissa.\" axes, tables = self.table_tree(statkey, axes) xticks = self.tickz(axes[\"inner\"][0]) # Figure panels if panels is None: nrows = len(axes['optim'] or ()) + 1 ncols = len(tables) maxW = 12.7 # my mac screen figsize = figsize or (min(5*ncols, maxW), 7) gs = dict( height_ratios=[6]+[1]*(nrows-1), hspace=0.05, wspace=0.05, # eyeballed: left=0.15/(1+np.log(ncols)), right=0.97, bottom=0.06, top=0.9) # Create _, panels = freshfig(num=fignum, figsize=figsize, nrows=nrows, sharex=True, ncols=ncols, sharey='row', gridspec_kw=gs) panels = np.ravel(panels).reshape((-1, ncols)) else: panels = np.atleast_2d(panels) # Title fig = panels[0, 0].figure fig_title = \"Average wrt. time\" if axes[\"mean\"] is not None: fig_title += f\" and {axes['mean']}\" if title2 is not None: fig_title += \"\\n\" + str(title2) fig.suptitle(fig_title) # Loop outer label_register = set() # mv inside loop to get legend on each panel for table_panels, (table_coord, table) in zip(panels.T, tables.items()): table.panels = table_panels title = '' if axes[\"outer\"] is None else repr(table_coord) # Plot for coord, row in table.items(): style = get_style(coord) # Rm duplicate labels (contrary to coords, labels can # be \"tampered\" with, and so can be duplicate) if unique_labels: if style.get(\"label\", None) in label_register: del style[\"label\"] else: label_register.add(style[\"label\"]) plot1(table.panels, row, style) # Beautify panel0 = table.panels[0] panel0.set_title(title) if panel0.is_first_col(): panel0.set_ylabel(statkey) with utils.set_tmp(mpl_logger, 'level ' , 99 ) : # silence \"no label\" msg panel0 . legend () table . panels [ - 1 ]. set_xlabel ( axes [ \"inner\" ][ 0 ]) # Tuning panels : for a , panel in zip ( axes [ \"optim\" ] or (), table . panels [ 1 : ]) : if panel . is_first_col () : panel . set_ylabel ( f \"Optim.\\n{a}\" ) tables . fig = fig tables . xp_dict = self tables . axes_roles = axes return tables","title":"xpSpace"},{"location":"reference/dapper/data_management/#ancestors-in-mro_1","text":"dapper.data_management.SparseSpace builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/dapper/data_management/#static-methods","text":"","title":"Static methods"},{"location":"reference/dapper/data_management/#from_list","text":"def from_list ( xps ) Init xpSpace from xpList. View Source @classmethod def from_list ( cls , xps ) : \"\"\"Init xpSpace from xpList.\"\"\" def make_ticks ( axes , ordering = dict ( N = 'default' , seed = 'default' , infl = 'default' , loc_rad = 'default' , rot = 'as_found' , da_method = 'as_found' , )) : \"\"\"Unique & sort, for each axis (individually) in axes.\"\"\" for ax_name , arr in axes . items () : ticks = set ( arr ) # unique ( jumbles order ) # Sort order = ordering . get ( ax_name , 'default' ). lower () if hasattr ( order , '__call__' ) : # eg . mylist . index ticks = sorted ( ticks , key = order ) elif 'as_found' in order : ticks = sorted ( ticks , key = arr . index ) else : # default sorting , with None placed at the end ticks = sorted ( ticks , key = lambda x : ( x is None , x )) if any ( x in order for x in [ 'rev' , 'inv' ]) : ticks = ticks [ ::- 1 ] axes [ ax_name ] = ticks # Define axes xp_list = xpList ( xps ) axes = xp_list . split_attrs ( nomerge = [ 'Const' ])[ 0 ] make_ticks ( axes ) self = cls ( axes . keys ()) # Note : this attr ( ticks ) will not be propagated through nest (). # That is fine . Otherwise we should have to prune the ticks # ( if they are to be useful ), which we don ' t want to do . self . ticks = axes # Fill self . update ({ self . __ getkey__ ( xp ) : xp for xp in xps }) return self","title":"from_list"},{"location":"reference/dapper/data_management/#instance-variables_1","text":"axes","title":"Instance variables"},{"location":"reference/dapper/data_management/#methods_1","text":"","title":"Methods"},{"location":"reference/dapper/data_management/#add_axis_1","text":"def add_axis ( self , axis ) View Source def add_axis ( self , axis ): self . __init__ ( self . axes + ( axis ,)) for coord in list ( self ): entry = self . pop ( coord ) self [ coord + ( None ,)] = entry","title":"add_axis"},{"location":"reference/dapper/data_management/#clear_1","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/dapper/data_management/#coords_1","text":"def coords ( self , ** kwargs ) Get all coord s matching kwargs. Unlike __getitem__(kwargs) , - A list is returned, not a subspace. - This list constains keys (coords), not values. - The coords refer to the original space, not the subspace. The last point is especially useful for label_xSection(). View Source def coords ( self , ** kwargs ): \"\"\"Get all ``coord``s matching kwargs. Unlike ``__getitem__(kwargs)``, - A list is returned, not a subspace. - This list constains keys (coords), not values. - The coords refer to the original space, not the subspace. The last point is especially useful for label_xSection(). \"\"\" def embed ( coord ): return { ** kwargs , ** coord . _asdict ()} return [ self . Coord ( ** embed ( x )) for x in self [ kwargs ]]","title":"coords"},{"location":"reference/dapper/data_management/#copy_1","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/dapper/data_management/#field","text":"def field ( self , statkey = 'rmse.a' ) Extract statkey for each item in self . View Source def field ( self , statkey = \"rmse.a\" ) : \" \"\" Extract ``statkey`` for each item in ``self``. \"\" \" # Init a new xpDict to hold field avrgs = self . __class__ ( self . axes ) found_anything = False for coord , xp in self . items () : val = getattr ( xp . avrgs , statkey , None ) avrgs [ coord ] = val found_anything = found_anything or ( val is not None ) if not found_anything : raise AttributeError ( f \"The stat. field '{statkey}' was not found\" \" among any of the xp's.\" ) return avrgs","title":"field"},{"location":"reference/dapper/data_management/#fromkeys_1","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/dapper/data_management/#get_1","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/dapper/data_management/#get_for_1","text":"def get_for ( self , ticks , default = None ) Almost [self.get(Coord(x)) for x in ticks] . NB: using the \"naive\" thing: [self[x] for x in ticks] would probably be a BUG coz x gets interpreted as indices for the internal list. View Source def get_for ( self , ticks , default = None ) : \"\"\"Almost ``[self.get(Coord(x)) for x in ticks]``. NB: using the \" naive \" thing: ``[self[x] for x in ticks]`` would probably be a BUG coz x gets interpreted as indices for the internal list.\"\"\" singleton = not hasattr ( ticks [ 0 ] , \"__iter__\" ) def coord ( xyz ) : return self . Coord ( xyz if singleton else xyz ) return [ self.get(coord(x), default) for x in ticks ]","title":"get_for"},{"location":"reference/dapper/data_management/#intersect_axes_1","text":"def intersect_axes ( self , attrs ) Rm those a in attrs that are not in self.axes. This allows errors in the axes allotment, for ease-of-use. View Source def intersect_axes ( self , attrs ): \"\"\"Rm those a in attrs that are not in self.axes. This allows errors in the axes allotment, for ease-of-use.\"\"\" absent = dict_tools . complement ( attrs , self . axes ) if absent : print ( color_text ( \"Warning:\" , colorama . Fore . RED ), \"The requested attributes\" , color_text ( str ( absent ), colorama . Fore . RED ), ( \"were not found among the\" \" xpSpace axes (attrs. used as coordinates\" \" for the set of experiments).\" \" This may be no problem if the attr. is redundant\" \" for the coord-sys.\" \" However, if it is caused by confusion or mis-spelling,\" \" then it is likely to cause mis-interpretation\" \" of the shown results.\" )) attrs = dict_tools . complement ( attrs , absent ) return attrs","title":"intersect_axes"},{"location":"reference/dapper/data_management/#items_1","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/dapper/data_management/#keys_1","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/dapper/data_management/#label_xsection_1","text":"def label_xSection ( self , label , * NoneAttrs , ** sub_coord ) Insert duplicate entries for the cross section whose coord s match sub_coord , adding the attr Const=label to their coord , reflecting the \"constance/constraint/fixation\" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up in nest() . If you wish, you can specify the NoneAttrs , which are consequently set to None for the duplicated entries, preventing them from getting plotted in tuning panels. View Source def label_xSection ( self , label , * NoneAttrs , ** sub_coord ) : \" \"\" Insert duplicate entries for the cross section whose ``coord``s match ``sub_coord``, adding the attr ``Const=label`` to their ``coord``, reflecting the \" constance / constraint / fixation \" this represents. This distinguishes the entries in this fixed-affine subspace, preventing them from being gobbled up in ``nest()``. If you wish, you can specify the ``NoneAttrs``, which are consequently set to None for the duplicated entries, preventing them from getting plotted in tuning panels. \"\" \" if \"Const\" not in self . axes : self . add_axis ( 'Const' ) for coord in self . coords ( ** self . intersect_axes ( sub_coord )) : entry = copy . deepcopy ( self [ coord ] ) coord = coord . _replace ( Const = label ) coord = coord . _replace ( ** { a : None for a in NoneAttrs } ) self [ coord ] = entry","title":"label_xSection"},{"location":"reference/dapper/data_management/#mean","text":"def mean ( self , axes = None ) View Source def mean ( self , axes = None ) : # Note : The case `` axes = () `` should work w / o special treatment . if axes is None : return self nested = self . nest ( axes ) for coord , space in nested . items () : def getval ( uq ) : return uq . val if isinstance ( uq , UncertainQtty ) else uq vals = [ getval(uq) for uq in space.values() ] # Don 't use nanmean! It would give false impressions. mu = np.mean(vals) with warnings.catch_warnings(): warnings.simplefilter(\"ignore\", category=RuntimeWarning) # Don' t print warnings caused by N = 1. # It already correctly yield nan ' s . var = np . var ( vals , ddof = 1 ) N = len ( vals ) uq = UncertainQtty ( mu , np . sqrt ( var / N )) uq . nTotal = N uq . nFail = N - np . isfinite ( vals ). sum () uq . nSuccess = N - uq . nFail nested [ coord ] = uq return nested","title":"mean"},{"location":"reference/dapper/data_management/#nest_1","text":"def nest ( self , inner_axes = None , outer_axes = None ) Return a new xpSpace with axes outer_axes , obtained by projecting along the inner_axes . The entries of this xpSpace are themselves xpSpace s, with axes inner_axes , each one regrouping the entries with the same (projected) coordinate. Note: is also called by __getitem__(key) if key is dict. View Source def nest ( self , inner_axes = None , outer_axes = None ) : \" \"\" Return a new xpSpace with axes ``outer_axes``, obtained by projecting along the ``inner_axes``. The entries of this ``xpSpace`` are themselves ``xpSpace``s, with axes ``inner_axes``, each one regrouping the entries with the same (projected) coordinate. Note: is also called by ``__getitem__(key)`` if ``key`` is dict. \"\" \" # Default: a singleton outer space, # with everything contained in the inner (projection) space. if inner_axes is None and outer_axes is None : outer_axes = () # Validate axes if inner_axes is None : assert outer_axes is not None inner_axes = dict_tools . complement ( self . axes , outer_axes ) else : assert outer_axes is None outer_axes = dict_tools . complement ( self . axes , inner_axes ) # Fill spaces outer_space = self . __class__ ( outer_axes ) for coord , entry in self . items () : outer_coord = outer_space . __getkey__ ( coord ) try : inner_space = outer_space [ outer_coord ] except KeyError : inner_space = self . __class__ ( inner_axes ) outer_space [ outer_coord ] = inner_space inner_space [ inner_space . __getkey__ ( coord ) ] = entry return outer_space","title":"nest"},{"location":"reference/dapper/data_management/#plot","text":"def plot ( self , statkey = 'rmse.a' , axes = { 'outer' : None , 'inner' : None , 'mean' : None , 'optim' : None }, get_style =< function default_styles at 0x7f8a722ab790 > , fignum = None , figsize = None , panels = None , title2 = None , costfun = None , unique_labels = True ) Plot the avrgs of statkey as a function of axis[\"inner\"] . Firs of all, though, mean and optimum computations are done for axis[\"mean\"] and axis[\"optim\"] . The optimal parameters are plotted in panels below the main plot. This can be prevented by providing the figure axes in panels . Optionally, the experiments can be grouped by axis[\"outer\"] , producing a figure with columns of panels. View Source def plot ( self , statkey = \"rmse.a\" , axes = AXES_ROLES , get_style = default_styles , fignum = None , figsize = None , panels = None , title2 = None , costfun = None , unique_labels = True ): \"\"\"Plot the avrgs of ``statkey`` as a function of ``axis[\"inner\"]``. Firs of all, though, mean and optimum computations are done for ``axis[\"mean\"]`` and ``axis[\"optim\"]``. The optimal parameters are plotted in panels below the main plot. This can be prevented by providing the figure axes in ``panels``. Optionally, the experiments can be grouped by ``axis[\"outer\"]``, producing a figure with columns of panels.\"\"\" def plot1 ( panelcol , row , style ): \"\"\"Plot a given line (row) in the main panel and the optim panels. Involves: Sort, insert None's, handle constant lines.\"\"\" # Make a full row (yy) of vals, whether is_constant or not. # row.is_constant = (len(row)==1 and next(iter(row))==row.Coord(None)) row . is_constant = all ( x == row . Coord ( None ) for x in row ) yy = [ row [ 0 ] if row . is_constant else y for y in row . get_for ( xticks )] # Plot main row . vals = [ getattr ( y , 'val' , None ) for y in yy ] row . handles = {} row . handles [ \"main_panel\" ] = panelcol [ 0 ] . plot ( xticks , row . vals , ** style )[ 0 ] # Plot tuning params row . tuned_coords = {} # Store ordered, \"transposed\" argmins argmins = [ getattr ( y , 'tuned_coord' , None ) for y in yy ] for a , panel in zip ( axes [ \"optim\" ], panelcol [ 1 :]): yy = [ getattr ( coord , a , None ) for coord in argmins ] row . tuned_coords [ a ] = yy # Plotting all None's sets axes units (like any plotting call) # which can cause trouble if the axes units were actually supposed # to be categorical (eg upd_a), but this is only revealed later. if not all ( y == None for y in yy ): row . handles [ a ] = panel . plot ( xticks , yy , ** style ) # Nest axes through table_tree() assert len ( axes [ \"inner\" ]) == 1 , \"You must chose the abscissa.\" axes , tables = self . table_tree ( statkey , axes ) xticks = self . tickz ( axes [ \"inner\" ][ 0 ]) # Figure panels if panels is None : nrows = len ( axes [ 'optim' ] or ()) + 1 ncols = len ( tables ) maxW = 12.7 # my mac screen figsize = figsize or ( min ( 5 * ncols , maxW ), 7 ) gs = dict ( height_ratios = [ 6 ] + [ 1 ] * ( nrows - 1 ), hspace = 0.05 , wspace = 0.05 , # eyeballed: left = 0.15 / ( 1 + np . log ( ncols )), right = 0.97 , bottom = 0.06 , top = 0.9 ) # Create _ , panels = freshfig ( num = fignum , figsize = figsize , nrows = nrows , sharex = True , ncols = ncols , sharey = 'row' , gridspec_kw = gs ) panels = np . ravel ( panels ) . reshape (( - 1 , ncols )) else : panels = np . atleast_2d ( panels ) # Title fig = panels [ 0 , 0 ] . figure fig_title = \"Average wrt. time\" if axes [ \"mean\" ] is not None : fig_title += f \" and {axes['mean']}\" if title2 is not None : fig_title += \" \\n \" + str ( title2 ) fig . suptitle ( fig_title ) # Loop outer label_register = set () # mv inside loop to get legend on each panel for table_panels , ( table_coord , table ) in zip ( panels . T , tables . items ()): table . panels = table_panels title = '' if axes [ \"outer\" ] is None else repr ( table_coord ) # Plot for coord , row in table . items (): style = get_style ( coord ) # Rm duplicate labels (contrary to coords, labels can # be \"tampered\" with, and so can be duplicate) if unique_labels : if style . get ( \"label\" , None ) in label_register : del style [ \"label\" ] else : label_register . add ( style [ \"label\" ]) plot1 ( table . panels , row , style ) # Beautify panel0 = table . panels [ 0 ] panel0 . set_title ( title ) if panel0 . is_first_col (): panel0 . set_ylabel ( statkey ) with utils . set_tmp ( mpl_logger , 'level' , 99 ): # silence \"no label\" msg panel0 . legend () table . panels [ - 1 ] . set_xlabel ( axes [ \"inner\" ][ 0 ]) # Tuning panels: for a , panel in zip ( axes [ \"optim\" ] or (), table . panels [ 1 :]): if panel . is_first_col (): panel . set_ylabel ( f \"Optim. \\n {a}\" ) tables . fig = fig tables . xp_dict = self tables . axes_roles = axes return tables","title":"plot"},{"location":"reference/dapper/data_management/#pop_1","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/dapper/data_management/#popitem_1","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/dapper/data_management/#print","text":"def print ( self , statkey = 'rmse.a' , axes = { 'outer' : None , 'inner' : None , 'mean' : None , 'optim' : None }, subcols = True , decimals = None ) Print tables of results. statkey: The statistical field from the experiments to report. subcols: If True, then subcolumns are added to indicate the 1\u03c3 confidence interval, and potentially some other stuff. axes: Allots (maps) each role to a set of axis of the xp_dict. Suggestion: dict( outer='da_method', inner='N', mean='seed', optim=('infl','loc_rad')) Example: If mean is assigned to: (\"seed\",): Experiments are averaged accross seeds, and the 1\u03c3 (sub)col is computed as sqrt(var(xps)/N), where xps is a set of experiments. () : Experiments are averaged across nothing (i.e. this is an edge case). None : Experiments are not averaged (i.e. the values are the same as above), and the 1\u03c3 (sub)col is computed from the time series of that single experiment. View Source def print ( self , statkey = \"rmse.a\" , axes = AXES_ROLES , subcols = True , decimals = None ) : \"\"\"Print tables of results. - statkey : The statistical field from the experiments to report . - subcols : If True , then subcolumns are added to indicate the 1 \u03c3 confidence interval , and potentially some other stuff . - axes : Allots ( maps ) each role to a set of axis of the xp_dict . Suggestion : >>> dict ( >>> outer = ' da_method ' , inner = 'N' , mean = ' seed ' , >>> optim = ( ' infl ',' loc_rad ' )) Example : If `` mean `` is assigned to : - ( \"seed\" ,) : Experiments are averaged accross seeds , and the 1 \u03c3 ( sub ) col is computed as sqrt ( var ( xps ) / N ), where xps is a set of experiments . - () : Experiments are averaged across nothing ( i . e . this is an edge case ). - None : Experiments are not averaged ( i . e . the values are the same as above ), and the 1 \u03c3 ( sub ) col is computed from the time series of that single experiment . \"\"\" import pandas as pd def align_subcols ( rows , cc , subcols , h2 ) : \"\"\"Subcolumns: align, justify, join.\"\"\" # Define subcol formats subc = dict () subc [ ' keys ' ] = [ \"val\" , \"conf\" ] subc [ ' headers ' ] = [ statkey , ' 1 \u03c3' ] subc [ ' frmts ' ] = [ None , None ] subc [ ' spaces ' ] = [ ' \u00b1' , ] # last one gets appended below . subc [ ' aligns ' ] = [ '>' , '<' ] # 4 header -- matter gets decimal - aligned . if axes [ ' optim ' ] is not None : subc [ ' keys ' ] += [ \"tuned_coord\" ] subc [ ' headers ' ] += [ axes [ ' optim ' ]] subc [ ' frmts ' ] += [ lambda x : tuple ( a for a in x )] subc [ ' spaces ' ] += [ ' * ' ] subc [ ' aligns ' ] += [ '<' ] elif axes [ ' mean ' ] is not None : subc [ ' keys ' ] += [ \"nFail\" , \"nSuccess\" ] subc [ ' headers ' ] += [ '\u2620' , '\u2713' ] # use width -1 symbols ! subc [ ' frmts ' ] += [ None , None ] subc [ ' spaces ' ] += [ ' ' , ' ' ] subc [ ' aligns ' ] += [ '>' , '>' ] subc [ ' spaces ' ]. append ( '' ) # no space after last subcol template = ' {} ' + ' {} ' . join ( subc [ ' spaces ' ]) # Transpose columns = [ list ( x ) for x in zip ( * rows )] # Iterate over columns. for j , ( col_coord , column ) in enumerate ( zip ( cc , columns )) : # Tabulate columns if subcols : column = unpack_uqs ( column , decimals , subc [ \"keys\" ]) # Tabulate subcolumns subheaders = [] for key , header , frmt , _ , align in zip ( * subc . values ()) : column [ key ] = tabulate_column ( column [ key ], header , frmt = frmt )[ 1 : ] L = len ( column [ -1 ][ key ]) if align == '<' : subheaders += [ str ( header ). ljust ( L )] else : subheaders += [ str ( header ). rjust ( L )] # Join subcolumns: matter = [ template . format ( * [ row [ k ] for k in subc [ ' keys ' ]]) for row in column ] header = template . format ( * subheaders ) else : column = unpack_uqs ( column , decimals )[ \"val\" ] column = tabulate_column ( column , statkey ) header , matter = column [ 0 ], column [ 1 : ] if h2 : # Do super_header if j : super_header = str ( col_coord ) else : super_header = repr ( col_coord ) width = len ( header ) # += 1 if using unicode chars like \u2714\ufe0f super_header = super_header . center ( width , \"_\" ) header = super_header + \" \\n \" + header columns [ j ] = [ header ] + matter # Un-transpose rows = [ list ( x ) for x in zip ( * columns )] return rows # Inform axes[\"mean\"] if axes . get ( ' mean ' , None ) : print ( f \"Averages (in time and) over {axes['mean']}.\" ) else : print ( \"Averages in time only\" \" (=> the 1\u03c3 estimates may be unreliable).\" ) axes , tables = self . table_tree ( statkey , axes ) for table_coord , table in tables . items () : # Get this table's column coords (cc). Use dict for sorted&unique. # cc = xp_dict.ticks[axes[\"inner\"]] # May be larger than needed. # cc = table[0].keys() # May be too small a set. cc = { c : None for row in table . values () for c in row } # Convert table (rows) into rows (lists) of equal length rows = [[ row . get ( c , None ) for c in cc ] for row in table . values ()] if False : # ****************** Simple ( for debugging ) table for i , ( row_coord , row ) in enumerate ( zip ( table , rows )) : row_key = \", \" . join ( str ( v ) for v in row_coord ) rows [ i ] = [ row_key ] + row rows . insert ( 0 , [ f \"{table.axes}\" ] + [ repr ( c ) for c in cc ]) else : # ********************** Elegant table . h2 = \" \\n \" if len ( cc ) > 1 else \"\" # do column - super - header rows = align_subcols ( rows , cc , subcols , h2 ) # Make and prepend left-side table # - It's prettier if row_keys don't have unnecessary cols. # For example, the table of Climatology should not have an # entire column repeatedly displaying \"infl=None\". # => split_attrs(). # - Why didn't we do this for the column attrs? # Coz there we have no ambition to split the attrs, # which would also require excessive processing: # nesting the table as cols, and then split_attrs() on cols. row_keys = xpList ( table . keys ()). split_attrs ()[ 0 ] row_keys = pd . DataFrame . from_dict ( row_keys , dtype = \"O\" ) # allow storing None if len ( row_keys . columns ) : # Header rows [ 0 ] = [ h2 + k for k in row_keys ] + [ h2 + '\u244a' ] + rows [ 0 ] # Matter for row , ( i , key ) in zip ( rows [ 1 : ], row_keys . iterrows ()) : rows [ i + 1 ] = [ * key ] + [ '|' ] + row # Print print ( \" \\n \" , end = \"\" ) if axes [ ' outer ' ] : table_title = \"Table for \" + repr ( table_coord ) print ( color_text ( table_title , colorama . Back . YELLOW )) headers , * rows = rows print ( utils . tab ( rows , headers ). replace ( '\u2423' , ' ' ))","title":"print"},{"location":"reference/dapper/data_management/#setdefault_1","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/dapper/data_management/#table_tree","text":"def table_tree ( self , statkey , axes ) Hierarchical nest(): xp_dict>outer>inner>mean>optim. as specified by axes . Returns this new xpSpace. print_1d / plot_1d (respectively) separate tables / panel(row)s for axes['outer'] , and columns/ x-axis for axes['inner'] . The axes['mean'] and axes['optim'] get eliminated by the mean()/tune() operations. Note: cannot support multiple statkeys because it's not (obviously) meaningful when optimizing over tuning_axes. View Source def table_tree ( self , statkey , axes ): \"\"\"Hierarchical nest(): xp_dict>outer>inner>mean>optim. as specified by ``axes``. Returns this new xpSpace. - print_1d / plot_1d (respectively) separate tables / panel(row)s for ``axes['outer']``, and columns/ x-axis for ``axes['inner']``. - The ``axes['mean']`` and ``axes['optim']`` get eliminated by the mean()/tune() operations. Note: cannot support multiple statkeys because it's not (obviously) meaningful when optimizing over tuning_axes. \"\"\" axes = self . validate_axes ( axes ) def mean_tune ( xp_dict ): \"\"\"Take mean, then tune. Note: the SparseDict implementation should be sufficiently \"uncluttered\" that mean_tune() (or a few of its code lines) could be called anywhere above/between/below the ``nest()``ing of ``outer`` or ``inner``. These possibile call locations are commented in the code.\"\"\" uq_dict = xp_dict . field ( statkey ) uq_dict = uq_dict . mean ( axes [ 'mean' ]) uq_dict = uq_dict . tune ( axes [ 'optim' ]) return uq_dict self = mean_tune ( self ) # Prefer calling mean_tune() [also see its docstring] # before doing outer/inner nesting. This is because then the axes of # a row (xpSpace) should not include mean&optim, and thus: # - Column header/coords may be had directly as row.keys(), # without extraction by __getkey__() from (e.g.) row[0]. # - Don't need to propagate mean&optim axes down to the row level. # which would require defining rows by the nesting: # rows = table.nest(outer_axes=dict_tools.complement(table.axes, # *(axes['inner'] or ()), # *(axes['mean'] or ()), # *(axes['optim'] or ()) )) # - Each level of the output from table_tree # is a smaller (and more manageable) dict. tables = self . nest ( outer_axes = axes [ 'outer' ]) for table_coord , table in tables . items (): # table = mean_tune(table) # Should not be used (nesting as rows is more natural, # and is required for getting distinct/row_keys). # cols = table.nest(outer_axes=axes['inner']) rows = table . nest ( inner_axes = axes [ 'inner' ] or ()) # Overwrite table by its nesting as rows tables [ table_coord ] = rows # for row_coord, row in rows.items(): # rows[row_coord] = mean_tune(row) return axes , tables","title":"table_tree"},{"location":"reference/dapper/data_management/#tickz","text":"def tickz ( self , axis_name ) Axis ticks without None View Source def tickz ( self , axis_name ) : \"\"\"Axis ticks without None\"\"\" return [ x for x in self.ticks[axis_name ] if x is not None ]","title":"tickz"},{"location":"reference/dapper/data_management/#tune","text":"def tune ( self , axes = None , costfun = None ) Get (compile/tabulate) a stat field optimised wrt. tuning params. View Source def tune ( self , axes = None , costfun = None ) : \"\"\"Get (compile/tabulate) a stat field optimised wrt. tuning params.\"\"\" # Define cost - function costfun = ( costfun or 'increasing' ). lower () if 'increas' in costfun : costfun = ( lambda x : + x ) elif 'decreas' in costfun : costfun = ( lambda x : - x ) else : assert hasattr ( costfun , '__call__' ) # custom # Note : The case `` axes = () `` should work w / o special treatment . if axes is None : return self nested = self . nest ( axes ) for coord , space in nested . items () : # Find optimal value and coord within space MIN = np . inf for i , ( inner_coord , uq ) in enumerate ( space . items ()) : cost = costfun ( uq . val ) if cost <= MIN : MIN = cost uq_opt = uq uq_opt . tuned_coord = inner_coord nested [ coord ] = uq_opt return nested","title":"tune"},{"location":"reference/dapper/data_management/#update_1","text":"def update ( self , * args , ** kwargs ) Update using custom setitem (). View Source def update ( self , * args , ** kwargs ) : \"\"\"Update using custom __setitem__().\"\"\" # See https : // stackoverflow . com / a / 2588648 # and https : // stackoverflow . com / a / 2390997 for k , v in dict ( * args , ** kwargs ). items () : self [ k ] = v","title":"update"},{"location":"reference/dapper/data_management/#validate_axes","text":"def validate_axes ( self , axes ) Validate axes. Note: This does not convert None to (), allowing None to remain special. Use axis or () wherever tuples are required. View Source def validate_axes ( self , axes ) : \"\"\"Validate axes. Note: This does not convert None to (), allowing None to remain special. Use ``axis or ()`` wherever tuples are required. \"\"\" roles = {} # \"inv\" for role in set ( axes ) | set ( AXES_ROLES ) : assert role in AXES_ROLES , f \"Invalid role {role!r}\" aa = axes . get ( role , AXES_ROLES [ role ] ) if aa is None : pass # Purposely special else : # Ensure iterable if isinstance ( aa , str ) or not hasattr ( aa , \"__iter__\" ) : aa = ( aa ,) aa = self . intersect_axes ( aa ) for axis in aa : # Ensure unique if axis in roles : raise TypeError ( f \"An axis (here {axis!r}) cannot be assigned to 2\" f \" roles (here {role!r} and {roles[axis]!r}).\" ) else : roles [ axis ] = role axes [ role ] = aa return axes","title":"validate_axes"},{"location":"reference/dapper/data_management/#values_1","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/dapper/dict_tools/","text":"Module dapper.dict_tools Tools for working with dicts. Bonus: also some list tools. View Source \"\"\"Tools for working with dicts. Bonus: also some list tools.\"\"\" # TODO 9 # - Make pip package, answer SO.com questions about # https://www.google.com/search?q=python+aligned+dict&oq=python+aligned+dict import itertools import shutil import sys # Since textwrap() only treats strings, while pprint is python-aware, # I would have preferred pprint.pformat(width=lw,compact=True,sort_dicts=False), # but pprint refuses to use my repr. # It also could mess up my recursion guard, which inserts \"<Recursion...>\", # because it tests for x.startswith(\"<\"). import textwrap from _thread import get_ident from contextlib import contextmanager import numpy as np def get0 ( dct ): \"\"\"Get first value.\"\"\" # Bad?: dct[list(dct.keys())[0]] return next ( iter ( dct . values ())) # TODO 4: rm? def flexcomp ( x , * criteria ): \"\"\"Compare in various ways.\"\"\" def _compare ( x , y ): # Callable (condition) compare try : return y ( x ) except TypeError : pass # Regex compare -- should be compiled on the outside try : return bool ( y . search ( x )) except AttributeError : # Value compare return y == x return any ( _compare ( x , y ) for y in criteria ) def _intersect ( iterable , criteria , inv = False ): \"\"\"Keep elements of ``iterable`` that match **any** criteria, as evaluated by flexcomp(). Returns dict/list if ``iterable`` is dict/iterable.\"\"\" def negate ( x ): return ( not x ) if inv else x keys = [ k for k in iterable if negate ( flexcomp ( k , * criteria ))] if isinstance ( iterable , dict ): # Dict return { k : iterable [ k ] for k in keys } return keys # For some reason, _intersect (with the `inv` switch) is # for the brain to parse. Use intersect and complement instead. def intersect ( iterable , wanteds ): return _intersect ( iterable , wanteds , inv = False ) def complement ( iterable , unwanteds ): return _intersect ( iterable , unwanteds , inv = True ) # Complement should be called \"relative complement\" (or set diff) def prodct ( dct ): \"\"\"Cartesian/Outer product, for dicts. Example: >>> list(prodct(dict(n=[1,2], c='ab'))) [{'n': 1, 'c': 'a'}, {'n': 1, 'c': 'b'}, {'n': 2, 'c': 'a'}, {'n': 2, 'c': 'b'}] Source: https://stackoverflow.com/a/40623158/38281.\"\"\" return ( dict ( zip ( dct , x )) for x in itertools . product ( * dct . values ())) def transpose_dicts ( DD , safe = True ): \"\"\"As the name says. Incredibly, it is difficult to make this less verbose (the one-liner is unpredicable for non-rectangular cases) Example: >>> DD = {chr(97+i): {chr(65+j):i*10+j for j in range(2)} for i in range(3)} >>> DD {'a': {'A': 0, 'B': 1}, 'b': {'A': 10, 'B': 11}, 'c': {'A': 20, 'B': 21}} >>> transpose_dicts(DD) {'A': {'a': 0, 'b': 10, 'c': 20}, 'B': {'a': 1, 'b': 11, 'c': 21}} \"\"\" new = {} prev = \"UNINITIALIZED\" for i in DD : # Validate column keys if safe and prev != \"UNINITIALIZED\" : assert prev == DD [ i ] . keys (), f \"Key mismatch for row { i } \" prev = DD [ i ] . keys () for j in DD [ i ]: new . setdefault ( j , {})[ i ] = DD [ i ][ j ] return new def transps ( thing2d , * args , ** kwargs ): \"\"\"Delegates transpose operation to the appropriate function.\"\"\" # Is dict? Otherwise assume list (tuple also works). dict1 = isinstance ( thing2d , dict ) item0 = get0 ( thing2d ) if dict1 else thing2d [ 0 ] # fine since py3.7 dict2 = isinstance ( item0 , dict ) if dict1 and dict2 : f = transpose_dicts # noqa elif dict1 and not dict2 : f = transpose_dict_of_lists # noqa elif not dict1 and dict2 : f = transpose_list_of_dicts # noqa elif not dict1 and not dict2 : f = transpose_lists # noqa return f ( thing2d , * args , ** kwargs ) def transpose_list_of_dicts ( LD , safe = True ): \"\"\" Example: >>> LD = [{chr(97+j):j for j in range(2)} for i in range(3)] >>> LD [{'a': 0, 'b': 1}, {'a': 0, 'b': 1}, {'a': 0, 'b': 1}] >>> transpose_list_of_dicts(LD) {'a': [0, 0, 0], 'b': [1, 1, 1]} \"\"\" new = {} prev = \"UNINITIALIZED\" for i , D in enumerate ( LD ): # Validate column keys if safe and prev != \"UNINITIALIZED\" : assert prev == D . keys (), f \"Key mismatch for dict number { i } \" prev = D . keys () for j in D : new . setdefault ( j , []) . append ( D [ j ]) return new def transpose_dict_of_lists ( DL , safe = True ): \"\"\"This is essentially a one-liner. However, the validation of `safe=True` requires some effort. Example: >>> DL = {chr(97+i): [i*10+j for j in range(2)] for i in range(3)} >>> DL {'a': [0, 1], 'b': [10, 11], 'c': [20, 21]} >>> transpose_dict_of_lists(DL) [{'a': 0, 'b': 10, 'c': 20}, {'a': 1, 'b': 11, 'c': 21}] \"\"\" # Validate row length if safe : lens = [ len ( DL [ k ]) for k in DL ] assert all ( lens [ 0 ] == L for L in lens ), \"Rows have unqual lengths.\" # https://stackoverflow.com/q/5558418 new = [ dict ( zip ( DL , t )) for t in zip ( * DL . values ())] return new def transpose_lists ( LL , safe = True , as_list = False ): \"\"\"This is essentially `zip(*LL)`. However, the validation of `safe=True` requires some effort, and converting to an explicit list requires another bit. Example: >>> LL = [[i*10 + j for j in range(2)] for i in range(3)] >>> LL [[0, 1], [10, 11], [20, 21]] >>> transpose_lists(LL,as_list=True) [[0, 10, 20], [1, 11, 21]] \"\"\" if safe : assert all ( len ( LL [ 0 ]) == len ( row ) for row in LL ) new = zip ( * LL ) # transpose if as_list : # new = list(map(list, new)) new = [ list ( row ) for row in new ] return new # from https://pingfive.typepad.com/blog/2010/04/ def deep_getattr ( obj , name , * default ): for n in name . split ( \".\" ): obj = getattr ( obj , n , * default ) return obj # Rm'd coz it's what type of obj to set at intermediate hierarchy levels: # deep_setattr() def deep_hasattr ( obj , name ): try : deep_getattr ( obj , name ) return True except AttributeError : return False MIN_LINEWIDTH = 5 # TODO 4: should also be made thread-safe? Ref _is_being_printed. _linewidth = None _top_dog = None _is_being_printed = set () @contextmanager def _shorten_linewidth_by ( n ): \"\"\"Shorten linewidth parameter. Also works on numpy.\"\"\" # Avoid (overhead of) importing numpy if not required. # https://stackoverflow.com/a/30483269/38281 if \"numpy\" in sys . modules : def np_lw ( lw ): return np . set_printoptions ( linewidth = lw ) else : # Provide pass-through def np_lw ( lw ): return lw # Structured (store, try, finally) just like np.printoptions(). global _linewidth old = _linewidth try : # Set lw _linewidth -= n _linewidth = max ( _linewidth , MIN_LINEWIDTH ) np_lw ( _linewidth ) yield _linewidth finally : # Restore lw _linewidth = old np_lw ( old ) def _init ( user_function ): \"\"\"Does two things: - Monitor recursion of self. - This is done is just like in ``reprlib`` whence I learned how to make it thread-safe. One diff. is that here _repr_running is global, which works better for this convoluted example: >>> d1 = AlignedDict(a=1) >>> d1[\"d2\"] = AlignedDict(self=d1) >>> d1[\"lst\"] = [d1[\"d2\"]] - Sibling repetition is not recursion. - Initializes the linewidth with ``get_terminal_size()``, unless the ``_linewidth`` variable has been set. Note: Both of these actions also require clean-up, so there's a ``finally`` section as well. Note: Cannot apply to core printing function (AlignedDict._repr), because each call to AlignedDict creates new (unregistered) id's.\"\"\" def wrapped ( self ): # Set linewidth global _top_dog , _linewidth old_linewidth = _linewidth if _top_dog is None : _top_dog = id ( self ) if _linewidth is None : _linewidth , _ = shutil . get_terminal_size () # Turn ON recursion guard id_thread = id ( self ), get_ident () if id_thread in _is_being_printed : return \"<Recurs- %d >\" % id ( self ) # return \"...\" _is_being_printed . add ( id_thread ) try : result = user_function ( self ) finally : # Turn OFF recursion guard _is_being_printed . discard ( id_thread ) # Restore linewidth if id ( self ) == _top_dog : _top_dog = None _linewidth = old_linewidth return result return wrapped class AlignedDict ( dict ): \"\"\"Dict whose items are printed aligned and line-wrapped. Initialized and used just like a regular dict, since only difference to ``dict`` is __str__ and __repr__ (of which the other functions are auxiliaries and hidden). Also provides printopts. Example: >>> a = np.arange(24) >>> A = a.reshape((4,-1)) >>> dct = AlignedDict(a=a, b=99, z=3, A=A) >>> dct[\"self\"] = dct >>> dct[\"sub\"] = AlignedDict(x=1,text=\"lorem\",self=dct) >>> dct.printopts = dict(excluded=[\"z\"], aliases={\"A\":\"matrix\"}) >>> print(repr(dct)) Note on ``printopts[\"indent\"]``: - If not set, then the keys are right-aligned (so that the colons line up), and the values (including multi-line) are printed to their right, allowing them to be placed on the same line as the key. This setting is pretty, unless the keys are really long. - If set (to an integer), then the values are printed on a new line, indented by that (unchanging) length (unless they're one-liners). A similar class is scipy.optimize.OptimizeResult See also: - pprint.pformat(x,linewidth,compact=True,sort_dicts=False) - json.dumps(x, indent=4, sort_keys=False, default=repr) But, both of these work recursively, which is NOT our aim. In particular, they need to implement behaviour for all types. \"\"\" @_init def __str__ ( self ): return self . _repr ( is_repr = False ) @_init def __repr__ ( self ): return self . _repr ( is_repr = True ) def _repr ( self , is_repr = False ): \"\"\"Both __str__ and __repr__ in one! Note: if ``self`` is printed as part of a standard object, __repr__ gets called, not __str__.\"\"\" # Constants sep = \": \" bullet = \" \" if is_repr else \" - \" comma = \",\" if is_repr else \"\" spinechar = \" \" if is_repr else \"\u00a6\" # \u00a6, \u2506, \u2502, \u23b8, \u258f format_key = repr if is_repr else str format_val = repr if is_repr else str # Apply print options dct = self opt = getattr ( self , \"printopts\" , {}) dct = intersect ( dct , opt . get ( \"included\" , dct )) dct = complement ( dct , opt . get ( \"excluded\" , [])) dct = { opt . get ( \"aliases\" , {}) . get ( k , k ): v for k , v in dct . items ()} if \"indent\" in opt : # LEFT-aligned keys -- indent is fixed, and val is always on newline val_indent = \" \" * len ( bullet ) + spinechar . ljust ( opt [ \"indent\" ]) else : # RIGHT-aligned keys -- indent ~ on keys, and val can start on key's line kWidth = max ([ len ( format_key ( k )) for k in dct ], default = 0 ) val_indent = \" \" * len ( bullet ) + \" \" * kWidth + spinechar . ljust ( len ( sep )) def format_key ( key , old = format_key ): return old ( key ) . rjust ( kWidth ) def iRepr ( key , val ): trim = len ( val_indent + comma ) # Try placing on 1 line, but use max() to insure against wrapping if \"indent\" in opt and ( \" \\n \" not in format_val ( val )): trim = max ( trim , len ( bullet + format_key ( key ) + sep + comma )) with _shorten_linewidth_by ( trim ): val = self . _wrap_item ( format_val ( val )) val = ( \" \\n \" + val_indent ) . join ( val ) if \"indent\" in opt and ( \" \\n \" in val ): val = \" \\n \" + val_indent + val # 1st line also on new line return format_key ( key ) + sep + val # dct --> text items = [ iRepr ( k , v ) for k , v in dct . items ()] # Sort items = self . _sort ( zip ( dct , items )) # Join txt = ( comma + \" \\n \" + bullet ) . join ( items ) if len ( items ) > 1 : txt = \" \\n \" + bullet + txt + \" \\n \" # Add braces, etc if is_repr : txt = \"{\" + txt + \"}\" return txt # AUX FUNCTIONS @staticmethod def _wrap_item ( txt ): \"\"\"Wrap txt.\"\"\" lines = txt . splitlines () ll = [ textwrap . wrap ( line , _linewidth ) for line in lines ] # list of lists hang = \" \" # No need to shorten_lw for this ll = [ w [: 1 ] + [ hang + line for line in w [ 1 :]] for w in ll ] ll = [ k for w in ll for k in w ] # flatten list-of-lists return ll def _sort ( self , dict_and_texts ): ORDR = getattr ( self , \"printopts\" , {}) . get ( \"ordering\" , []) or [] reverse = getattr ( self , \"printopts\" , {}) . get ( \"reverse\" , False ) def indexer ( key_text ): key , text = key_text if not isinstance ( ORDR , str ): # assume list # Manual ordering. if key in ORDR : # -10000 => priority idx = - 10000 + ORDR . index ( key ) else : idx = 0 # neutral elif \"line\" in ORDR : # Line-number count idx = 100 * text . count ( \" \\n \" ) # Line-length count idx += max ( len ( line ) for line in text . splitlines ()) elif \"alpha\" in ORDR : # Alphabetic (tuple compare) idx = key . lower () return idx pairs = sorted ( dict_and_texts , key = indexer , reverse = reverse ) return [ text for key , text in pairs ] class NicePrint : \"\"\"Provides __repr__ and __str__ by AlignedDict(vars(self)). Example usage: >>> class MyClass(NicePrint): >>> printopts = NicePrint.printopts.copy() >>> printopts[\"excluded\"] += [\"my_hidden_var\"] >>> printopts[\"aliases\"] = {\"asdf\":\"better_name\"} >>> ... \"\"\" # _underscored = re.compile('^_') _underscored = lambda s : s . startswith ( \"_\" ) # noqa printopts = dict ( excluded = [ _underscored , \"printopts\" ]) def _repr ( self , is_repr = True ): cls_name = type ( self ) . __name__ + \"(\" if is_repr else \"\" dct = AlignedDict ( vars ( self )) dct . printopts = self . printopts with _shorten_linewidth_by ( len ( cls_name )): txt = repr ( dct ) if is_repr else str ( dct ) txt = ( \" \\n \" + \" \" * len ( cls_name )) . join ( txt . splitlines ()) return cls_name + txt + ( \")\" if is_repr else \"\" ) @_init def __str__ ( self ): return self . _repr ( is_repr = False ) @_init def __repr__ ( self ): return self . _repr ( is_repr = True ) class DotDict ( AlignedDict ): \"\"\"Dict that *also* supports attribute (dot) access. Benefit compared to a dict: - Verbosity of ``d['a']`` vs. ``d.a``. - Includes ``AlignedDict``. DotDict is not terribly hackey, and is quite robust. Similar constructs are quite common, eg IPython/utils/ipstruct.py. Main inspiration: https://stackoverflow.com/a/14620633 \"\"\" printopts = dict ( excluded = [ \"printopts\" ]) def __init__ ( self , * args , ** kwargs ): \"\"\"Init like a normal dict.\"\"\" super () . __init__ ( * args , ** kwargs ) # Make a (normal) dict self . __dict__ = self # Assign it to self.__dict__ # def print_nested(dct): # \"\"\"Print nested dicts\"\"\" # # # Note: if a dict is inside of a list (for example), # # it will be printed in the standard fashion, # # despite our overriding the builtins.repr. # # There appears to be no way around this, except, # # like reprlib does, defining custom treatment for each # # type (dicts, list, sets, tuples, deques, etc). # # # Another issue is that I dont know how to check if an object # # is suitable for AlignedDict printing or not, # # so this function is restricted to dicts. # # @_init # def new_repr(obj): # if hasattr(obj,\"items\"): # obj = AlignedDict(obj) # return orig_repr(obj) # # import builtins # orig_repr = builtins.repr # try: # builtins.repr = new_repr # txt = repr(dct) # finally: # builtins.repr = orig_repr # # print(txt) if __name__ == \"__main__\" : # Note: this setup is purposely messy, # in order to test recursion treatments. a = np . arange ( 24 ) A = a . reshape (( 4 , - 1 )) d1 = dict ( a = a , b = 99 , z = 3 , A = A ) d2 = dict ( x = 1 , lorem = \"ipsum\" ) # Dont move this block below (coz then it will contain # d1/d2 recursions, rather than a1/a2) a1 = AlignedDict ( d1 ) a2 = AlignedDict ( d2 ) # Json -- cannot handle recursions # import json d1 [ \"d2\" ] = d2 # print(\"\\njson.dumps:\\n================\") # print(json.dumps(d1, indent=4, default=repr)) # pprint # import pprint d1 [ \"d2\" ] = d2 d2 [ \"d1\" ] = d1 d1 [ \"lst\" ] = [ 0 , 1 , d2 ] # print(\"\\npprint:\\n================\") # pprint.pprint(d1,compact=True) # Regular dict/print # print(\"\\nRegular dict/print:\\n================\") # print(d1) # Add recursions similar to d1/d2 # print(\"\\nAlignedDict:\\n================\") a2 [ \"a1\" ] = a1 a1 [ \"a2\" ] = a2 a1 [ \"one\" ] = AlignedDict ( item = \"hello\" ) a1 [ \"empty\" ] = AlignedDict () a1 [ \"really long name that goes on and on\" ] = [ 0 , 1 , a2 ] a1 . printopts = dict ( excluded = [ \"z\" ], aliases = { \"A\" : \"aMatrix\" }, ordering = \"line\" , # or alpha or [\"a2\", \"self\"] # reverse=True, ) print ( \" \\n str: \\n ================\" ) print ( a1 ) print ( \" \\n repr: \\n ================\" ) print ( repr ( a1 )) print ( \" \\n ================ \\n with const. indent: \\n ================\" ) a1 . printopts [ \"indent\" ] = 1 a2 . printopts = dict ( indent = 1 ) print ( \" \\n str: \\n ================\" ) print ( a1 ) print ( \" \\n repr: \\n ================\" ) print ( repr ( a1 )) print ( \" \\n NicePrint: \\n ================\" ) class MyClass ( NicePrint ): printopts = NicePrint . printopts . copy () def __init__ ( self ): self . _a = 99 self . a = np . arange ( 24 ) # self.a = 1 # self.lorem = \"ipsum\" # self.lst = np.arange(20) obj1 = MyClass () # obj1.obj2 = MyClass() # obj1.self = obj1 obj1 . printopts [ \"excluded\" ] += [ \"lst\" ] print ( repr ( obj1 )) print ( obj1 ) print ( \" \\n DotDict: \\n ================\" ) dd = DotDict ( a = a , b = 99 , z = 3 , A = A ) dd . dd2 = DotDict ( a = a , b = 99 , z = 3 , A = A ) dd . self = dd dd . printopts [ \"excluded\" ] += [ \"A\" ] print ( repr ( dd )) print ( dd ) # Other tests print ( \" \\n deep_getattr: \\n ================\" ) key2 = \"self.self.self.self.a\" print ( key2 , \":\" , deep_getattr ( dd , key2 )) Variables MIN_LINEWIDTH Functions complement def complement ( iterable , unwanteds ) View Source def complement ( iterable , unwanteds ): return _intersect ( iterable , unwanteds , inv = True ) deep_getattr def deep_getattr ( obj , name , * default ) View Source def deep_getattr ( obj , name , * default ): for n in name . split ( \".\" ): obj = getattr ( obj , n , * default ) return obj deep_hasattr def deep_hasattr ( obj , name ) View Source def deep_hasattr ( obj , name ): try : deep_getattr ( obj , name ) return True except AttributeError : return False flexcomp def flexcomp ( x , * criteria ) Compare in various ways. View Source def flexcomp ( x , * criteria ): \"\"\"Compare in various ways.\"\"\" def _compare ( x , y ): # Callable (condition) compare try : return y ( x ) except TypeError : pass # Regex compare -- should be compiled on the outside try : return bool ( y . search ( x )) except AttributeError : # Value compare return y == x return any ( _compare ( x , y ) for y in criteria ) get0 def get0 ( dct ) Get first value. View Source def get0 ( dct ): \"\"\"Get first value.\"\"\" # Bad ? : dct [ list ( dct . keys ())[ 0 ]] return next ( iter ( dct . values ())) intersect def intersect ( iterable , wanteds ) View Source def intersect ( iterable , wanteds ): return _intersect ( iterable , wanteds , inv = False ) prodct def prodct ( dct ) Cartesian/Outer product, for dicts. Example: list(prodct(dict(n=[1,2], c='ab'))) [{'n': 1, 'c': 'a'}, {'n': 1, 'c': 'b'}, {'n': 2, 'c': 'a'}, {'n': 2, 'c': 'b'}] Source: https://stackoverflow.com/a/40623158/38281. View Source def prodct ( dct ): \"\"\"Cartesian/Outer product, for dicts. Example: >>> list(prodct(dict(n=[1,2], c='ab'))) [{'n': 1, 'c': 'a'}, {'n': 1, 'c': 'b'}, {'n': 2, 'c': 'a'}, {'n': 2, 'c': 'b'}] Source: https://stackoverflow.com/a/40623158/38281.\"\"\" return ( dict ( zip ( dct , x )) for x in itertools . product ( * dct . values ())) transpose_dict_of_lists def transpose_dict_of_lists ( DL , safe = True ) This is essentially a one-liner. However, the validation of safe=True requires some effort. Example: DL = {chr(97+i): [i*10+j for j in range(2)] for i in range(3)} DL {'a': [0, 1], 'b': [10, 11], 'c': [20, 21]} transpose_dict_of_lists(DL) [{'a': 0, 'b': 10, 'c': 20}, {'a': 1, 'b': 11, 'c': 21}] View Source def transpose_dict_of_lists ( DL , safe = True ) : \"\"\"This is essentially a one-liner. However, the validation of `safe=True` requires some effort. Example: >>> DL = {chr(97+i): [i*10+j for j in range(2)] for i in range(3)} >>> DL {'a': [0, 1], 'b': [10, 11], 'c': [20, 21]} >>> transpose_dict_of_lists(DL) [{'a': 0, 'b': 10, 'c': 20}, {'a': 1, 'b': 11, 'c': 21}] \"\"\" # Validate row length if safe : lens = [ len(DL[k ] ) for k in DL ] assert all ( lens [ 0 ] == L for L in lens ), \"Rows have unqual lengths.\" # https : // stackoverflow . com / q / 5558418 new = [ dict(zip(DL, t)) for t in zip(*DL.values()) ] return new transpose_dicts def transpose_dicts ( DD , safe = True ) As the name says. Incredibly, it is difficult to make this less verbose (the one-liner is unpredicable for non-rectangular cases) Example: DD = {chr(97+i): {chr(65+j):i*10+j for j in range(2)} for i in range(3)} DD {'a': {'A': 0, 'B': 1}, 'b': {'A': 10, 'B': 11}, 'c': {'A': 20, 'B': 21}} transpose_dicts(DD) {'A': {'a': 0, 'b': 10, 'c': 20}, 'B': {'a': 1, 'b': 11, 'c': 21}} View Source def transpose_dicts ( DD , safe = True ) : \"\"\"As the name says. Incredibly, it is difficult to make this less verbose (the one-liner is unpredicable for non-rectangular cases) Example: >>> DD = {chr(97+i): {chr(65+j):i*10+j for j in range(2)} for i in range(3)} >>> DD {'a': {'A': 0, 'B': 1}, 'b': {'A': 10, 'B': 11}, 'c': {'A': 20, 'B': 21}} >>> transpose_dicts(DD) {'A': {'a': 0, 'b': 10, 'c': 20}, 'B': {'a': 1, 'b': 11, 'c': 21}} \"\"\" new = {} prev = \"UNINITIALIZED\" for i in DD : # Validate column keys if safe and prev != \"UNINITIALIZED\" : assert prev == DD [ i ] . keys (), f \"Key mismatch for row {i}\" prev = DD [ i ] . keys () for j in DD [ i ] : new . setdefault ( j , {} ) [ i ] = DD [ i ][ j ] return new transpose_list_of_dicts def transpose_list_of_dicts ( LD , safe = True ) Example: LD = [{chr(97+j):j for j in range(2)} for i in range(3)] LD [{'a': 0, 'b': 1}, {'a': 0, 'b': 1}, {'a': 0, 'b': 1}] transpose_list_of_dicts(LD) View Source def transpose_list_of_dicts ( LD , safe = True ) : \"\"\" Example: >>> LD = [{chr(97+j):j for j in range(2)} for i in range(3)] >>> LD [{'a': 0, 'b': 1}, {'a': 0, 'b': 1}, {'a': 0, 'b': 1}] >>> transpose_list_of_dicts(LD) {'a': [0, 0, 0], 'b': [1, 1, 1]} \"\"\" new = {} prev = \"UNINITIALIZED\" for i , D in enumerate ( LD ) : # Validate column keys if safe and prev != \"UNINITIALIZED\" : assert prev == D . keys (), f \"Key mismatch for dict number {i}\" prev = D . keys () for j in D : new . setdefault ( j , [] ). append ( D [ j ] ) return new transpose_lists def transpose_lists ( LL , safe = True , as_list = False ) This is essentially zip(*LL) . However, the validation of safe=True requires some effort, and converting to an explicit list requires another bit. Example: LL = [[i*10 + j for j in range(2)] for i in range(3)] LL [[0, 1], [10, 11], [20, 21]] transpose_lists(LL,as_list=True) [[0, 10, 20], [1, 11, 21]] View Source def transpose_lists ( LL , safe = True , as_list = False ): \"\"\"This is essentially `zip(*LL)`. However, the validation of `safe=True` requires some effort, and converting to an explicit list requires another bit. Example: >>> LL = [[i*10 + j for j in range(2)] for i in range(3)] >>> LL [[0, 1], [10, 11], [20, 21]] >>> transpose_lists(LL,as_list=True) [[0, 10, 20], [1, 11, 21]] \"\"\" if safe : assert all ( len ( LL [ 0 ]) == len ( row ) for row in LL ) new = zip ( * LL ) # transpose if as_list : # new = list ( map ( list , new )) new = [ list ( row ) for row in new ] return new transps def transps ( thing2d , * args , ** kwargs ) Delegates transpose operation to the appropriate function. View Source def transps ( thing2d , * args , ** kwargs ): \"\"\"Delegates transpose operation to the appropriate function.\"\"\" # Is dict ? Otherwise assume list ( tuple also works ). dict1 = isinstance ( thing2d , dict ) item0 = get0 ( thing2d ) if dict1 else thing2d [ 0 ] # fine since py3 . 7 dict2 = isinstance ( item0 , dict ) if dict1 and dict2 : f = transpose_dicts # noqa elif dict1 and not dict2 : f = transpose_dict_of_lists # noqa elif not dict1 and dict2 : f = transpose_list_of_dicts # noqa elif not dict1 and not dict2 : f = transpose_lists # noqa return f ( thing2d , * args , ** kwargs ) Classes AlignedDict class AlignedDict ( / , * args , ** kwargs ) Dict whose items are printed aligned and line-wrapped. Initialized and used just like a regular dict, since only difference to dict is str and repr (of which the other functions are auxiliaries and hidden). Also provides printopts. Example: a = np.arange(24) A = a.reshape((4,-1)) dct = AlignedDict(a=a, b=99, z=3, A=A) dct[\"self\"] = dct dct[\"sub\"] = AlignedDict(x=1,text=\"lorem\",self=dct) dct.printopts = dict(excluded=[\"z\"], aliases={\"A\":\"matrix\"}) print(repr(dct)) Note on printopts[\"indent\"] : - If not set, then the keys are right-aligned (so that the colons line up), and the values (including multi-line) are printed to their right, allowing them to be placed on the same line as the key. This setting is pretty, unless the keys are really long. - If set (to an integer), then the values are printed on a new line, indented by that (unchanging) length (unless they're one-liners). A similar class is scipy.optimize.OptimizeResult See also: - pprint.pformat(x,linewidth,compact=True,sort_dicts=False) - json.dumps(x, indent=4, sort_keys=False, default=repr) But, both of these work recursively, which is NOT our aim. In particular, they need to implement behaviour for all types. View Source class AlignedDict ( dict ) : \" \"\" Dict whose items are printed aligned and line-wrapped. Initialized and used just like a regular dict, since only difference to ``dict`` is __str__ and __repr__ (of which the other functions are auxiliaries and hidden). Also provides printopts. Example: >>> a = np.arange(24) >>> A = a.reshape((4,-1)) >>> dct = AlignedDict(a=a, b=99, z=3, A=A) >>> dct[\" self \"] = dct >>> dct[\" sub \"] = AlignedDict(x=1,text=\" lorem \",self=dct) >>> dct.printopts = dict(excluded=[\" z \"], aliases={\" A \":\" matrix \"}) >>> print(repr(dct)) Note on ``printopts[\" indent \"]``: - If not set, then the keys are right-aligned (so that the colons line up), and the values (including multi-line) are printed to their right, allowing them to be placed on the same line as the key. This setting is pretty, unless the keys are really long. - If set (to an integer), then the values are printed on a new line, indented by that (unchanging) length (unless they're one-liners). A similar class is scipy.optimize.OptimizeResult See also: - pprint.pformat(x,linewidth,compact=True,sort_dicts=False) - json.dumps(x, indent=4, sort_keys=False, default=repr) But, both of these work recursively, which is NOT our aim. In particular, they need to implement behaviour for all types. \"\" \" @_init def __str__ ( self ) : return self . _repr ( is_repr = False ) @_init def __repr__ ( self ) : return self . _repr ( is_repr = True ) def _repr ( self , is_repr = False ) : \" \"\" Both __str__ and __repr__ in one! Note: if ``self`` is printed as part of a standard object, __repr__ gets called, not __str__. \"\" \" # Constants sep = \": \" bullet = \" \" if is_repr else \" - \" comma = \",\" if is_repr else \"\" spinechar = \" \" if is_repr else \"\u00a6\" # \u00a6, \u2506, \u2502, \u23b8, \u258f format_key = repr if is_repr else str format_val = repr if is_repr else str # Apply print options dct = self opt = getattr ( self , \"printopts\" , {} ) dct = intersect ( dct , opt . get ( \"included\" , dct )) dct = complement ( dct , opt . get ( \"excluded\" , [] )) dct = { opt . get ( \"aliases\" , {} ). get ( k , k ) : v for k , v in dct . items () } if \"indent\" in opt : # LEFT-aligned keys -- indent is fixed, and val is always on newline val_indent = \" \" * len ( bullet ) + spinechar . ljust ( opt [ \"indent\" ] ) else : # RIGHT-aligned keys -- indent ~ on keys, and val can start on key's line kWidth = max ( [ len ( format_key ( k )) for k in dct ] , default = 0 ) val_indent = \" \" * len ( bullet ) + \" \" * kWidth + spinechar . ljust ( len ( sep )) def format_key ( key , old = format_key ) : return old ( key ). rjust ( kWidth ) def iRepr ( key , val ) : trim = len ( val_indent + comma ) # Try placing on 1 line, but use max() to insure against wrapping if \"indent\" in opt and ( \" \\n \" not in format_val ( val )) : trim = max ( trim , len ( bullet + format_key ( key ) + sep + comma )) with _shorten_linewidth_by ( trim ) : val = self . _wrap_item ( format_val ( val )) val = ( \" \\n \" + val_indent ). join ( val ) if \"indent\" in opt and ( \" \\n \" in val ) : val = \" \\n \" + val_indent + val # 1st line also on new line return format_key ( key ) + sep + val # dct --> text items = [ iRepr ( k , v ) for k , v in dct . items () ] # Sort items = self . _sort ( zip ( dct , items )) # Join txt = ( comma + \" \\n \" + bullet ). join ( items ) if len ( items ) > 1 : txt = \" \\n \" + bullet + txt + \" \\n \" # Add braces, etc if is_repr : txt = \"{\" + txt + \"}\" return txt # AUX FUNCTIONS @staticmethod def _wrap_item ( txt ) : \" \"\" Wrap txt. \"\" \" lines = txt . splitlines () ll = [ textwrap . wrap ( line , _linewidth ) for line in lines ] # list of lists hang = \" \" # No need to shorten_lw for this ll = [ w [ : 1 ] + [ hang + line for line in w [ 1 : ]] for w in ll ] ll = [ k for w in ll for k in w ] # flatten list-of-lists return ll def _sort ( self , dict_and_texts ) : ORDR = getattr ( self , \"printopts\" , {} ). get ( \"ordering\" , [] ) or [] reverse = getattr ( self , \"printopts\" , {} ). get ( \"reverse\" , False ) def indexer ( key_text ) : key , text = key_text if not isinstance ( ORDR , str ) : # assume list # Manual ordering. if key in ORDR : # -10000 => priority idx = - 10000 + ORDR . index ( key ) else : idx = 0 # neutral elif \"line\" in ORDR : # Line-number count idx = 100 * text . count ( \" \\n \" ) # Line-length count idx += max ( len ( line ) for line in text . splitlines ()) elif \"alpha\" in ORDR : # Alphabetic (tuple compare) idx = key . lower () return idx pairs = sorted ( dict_and_texts , key = indexer , reverse = reverse ) return [ text for key , text in pairs ] Ancestors (in MRO) builtins.dict Descendants dapper.dict_tools.DotDict Methods clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] values def values ( ... ) D.values() -> an object providing a view on D's values DotDict class DotDict ( * args , ** kwargs ) Dict that also supports attribute (dot) access. Benefit compared to a dict: Verbosity of d['a'] vs. d.a . Includes AlignedDict . DotDict is not terribly hackey, and is quite robust. Similar constructs are quite common, eg IPython/utils/ipstruct.py. Main inspiration: https://stackoverflow.com/a/14620633 View Source class DotDict ( AlignedDict ) : \" \"\" Dict that *also* supports attribute (dot) access. Benefit compared to a dict: - Verbosity of ``d['a']`` vs. ``d.a``. - Includes ``AlignedDict``. DotDict is not terribly hackey, and is quite robust. Similar constructs are quite common, eg IPython/utils/ipstruct.py. Main inspiration: https://stackoverflow.com/a/14620633 \"\" \" printopts = dict ( excluded = [ \"printopts\" ] ) def __init__ ( self , * args , ** kwargs ) : \" \"\" Init like a normal dict. \"\" \" super (). __init__ ( * args , ** kwargs ) # Make a (normal) dict self . __dict__ = self # Assign it to self.__dict__ Ancestors (in MRO) dapper.dict_tools.AlignedDict builtins.dict Descendants dapper.stats.Avrgs Class variables printopts Methods clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] values def values ( ... ) D.values() -> an object providing a view on D's values NicePrint class NicePrint ( / , * args , ** kwargs ) Provides repr and str by AlignedDict(vars(self)). Example usage: class MyClass(NicePrint): printopts = NicePrint.printopts.copy() printopts[\"excluded\"] += [\"my_hidden_var\"] printopts[\"aliases\"] = {\"asdf\":\"better_name\"} ... View Source class NicePrint : \"\"\"Provides __repr__ and __str__ by AlignedDict(vars(self)). Example usage: >>> class MyClass(NicePrint): >>> printopts = NicePrint.printopts.copy() >>> printopts[\"excluded\"] += [\"my_hidden_var\"] >>> printopts[\"aliases\"] = {\"asdf\":\"better_name\"} >>> ... \"\"\" # _underscored = re.compile('^_') _underscored = lambda s : s . startswith ( \"_\" ) # noqa printopts = dict ( excluded = [ _underscored , \"printopts\" ]) def _repr ( self , is_repr = True ): cls_name = type ( self ) . __name__ + \"(\" if is_repr else \"\" dct = AlignedDict ( vars ( self )) dct . printopts = self . printopts with _shorten_linewidth_by ( len ( cls_name )): txt = repr ( dct ) if is_repr else str ( dct ) txt = ( \" \\n \" + \" \" * len ( cls_name )) . join ( txt . splitlines ()) return cls_name + txt + ( \")\" if is_repr else \"\" ) @ _init def __str__ ( self ): return self . _repr ( is_repr = False ) @ _init def __repr__ ( self ): return self . _repr ( is_repr = True ) Descendants dapper.tools.series.StatPrint dapper.tools.viz.FigSaver dapper.tools.randvars.RV dapper.admin.HiddenMarkovModel dapper.admin.Operator Class variables printopts","title":"Dict Tools"},{"location":"reference/dapper/dict_tools/#module-dapperdict_tools","text":"Tools for working with dicts. Bonus: also some list tools. View Source \"\"\"Tools for working with dicts. Bonus: also some list tools.\"\"\" # TODO 9 # - Make pip package, answer SO.com questions about # https://www.google.com/search?q=python+aligned+dict&oq=python+aligned+dict import itertools import shutil import sys # Since textwrap() only treats strings, while pprint is python-aware, # I would have preferred pprint.pformat(width=lw,compact=True,sort_dicts=False), # but pprint refuses to use my repr. # It also could mess up my recursion guard, which inserts \"<Recursion...>\", # because it tests for x.startswith(\"<\"). import textwrap from _thread import get_ident from contextlib import contextmanager import numpy as np def get0 ( dct ): \"\"\"Get first value.\"\"\" # Bad?: dct[list(dct.keys())[0]] return next ( iter ( dct . values ())) # TODO 4: rm? def flexcomp ( x , * criteria ): \"\"\"Compare in various ways.\"\"\" def _compare ( x , y ): # Callable (condition) compare try : return y ( x ) except TypeError : pass # Regex compare -- should be compiled on the outside try : return bool ( y . search ( x )) except AttributeError : # Value compare return y == x return any ( _compare ( x , y ) for y in criteria ) def _intersect ( iterable , criteria , inv = False ): \"\"\"Keep elements of ``iterable`` that match **any** criteria, as evaluated by flexcomp(). Returns dict/list if ``iterable`` is dict/iterable.\"\"\" def negate ( x ): return ( not x ) if inv else x keys = [ k for k in iterable if negate ( flexcomp ( k , * criteria ))] if isinstance ( iterable , dict ): # Dict return { k : iterable [ k ] for k in keys } return keys # For some reason, _intersect (with the `inv` switch) is # for the brain to parse. Use intersect and complement instead. def intersect ( iterable , wanteds ): return _intersect ( iterable , wanteds , inv = False ) def complement ( iterable , unwanteds ): return _intersect ( iterable , unwanteds , inv = True ) # Complement should be called \"relative complement\" (or set diff) def prodct ( dct ): \"\"\"Cartesian/Outer product, for dicts. Example: >>> list(prodct(dict(n=[1,2], c='ab'))) [{'n': 1, 'c': 'a'}, {'n': 1, 'c': 'b'}, {'n': 2, 'c': 'a'}, {'n': 2, 'c': 'b'}] Source: https://stackoverflow.com/a/40623158/38281.\"\"\" return ( dict ( zip ( dct , x )) for x in itertools . product ( * dct . values ())) def transpose_dicts ( DD , safe = True ): \"\"\"As the name says. Incredibly, it is difficult to make this less verbose (the one-liner is unpredicable for non-rectangular cases) Example: >>> DD = {chr(97+i): {chr(65+j):i*10+j for j in range(2)} for i in range(3)} >>> DD {'a': {'A': 0, 'B': 1}, 'b': {'A': 10, 'B': 11}, 'c': {'A': 20, 'B': 21}} >>> transpose_dicts(DD) {'A': {'a': 0, 'b': 10, 'c': 20}, 'B': {'a': 1, 'b': 11, 'c': 21}} \"\"\" new = {} prev = \"UNINITIALIZED\" for i in DD : # Validate column keys if safe and prev != \"UNINITIALIZED\" : assert prev == DD [ i ] . keys (), f \"Key mismatch for row { i } \" prev = DD [ i ] . keys () for j in DD [ i ]: new . setdefault ( j , {})[ i ] = DD [ i ][ j ] return new def transps ( thing2d , * args , ** kwargs ): \"\"\"Delegates transpose operation to the appropriate function.\"\"\" # Is dict? Otherwise assume list (tuple also works). dict1 = isinstance ( thing2d , dict ) item0 = get0 ( thing2d ) if dict1 else thing2d [ 0 ] # fine since py3.7 dict2 = isinstance ( item0 , dict ) if dict1 and dict2 : f = transpose_dicts # noqa elif dict1 and not dict2 : f = transpose_dict_of_lists # noqa elif not dict1 and dict2 : f = transpose_list_of_dicts # noqa elif not dict1 and not dict2 : f = transpose_lists # noqa return f ( thing2d , * args , ** kwargs ) def transpose_list_of_dicts ( LD , safe = True ): \"\"\" Example: >>> LD = [{chr(97+j):j for j in range(2)} for i in range(3)] >>> LD [{'a': 0, 'b': 1}, {'a': 0, 'b': 1}, {'a': 0, 'b': 1}] >>> transpose_list_of_dicts(LD) {'a': [0, 0, 0], 'b': [1, 1, 1]} \"\"\" new = {} prev = \"UNINITIALIZED\" for i , D in enumerate ( LD ): # Validate column keys if safe and prev != \"UNINITIALIZED\" : assert prev == D . keys (), f \"Key mismatch for dict number { i } \" prev = D . keys () for j in D : new . setdefault ( j , []) . append ( D [ j ]) return new def transpose_dict_of_lists ( DL , safe = True ): \"\"\"This is essentially a one-liner. However, the validation of `safe=True` requires some effort. Example: >>> DL = {chr(97+i): [i*10+j for j in range(2)] for i in range(3)} >>> DL {'a': [0, 1], 'b': [10, 11], 'c': [20, 21]} >>> transpose_dict_of_lists(DL) [{'a': 0, 'b': 10, 'c': 20}, {'a': 1, 'b': 11, 'c': 21}] \"\"\" # Validate row length if safe : lens = [ len ( DL [ k ]) for k in DL ] assert all ( lens [ 0 ] == L for L in lens ), \"Rows have unqual lengths.\" # https://stackoverflow.com/q/5558418 new = [ dict ( zip ( DL , t )) for t in zip ( * DL . values ())] return new def transpose_lists ( LL , safe = True , as_list = False ): \"\"\"This is essentially `zip(*LL)`. However, the validation of `safe=True` requires some effort, and converting to an explicit list requires another bit. Example: >>> LL = [[i*10 + j for j in range(2)] for i in range(3)] >>> LL [[0, 1], [10, 11], [20, 21]] >>> transpose_lists(LL,as_list=True) [[0, 10, 20], [1, 11, 21]] \"\"\" if safe : assert all ( len ( LL [ 0 ]) == len ( row ) for row in LL ) new = zip ( * LL ) # transpose if as_list : # new = list(map(list, new)) new = [ list ( row ) for row in new ] return new # from https://pingfive.typepad.com/blog/2010/04/ def deep_getattr ( obj , name , * default ): for n in name . split ( \".\" ): obj = getattr ( obj , n , * default ) return obj # Rm'd coz it's what type of obj to set at intermediate hierarchy levels: # deep_setattr() def deep_hasattr ( obj , name ): try : deep_getattr ( obj , name ) return True except AttributeError : return False MIN_LINEWIDTH = 5 # TODO 4: should also be made thread-safe? Ref _is_being_printed. _linewidth = None _top_dog = None _is_being_printed = set () @contextmanager def _shorten_linewidth_by ( n ): \"\"\"Shorten linewidth parameter. Also works on numpy.\"\"\" # Avoid (overhead of) importing numpy if not required. # https://stackoverflow.com/a/30483269/38281 if \"numpy\" in sys . modules : def np_lw ( lw ): return np . set_printoptions ( linewidth = lw ) else : # Provide pass-through def np_lw ( lw ): return lw # Structured (store, try, finally) just like np.printoptions(). global _linewidth old = _linewidth try : # Set lw _linewidth -= n _linewidth = max ( _linewidth , MIN_LINEWIDTH ) np_lw ( _linewidth ) yield _linewidth finally : # Restore lw _linewidth = old np_lw ( old ) def _init ( user_function ): \"\"\"Does two things: - Monitor recursion of self. - This is done is just like in ``reprlib`` whence I learned how to make it thread-safe. One diff. is that here _repr_running is global, which works better for this convoluted example: >>> d1 = AlignedDict(a=1) >>> d1[\"d2\"] = AlignedDict(self=d1) >>> d1[\"lst\"] = [d1[\"d2\"]] - Sibling repetition is not recursion. - Initializes the linewidth with ``get_terminal_size()``, unless the ``_linewidth`` variable has been set. Note: Both of these actions also require clean-up, so there's a ``finally`` section as well. Note: Cannot apply to core printing function (AlignedDict._repr), because each call to AlignedDict creates new (unregistered) id's.\"\"\" def wrapped ( self ): # Set linewidth global _top_dog , _linewidth old_linewidth = _linewidth if _top_dog is None : _top_dog = id ( self ) if _linewidth is None : _linewidth , _ = shutil . get_terminal_size () # Turn ON recursion guard id_thread = id ( self ), get_ident () if id_thread in _is_being_printed : return \"<Recurs- %d >\" % id ( self ) # return \"...\" _is_being_printed . add ( id_thread ) try : result = user_function ( self ) finally : # Turn OFF recursion guard _is_being_printed . discard ( id_thread ) # Restore linewidth if id ( self ) == _top_dog : _top_dog = None _linewidth = old_linewidth return result return wrapped class AlignedDict ( dict ): \"\"\"Dict whose items are printed aligned and line-wrapped. Initialized and used just like a regular dict, since only difference to ``dict`` is __str__ and __repr__ (of which the other functions are auxiliaries and hidden). Also provides printopts. Example: >>> a = np.arange(24) >>> A = a.reshape((4,-1)) >>> dct = AlignedDict(a=a, b=99, z=3, A=A) >>> dct[\"self\"] = dct >>> dct[\"sub\"] = AlignedDict(x=1,text=\"lorem\",self=dct) >>> dct.printopts = dict(excluded=[\"z\"], aliases={\"A\":\"matrix\"}) >>> print(repr(dct)) Note on ``printopts[\"indent\"]``: - If not set, then the keys are right-aligned (so that the colons line up), and the values (including multi-line) are printed to their right, allowing them to be placed on the same line as the key. This setting is pretty, unless the keys are really long. - If set (to an integer), then the values are printed on a new line, indented by that (unchanging) length (unless they're one-liners). A similar class is scipy.optimize.OptimizeResult See also: - pprint.pformat(x,linewidth,compact=True,sort_dicts=False) - json.dumps(x, indent=4, sort_keys=False, default=repr) But, both of these work recursively, which is NOT our aim. In particular, they need to implement behaviour for all types. \"\"\" @_init def __str__ ( self ): return self . _repr ( is_repr = False ) @_init def __repr__ ( self ): return self . _repr ( is_repr = True ) def _repr ( self , is_repr = False ): \"\"\"Both __str__ and __repr__ in one! Note: if ``self`` is printed as part of a standard object, __repr__ gets called, not __str__.\"\"\" # Constants sep = \": \" bullet = \" \" if is_repr else \" - \" comma = \",\" if is_repr else \"\" spinechar = \" \" if is_repr else \"\u00a6\" # \u00a6, \u2506, \u2502, \u23b8, \u258f format_key = repr if is_repr else str format_val = repr if is_repr else str # Apply print options dct = self opt = getattr ( self , \"printopts\" , {}) dct = intersect ( dct , opt . get ( \"included\" , dct )) dct = complement ( dct , opt . get ( \"excluded\" , [])) dct = { opt . get ( \"aliases\" , {}) . get ( k , k ): v for k , v in dct . items ()} if \"indent\" in opt : # LEFT-aligned keys -- indent is fixed, and val is always on newline val_indent = \" \" * len ( bullet ) + spinechar . ljust ( opt [ \"indent\" ]) else : # RIGHT-aligned keys -- indent ~ on keys, and val can start on key's line kWidth = max ([ len ( format_key ( k )) for k in dct ], default = 0 ) val_indent = \" \" * len ( bullet ) + \" \" * kWidth + spinechar . ljust ( len ( sep )) def format_key ( key , old = format_key ): return old ( key ) . rjust ( kWidth ) def iRepr ( key , val ): trim = len ( val_indent + comma ) # Try placing on 1 line, but use max() to insure against wrapping if \"indent\" in opt and ( \" \\n \" not in format_val ( val )): trim = max ( trim , len ( bullet + format_key ( key ) + sep + comma )) with _shorten_linewidth_by ( trim ): val = self . _wrap_item ( format_val ( val )) val = ( \" \\n \" + val_indent ) . join ( val ) if \"indent\" in opt and ( \" \\n \" in val ): val = \" \\n \" + val_indent + val # 1st line also on new line return format_key ( key ) + sep + val # dct --> text items = [ iRepr ( k , v ) for k , v in dct . items ()] # Sort items = self . _sort ( zip ( dct , items )) # Join txt = ( comma + \" \\n \" + bullet ) . join ( items ) if len ( items ) > 1 : txt = \" \\n \" + bullet + txt + \" \\n \" # Add braces, etc if is_repr : txt = \"{\" + txt + \"}\" return txt # AUX FUNCTIONS @staticmethod def _wrap_item ( txt ): \"\"\"Wrap txt.\"\"\" lines = txt . splitlines () ll = [ textwrap . wrap ( line , _linewidth ) for line in lines ] # list of lists hang = \" \" # No need to shorten_lw for this ll = [ w [: 1 ] + [ hang + line for line in w [ 1 :]] for w in ll ] ll = [ k for w in ll for k in w ] # flatten list-of-lists return ll def _sort ( self , dict_and_texts ): ORDR = getattr ( self , \"printopts\" , {}) . get ( \"ordering\" , []) or [] reverse = getattr ( self , \"printopts\" , {}) . get ( \"reverse\" , False ) def indexer ( key_text ): key , text = key_text if not isinstance ( ORDR , str ): # assume list # Manual ordering. if key in ORDR : # -10000 => priority idx = - 10000 + ORDR . index ( key ) else : idx = 0 # neutral elif \"line\" in ORDR : # Line-number count idx = 100 * text . count ( \" \\n \" ) # Line-length count idx += max ( len ( line ) for line in text . splitlines ()) elif \"alpha\" in ORDR : # Alphabetic (tuple compare) idx = key . lower () return idx pairs = sorted ( dict_and_texts , key = indexer , reverse = reverse ) return [ text for key , text in pairs ] class NicePrint : \"\"\"Provides __repr__ and __str__ by AlignedDict(vars(self)). Example usage: >>> class MyClass(NicePrint): >>> printopts = NicePrint.printopts.copy() >>> printopts[\"excluded\"] += [\"my_hidden_var\"] >>> printopts[\"aliases\"] = {\"asdf\":\"better_name\"} >>> ... \"\"\" # _underscored = re.compile('^_') _underscored = lambda s : s . startswith ( \"_\" ) # noqa printopts = dict ( excluded = [ _underscored , \"printopts\" ]) def _repr ( self , is_repr = True ): cls_name = type ( self ) . __name__ + \"(\" if is_repr else \"\" dct = AlignedDict ( vars ( self )) dct . printopts = self . printopts with _shorten_linewidth_by ( len ( cls_name )): txt = repr ( dct ) if is_repr else str ( dct ) txt = ( \" \\n \" + \" \" * len ( cls_name )) . join ( txt . splitlines ()) return cls_name + txt + ( \")\" if is_repr else \"\" ) @_init def __str__ ( self ): return self . _repr ( is_repr = False ) @_init def __repr__ ( self ): return self . _repr ( is_repr = True ) class DotDict ( AlignedDict ): \"\"\"Dict that *also* supports attribute (dot) access. Benefit compared to a dict: - Verbosity of ``d['a']`` vs. ``d.a``. - Includes ``AlignedDict``. DotDict is not terribly hackey, and is quite robust. Similar constructs are quite common, eg IPython/utils/ipstruct.py. Main inspiration: https://stackoverflow.com/a/14620633 \"\"\" printopts = dict ( excluded = [ \"printopts\" ]) def __init__ ( self , * args , ** kwargs ): \"\"\"Init like a normal dict.\"\"\" super () . __init__ ( * args , ** kwargs ) # Make a (normal) dict self . __dict__ = self # Assign it to self.__dict__ # def print_nested(dct): # \"\"\"Print nested dicts\"\"\" # # # Note: if a dict is inside of a list (for example), # # it will be printed in the standard fashion, # # despite our overriding the builtins.repr. # # There appears to be no way around this, except, # # like reprlib does, defining custom treatment for each # # type (dicts, list, sets, tuples, deques, etc). # # # Another issue is that I dont know how to check if an object # # is suitable for AlignedDict printing or not, # # so this function is restricted to dicts. # # @_init # def new_repr(obj): # if hasattr(obj,\"items\"): # obj = AlignedDict(obj) # return orig_repr(obj) # # import builtins # orig_repr = builtins.repr # try: # builtins.repr = new_repr # txt = repr(dct) # finally: # builtins.repr = orig_repr # # print(txt) if __name__ == \"__main__\" : # Note: this setup is purposely messy, # in order to test recursion treatments. a = np . arange ( 24 ) A = a . reshape (( 4 , - 1 )) d1 = dict ( a = a , b = 99 , z = 3 , A = A ) d2 = dict ( x = 1 , lorem = \"ipsum\" ) # Dont move this block below (coz then it will contain # d1/d2 recursions, rather than a1/a2) a1 = AlignedDict ( d1 ) a2 = AlignedDict ( d2 ) # Json -- cannot handle recursions # import json d1 [ \"d2\" ] = d2 # print(\"\\njson.dumps:\\n================\") # print(json.dumps(d1, indent=4, default=repr)) # pprint # import pprint d1 [ \"d2\" ] = d2 d2 [ \"d1\" ] = d1 d1 [ \"lst\" ] = [ 0 , 1 , d2 ] # print(\"\\npprint:\\n================\") # pprint.pprint(d1,compact=True) # Regular dict/print # print(\"\\nRegular dict/print:\\n================\") # print(d1) # Add recursions similar to d1/d2 # print(\"\\nAlignedDict:\\n================\") a2 [ \"a1\" ] = a1 a1 [ \"a2\" ] = a2 a1 [ \"one\" ] = AlignedDict ( item = \"hello\" ) a1 [ \"empty\" ] = AlignedDict () a1 [ \"really long name that goes on and on\" ] = [ 0 , 1 , a2 ] a1 . printopts = dict ( excluded = [ \"z\" ], aliases = { \"A\" : \"aMatrix\" }, ordering = \"line\" , # or alpha or [\"a2\", \"self\"] # reverse=True, ) print ( \" \\n str: \\n ================\" ) print ( a1 ) print ( \" \\n repr: \\n ================\" ) print ( repr ( a1 )) print ( \" \\n ================ \\n with const. indent: \\n ================\" ) a1 . printopts [ \"indent\" ] = 1 a2 . printopts = dict ( indent = 1 ) print ( \" \\n str: \\n ================\" ) print ( a1 ) print ( \" \\n repr: \\n ================\" ) print ( repr ( a1 )) print ( \" \\n NicePrint: \\n ================\" ) class MyClass ( NicePrint ): printopts = NicePrint . printopts . copy () def __init__ ( self ): self . _a = 99 self . a = np . arange ( 24 ) # self.a = 1 # self.lorem = \"ipsum\" # self.lst = np.arange(20) obj1 = MyClass () # obj1.obj2 = MyClass() # obj1.self = obj1 obj1 . printopts [ \"excluded\" ] += [ \"lst\" ] print ( repr ( obj1 )) print ( obj1 ) print ( \" \\n DotDict: \\n ================\" ) dd = DotDict ( a = a , b = 99 , z = 3 , A = A ) dd . dd2 = DotDict ( a = a , b = 99 , z = 3 , A = A ) dd . self = dd dd . printopts [ \"excluded\" ] += [ \"A\" ] print ( repr ( dd )) print ( dd ) # Other tests print ( \" \\n deep_getattr: \\n ================\" ) key2 = \"self.self.self.self.a\" print ( key2 , \":\" , deep_getattr ( dd , key2 ))","title":"Module dapper.dict_tools"},{"location":"reference/dapper/dict_tools/#variables","text":"MIN_LINEWIDTH","title":"Variables"},{"location":"reference/dapper/dict_tools/#functions","text":"","title":"Functions"},{"location":"reference/dapper/dict_tools/#complement","text":"def complement ( iterable , unwanteds ) View Source def complement ( iterable , unwanteds ): return _intersect ( iterable , unwanteds , inv = True )","title":"complement"},{"location":"reference/dapper/dict_tools/#deep_getattr","text":"def deep_getattr ( obj , name , * default ) View Source def deep_getattr ( obj , name , * default ): for n in name . split ( \".\" ): obj = getattr ( obj , n , * default ) return obj","title":"deep_getattr"},{"location":"reference/dapper/dict_tools/#deep_hasattr","text":"def deep_hasattr ( obj , name ) View Source def deep_hasattr ( obj , name ): try : deep_getattr ( obj , name ) return True except AttributeError : return False","title":"deep_hasattr"},{"location":"reference/dapper/dict_tools/#flexcomp","text":"def flexcomp ( x , * criteria ) Compare in various ways. View Source def flexcomp ( x , * criteria ): \"\"\"Compare in various ways.\"\"\" def _compare ( x , y ): # Callable (condition) compare try : return y ( x ) except TypeError : pass # Regex compare -- should be compiled on the outside try : return bool ( y . search ( x )) except AttributeError : # Value compare return y == x return any ( _compare ( x , y ) for y in criteria )","title":"flexcomp"},{"location":"reference/dapper/dict_tools/#get0","text":"def get0 ( dct ) Get first value. View Source def get0 ( dct ): \"\"\"Get first value.\"\"\" # Bad ? : dct [ list ( dct . keys ())[ 0 ]] return next ( iter ( dct . values ()))","title":"get0"},{"location":"reference/dapper/dict_tools/#intersect","text":"def intersect ( iterable , wanteds ) View Source def intersect ( iterable , wanteds ): return _intersect ( iterable , wanteds , inv = False )","title":"intersect"},{"location":"reference/dapper/dict_tools/#prodct","text":"def prodct ( dct ) Cartesian/Outer product, for dicts. Example: list(prodct(dict(n=[1,2], c='ab'))) [{'n': 1, 'c': 'a'}, {'n': 1, 'c': 'b'}, {'n': 2, 'c': 'a'}, {'n': 2, 'c': 'b'}] Source: https://stackoverflow.com/a/40623158/38281. View Source def prodct ( dct ): \"\"\"Cartesian/Outer product, for dicts. Example: >>> list(prodct(dict(n=[1,2], c='ab'))) [{'n': 1, 'c': 'a'}, {'n': 1, 'c': 'b'}, {'n': 2, 'c': 'a'}, {'n': 2, 'c': 'b'}] Source: https://stackoverflow.com/a/40623158/38281.\"\"\" return ( dict ( zip ( dct , x )) for x in itertools . product ( * dct . values ()))","title":"prodct"},{"location":"reference/dapper/dict_tools/#transpose_dict_of_lists","text":"def transpose_dict_of_lists ( DL , safe = True ) This is essentially a one-liner. However, the validation of safe=True requires some effort. Example: DL = {chr(97+i): [i*10+j for j in range(2)] for i in range(3)} DL {'a': [0, 1], 'b': [10, 11], 'c': [20, 21]} transpose_dict_of_lists(DL) [{'a': 0, 'b': 10, 'c': 20}, {'a': 1, 'b': 11, 'c': 21}] View Source def transpose_dict_of_lists ( DL , safe = True ) : \"\"\"This is essentially a one-liner. However, the validation of `safe=True` requires some effort. Example: >>> DL = {chr(97+i): [i*10+j for j in range(2)] for i in range(3)} >>> DL {'a': [0, 1], 'b': [10, 11], 'c': [20, 21]} >>> transpose_dict_of_lists(DL) [{'a': 0, 'b': 10, 'c': 20}, {'a': 1, 'b': 11, 'c': 21}] \"\"\" # Validate row length if safe : lens = [ len(DL[k ] ) for k in DL ] assert all ( lens [ 0 ] == L for L in lens ), \"Rows have unqual lengths.\" # https : // stackoverflow . com / q / 5558418 new = [ dict(zip(DL, t)) for t in zip(*DL.values()) ] return new","title":"transpose_dict_of_lists"},{"location":"reference/dapper/dict_tools/#transpose_dicts","text":"def transpose_dicts ( DD , safe = True ) As the name says. Incredibly, it is difficult to make this less verbose (the one-liner is unpredicable for non-rectangular cases) Example: DD = {chr(97+i): {chr(65+j):i*10+j for j in range(2)} for i in range(3)} DD {'a': {'A': 0, 'B': 1}, 'b': {'A': 10, 'B': 11}, 'c': {'A': 20, 'B': 21}} transpose_dicts(DD) {'A': {'a': 0, 'b': 10, 'c': 20}, 'B': {'a': 1, 'b': 11, 'c': 21}} View Source def transpose_dicts ( DD , safe = True ) : \"\"\"As the name says. Incredibly, it is difficult to make this less verbose (the one-liner is unpredicable for non-rectangular cases) Example: >>> DD = {chr(97+i): {chr(65+j):i*10+j for j in range(2)} for i in range(3)} >>> DD {'a': {'A': 0, 'B': 1}, 'b': {'A': 10, 'B': 11}, 'c': {'A': 20, 'B': 21}} >>> transpose_dicts(DD) {'A': {'a': 0, 'b': 10, 'c': 20}, 'B': {'a': 1, 'b': 11, 'c': 21}} \"\"\" new = {} prev = \"UNINITIALIZED\" for i in DD : # Validate column keys if safe and prev != \"UNINITIALIZED\" : assert prev == DD [ i ] . keys (), f \"Key mismatch for row {i}\" prev = DD [ i ] . keys () for j in DD [ i ] : new . setdefault ( j , {} ) [ i ] = DD [ i ][ j ] return new","title":"transpose_dicts"},{"location":"reference/dapper/dict_tools/#transpose_list_of_dicts","text":"def transpose_list_of_dicts ( LD , safe = True ) Example: LD = [{chr(97+j):j for j in range(2)} for i in range(3)] LD [{'a': 0, 'b': 1}, {'a': 0, 'b': 1}, {'a': 0, 'b': 1}] transpose_list_of_dicts(LD) View Source def transpose_list_of_dicts ( LD , safe = True ) : \"\"\" Example: >>> LD = [{chr(97+j):j for j in range(2)} for i in range(3)] >>> LD [{'a': 0, 'b': 1}, {'a': 0, 'b': 1}, {'a': 0, 'b': 1}] >>> transpose_list_of_dicts(LD) {'a': [0, 0, 0], 'b': [1, 1, 1]} \"\"\" new = {} prev = \"UNINITIALIZED\" for i , D in enumerate ( LD ) : # Validate column keys if safe and prev != \"UNINITIALIZED\" : assert prev == D . keys (), f \"Key mismatch for dict number {i}\" prev = D . keys () for j in D : new . setdefault ( j , [] ). append ( D [ j ] ) return new","title":"transpose_list_of_dicts"},{"location":"reference/dapper/dict_tools/#transpose_lists","text":"def transpose_lists ( LL , safe = True , as_list = False ) This is essentially zip(*LL) . However, the validation of safe=True requires some effort, and converting to an explicit list requires another bit. Example: LL = [[i*10 + j for j in range(2)] for i in range(3)] LL [[0, 1], [10, 11], [20, 21]] transpose_lists(LL,as_list=True) [[0, 10, 20], [1, 11, 21]] View Source def transpose_lists ( LL , safe = True , as_list = False ): \"\"\"This is essentially `zip(*LL)`. However, the validation of `safe=True` requires some effort, and converting to an explicit list requires another bit. Example: >>> LL = [[i*10 + j for j in range(2)] for i in range(3)] >>> LL [[0, 1], [10, 11], [20, 21]] >>> transpose_lists(LL,as_list=True) [[0, 10, 20], [1, 11, 21]] \"\"\" if safe : assert all ( len ( LL [ 0 ]) == len ( row ) for row in LL ) new = zip ( * LL ) # transpose if as_list : # new = list ( map ( list , new )) new = [ list ( row ) for row in new ] return new","title":"transpose_lists"},{"location":"reference/dapper/dict_tools/#transps","text":"def transps ( thing2d , * args , ** kwargs ) Delegates transpose operation to the appropriate function. View Source def transps ( thing2d , * args , ** kwargs ): \"\"\"Delegates transpose operation to the appropriate function.\"\"\" # Is dict ? Otherwise assume list ( tuple also works ). dict1 = isinstance ( thing2d , dict ) item0 = get0 ( thing2d ) if dict1 else thing2d [ 0 ] # fine since py3 . 7 dict2 = isinstance ( item0 , dict ) if dict1 and dict2 : f = transpose_dicts # noqa elif dict1 and not dict2 : f = transpose_dict_of_lists # noqa elif not dict1 and dict2 : f = transpose_list_of_dicts # noqa elif not dict1 and not dict2 : f = transpose_lists # noqa return f ( thing2d , * args , ** kwargs )","title":"transps"},{"location":"reference/dapper/dict_tools/#classes","text":"","title":"Classes"},{"location":"reference/dapper/dict_tools/#aligneddict","text":"class AlignedDict ( / , * args , ** kwargs ) Dict whose items are printed aligned and line-wrapped. Initialized and used just like a regular dict, since only difference to dict is str and repr (of which the other functions are auxiliaries and hidden). Also provides printopts. Example: a = np.arange(24) A = a.reshape((4,-1)) dct = AlignedDict(a=a, b=99, z=3, A=A) dct[\"self\"] = dct dct[\"sub\"] = AlignedDict(x=1,text=\"lorem\",self=dct) dct.printopts = dict(excluded=[\"z\"], aliases={\"A\":\"matrix\"}) print(repr(dct)) Note on printopts[\"indent\"] : - If not set, then the keys are right-aligned (so that the colons line up), and the values (including multi-line) are printed to their right, allowing them to be placed on the same line as the key. This setting is pretty, unless the keys are really long. - If set (to an integer), then the values are printed on a new line, indented by that (unchanging) length (unless they're one-liners). A similar class is scipy.optimize.OptimizeResult See also: - pprint.pformat(x,linewidth,compact=True,sort_dicts=False) - json.dumps(x, indent=4, sort_keys=False, default=repr) But, both of these work recursively, which is NOT our aim. In particular, they need to implement behaviour for all types. View Source class AlignedDict ( dict ) : \" \"\" Dict whose items are printed aligned and line-wrapped. Initialized and used just like a regular dict, since only difference to ``dict`` is __str__ and __repr__ (of which the other functions are auxiliaries and hidden). Also provides printopts. Example: >>> a = np.arange(24) >>> A = a.reshape((4,-1)) >>> dct = AlignedDict(a=a, b=99, z=3, A=A) >>> dct[\" self \"] = dct >>> dct[\" sub \"] = AlignedDict(x=1,text=\" lorem \",self=dct) >>> dct.printopts = dict(excluded=[\" z \"], aliases={\" A \":\" matrix \"}) >>> print(repr(dct)) Note on ``printopts[\" indent \"]``: - If not set, then the keys are right-aligned (so that the colons line up), and the values (including multi-line) are printed to their right, allowing them to be placed on the same line as the key. This setting is pretty, unless the keys are really long. - If set (to an integer), then the values are printed on a new line, indented by that (unchanging) length (unless they're one-liners). A similar class is scipy.optimize.OptimizeResult See also: - pprint.pformat(x,linewidth,compact=True,sort_dicts=False) - json.dumps(x, indent=4, sort_keys=False, default=repr) But, both of these work recursively, which is NOT our aim. In particular, they need to implement behaviour for all types. \"\" \" @_init def __str__ ( self ) : return self . _repr ( is_repr = False ) @_init def __repr__ ( self ) : return self . _repr ( is_repr = True ) def _repr ( self , is_repr = False ) : \" \"\" Both __str__ and __repr__ in one! Note: if ``self`` is printed as part of a standard object, __repr__ gets called, not __str__. \"\" \" # Constants sep = \": \" bullet = \" \" if is_repr else \" - \" comma = \",\" if is_repr else \"\" spinechar = \" \" if is_repr else \"\u00a6\" # \u00a6, \u2506, \u2502, \u23b8, \u258f format_key = repr if is_repr else str format_val = repr if is_repr else str # Apply print options dct = self opt = getattr ( self , \"printopts\" , {} ) dct = intersect ( dct , opt . get ( \"included\" , dct )) dct = complement ( dct , opt . get ( \"excluded\" , [] )) dct = { opt . get ( \"aliases\" , {} ). get ( k , k ) : v for k , v in dct . items () } if \"indent\" in opt : # LEFT-aligned keys -- indent is fixed, and val is always on newline val_indent = \" \" * len ( bullet ) + spinechar . ljust ( opt [ \"indent\" ] ) else : # RIGHT-aligned keys -- indent ~ on keys, and val can start on key's line kWidth = max ( [ len ( format_key ( k )) for k in dct ] , default = 0 ) val_indent = \" \" * len ( bullet ) + \" \" * kWidth + spinechar . ljust ( len ( sep )) def format_key ( key , old = format_key ) : return old ( key ). rjust ( kWidth ) def iRepr ( key , val ) : trim = len ( val_indent + comma ) # Try placing on 1 line, but use max() to insure against wrapping if \"indent\" in opt and ( \" \\n \" not in format_val ( val )) : trim = max ( trim , len ( bullet + format_key ( key ) + sep + comma )) with _shorten_linewidth_by ( trim ) : val = self . _wrap_item ( format_val ( val )) val = ( \" \\n \" + val_indent ). join ( val ) if \"indent\" in opt and ( \" \\n \" in val ) : val = \" \\n \" + val_indent + val # 1st line also on new line return format_key ( key ) + sep + val # dct --> text items = [ iRepr ( k , v ) for k , v in dct . items () ] # Sort items = self . _sort ( zip ( dct , items )) # Join txt = ( comma + \" \\n \" + bullet ). join ( items ) if len ( items ) > 1 : txt = \" \\n \" + bullet + txt + \" \\n \" # Add braces, etc if is_repr : txt = \"{\" + txt + \"}\" return txt # AUX FUNCTIONS @staticmethod def _wrap_item ( txt ) : \" \"\" Wrap txt. \"\" \" lines = txt . splitlines () ll = [ textwrap . wrap ( line , _linewidth ) for line in lines ] # list of lists hang = \" \" # No need to shorten_lw for this ll = [ w [ : 1 ] + [ hang + line for line in w [ 1 : ]] for w in ll ] ll = [ k for w in ll for k in w ] # flatten list-of-lists return ll def _sort ( self , dict_and_texts ) : ORDR = getattr ( self , \"printopts\" , {} ). get ( \"ordering\" , [] ) or [] reverse = getattr ( self , \"printopts\" , {} ). get ( \"reverse\" , False ) def indexer ( key_text ) : key , text = key_text if not isinstance ( ORDR , str ) : # assume list # Manual ordering. if key in ORDR : # -10000 => priority idx = - 10000 + ORDR . index ( key ) else : idx = 0 # neutral elif \"line\" in ORDR : # Line-number count idx = 100 * text . count ( \" \\n \" ) # Line-length count idx += max ( len ( line ) for line in text . splitlines ()) elif \"alpha\" in ORDR : # Alphabetic (tuple compare) idx = key . lower () return idx pairs = sorted ( dict_and_texts , key = indexer , reverse = reverse ) return [ text for key , text in pairs ]","title":"AlignedDict"},{"location":"reference/dapper/dict_tools/#ancestors-in-mro","text":"builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/dapper/dict_tools/#descendants","text":"dapper.dict_tools.DotDict","title":"Descendants"},{"location":"reference/dapper/dict_tools/#methods","text":"","title":"Methods"},{"location":"reference/dapper/dict_tools/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/dapper/dict_tools/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/dapper/dict_tools/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/dapper/dict_tools/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/dapper/dict_tools/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/dapper/dict_tools/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/dapper/dict_tools/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/dapper/dict_tools/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/dapper/dict_tools/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/dapper/dict_tools/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/dapper/dict_tools/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/dapper/dict_tools/#dotdict","text":"class DotDict ( * args , ** kwargs ) Dict that also supports attribute (dot) access. Benefit compared to a dict: Verbosity of d['a'] vs. d.a . Includes AlignedDict . DotDict is not terribly hackey, and is quite robust. Similar constructs are quite common, eg IPython/utils/ipstruct.py. Main inspiration: https://stackoverflow.com/a/14620633 View Source class DotDict ( AlignedDict ) : \" \"\" Dict that *also* supports attribute (dot) access. Benefit compared to a dict: - Verbosity of ``d['a']`` vs. ``d.a``. - Includes ``AlignedDict``. DotDict is not terribly hackey, and is quite robust. Similar constructs are quite common, eg IPython/utils/ipstruct.py. Main inspiration: https://stackoverflow.com/a/14620633 \"\" \" printopts = dict ( excluded = [ \"printopts\" ] ) def __init__ ( self , * args , ** kwargs ) : \" \"\" Init like a normal dict. \"\" \" super (). __init__ ( * args , ** kwargs ) # Make a (normal) dict self . __dict__ = self # Assign it to self.__dict__","title":"DotDict"},{"location":"reference/dapper/dict_tools/#ancestors-in-mro_1","text":"dapper.dict_tools.AlignedDict builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/dapper/dict_tools/#descendants_1","text":"dapper.stats.Avrgs","title":"Descendants"},{"location":"reference/dapper/dict_tools/#class-variables","text":"printopts","title":"Class variables"},{"location":"reference/dapper/dict_tools/#methods_1","text":"","title":"Methods"},{"location":"reference/dapper/dict_tools/#clear_1","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/dapper/dict_tools/#copy_1","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/dapper/dict_tools/#fromkeys_1","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/dapper/dict_tools/#get_1","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/dapper/dict_tools/#items_1","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/dapper/dict_tools/#keys_1","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/dapper/dict_tools/#pop_1","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/dapper/dict_tools/#popitem_1","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/dapper/dict_tools/#setdefault_1","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/dapper/dict_tools/#update_1","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/dapper/dict_tools/#values_1","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/dapper/dict_tools/#niceprint","text":"class NicePrint ( / , * args , ** kwargs ) Provides repr and str by AlignedDict(vars(self)). Example usage: class MyClass(NicePrint): printopts = NicePrint.printopts.copy() printopts[\"excluded\"] += [\"my_hidden_var\"] printopts[\"aliases\"] = {\"asdf\":\"better_name\"} ... View Source class NicePrint : \"\"\"Provides __repr__ and __str__ by AlignedDict(vars(self)). Example usage: >>> class MyClass(NicePrint): >>> printopts = NicePrint.printopts.copy() >>> printopts[\"excluded\"] += [\"my_hidden_var\"] >>> printopts[\"aliases\"] = {\"asdf\":\"better_name\"} >>> ... \"\"\" # _underscored = re.compile('^_') _underscored = lambda s : s . startswith ( \"_\" ) # noqa printopts = dict ( excluded = [ _underscored , \"printopts\" ]) def _repr ( self , is_repr = True ): cls_name = type ( self ) . __name__ + \"(\" if is_repr else \"\" dct = AlignedDict ( vars ( self )) dct . printopts = self . printopts with _shorten_linewidth_by ( len ( cls_name )): txt = repr ( dct ) if is_repr else str ( dct ) txt = ( \" \\n \" + \" \" * len ( cls_name )) . join ( txt . splitlines ()) return cls_name + txt + ( \")\" if is_repr else \"\" ) @ _init def __str__ ( self ): return self . _repr ( is_repr = False ) @ _init def __repr__ ( self ): return self . _repr ( is_repr = True )","title":"NicePrint"},{"location":"reference/dapper/dict_tools/#descendants_2","text":"dapper.tools.series.StatPrint dapper.tools.viz.FigSaver dapper.tools.randvars.RV dapper.admin.HiddenMarkovModel dapper.admin.Operator","title":"Descendants"},{"location":"reference/dapper/dict_tools/#class-variables_1","text":"printopts","title":"Class variables"},{"location":"reference/dapper/dpr_config/","text":"Module dapper.dpr_config Load DAPPER configuration settings into rc . View Source \"\"\"Load DAPPER configuration settings into `rc`.\"\"\" import os import sys from pathlib import Path import matplotlib as mpl import yaml from dapper.dict_tools import DotDict ################################## # Load configurations ################################## dapper_dir = Path ( __file__ ) . absolute () . parent rc = DotDict () for d in [ dapper_dir , \"~\" , sys . path [ 0 ]]: d = Path ( d ) . expanduser () for prefix in [ \".\" , \"\" ]: f = d / ( prefix + \"dpr_config.yaml\" ) if f . is_file (): rc . update ( yaml . load ( open ( f ), Loader = yaml . SafeLoader )) ################################## # Setup dir paths ################################## rc . dirs = DotDict () rc . dirs . dapper = dapper_dir rc . dirs . DAPPER = rc . dirs . dapper . parent # Data path x = rc . pop ( \"data_root\" ) if x . lower () == \"$cwd\" : x = Path . cwd () elif x . lower () == \"$dapper\" : x = rc . dirs . DAPPER else : x = Path ( x ) rc . dirs . data = x / \"dpr_data\" rc . dirs . samples = rc . dirs . data / \"samples\" # Expanduser, create dir for d in rc . dirs : rc . dirs [ d ] = rc . dirs [ d ] . expanduser () os . makedirs ( rc . dirs [ d ], exist_ok = True ) ################################## # Disable rc.liveplotting ? ################################## LP = rc . liveplotting if LP : backend = mpl . get_backend () . lower () non_interactive = [ 'agg' , 'ps' , 'pdf' , 'svg' , 'cairo' , 'gdk' ] LP &= not any ([ backend == x for x in non_interactive ]) # Also disable for inline backends, which are buggy with liveplotting LP &= 'inline' not in backend LP &= 'nbagg' not in backend if not LP : print ( \" \\n Warning: You have not disableed interactive/live plotting\" \" in your dpr_config.py,\" \" but this is not supported by current backend:\" f \" {mpl.get_backend()}.\" \" To enable it, try using another backend,\" \" e.g., mpl.use('Qt5Agg'). \\n \" ) rc . liveplotting = LP Variables LP backend d dapper_dir non_interactive prefix rc x","title":"Dpr Config"},{"location":"reference/dapper/dpr_config/#module-dapperdpr_config","text":"Load DAPPER configuration settings into rc . View Source \"\"\"Load DAPPER configuration settings into `rc`.\"\"\" import os import sys from pathlib import Path import matplotlib as mpl import yaml from dapper.dict_tools import DotDict ################################## # Load configurations ################################## dapper_dir = Path ( __file__ ) . absolute () . parent rc = DotDict () for d in [ dapper_dir , \"~\" , sys . path [ 0 ]]: d = Path ( d ) . expanduser () for prefix in [ \".\" , \"\" ]: f = d / ( prefix + \"dpr_config.yaml\" ) if f . is_file (): rc . update ( yaml . load ( open ( f ), Loader = yaml . SafeLoader )) ################################## # Setup dir paths ################################## rc . dirs = DotDict () rc . dirs . dapper = dapper_dir rc . dirs . DAPPER = rc . dirs . dapper . parent # Data path x = rc . pop ( \"data_root\" ) if x . lower () == \"$cwd\" : x = Path . cwd () elif x . lower () == \"$dapper\" : x = rc . dirs . DAPPER else : x = Path ( x ) rc . dirs . data = x / \"dpr_data\" rc . dirs . samples = rc . dirs . data / \"samples\" # Expanduser, create dir for d in rc . dirs : rc . dirs [ d ] = rc . dirs [ d ] . expanduser () os . makedirs ( rc . dirs [ d ], exist_ok = True ) ################################## # Disable rc.liveplotting ? ################################## LP = rc . liveplotting if LP : backend = mpl . get_backend () . lower () non_interactive = [ 'agg' , 'ps' , 'pdf' , 'svg' , 'cairo' , 'gdk' ] LP &= not any ([ backend == x for x in non_interactive ]) # Also disable for inline backends, which are buggy with liveplotting LP &= 'inline' not in backend LP &= 'nbagg' not in backend if not LP : print ( \" \\n Warning: You have not disableed interactive/live plotting\" \" in your dpr_config.py,\" \" but this is not supported by current backend:\" f \" {mpl.get_backend()}.\" \" To enable it, try using another backend,\" \" e.g., mpl.use('Qt5Agg'). \\n \" ) rc . liveplotting = LP","title":"Module dapper.dpr_config"},{"location":"reference/dapper/dpr_config/#variables","text":"LP backend d dapper_dir non_interactive prefix rc x","title":"Variables"},{"location":"reference/dapper/stats/","text":"Module dapper.stats Provide the stats class which defines the \"builtin\" stats to be computed. View Source \"\"\"Provide the stats class which defines the \"builtin\" stats to be computed.\"\"\" from dapper.dict_tools import DotDict from dapper.tools.matrices import CovMat import dapper.dict_tools as dict_tools from matplotlib import pyplot as plt from dapper.dpr_config import rc import dapper.tools.liveplotting as liveplotting import dapper.tools.series as series from dapper.tools.series import StatPrint , DataSeries import dapper.tools.utils as utils from dapper.tools.math import unbias_var import numpy as np import scipy.linalg as sla import warnings class Stats ( StatPrint ): \"\"\"Contains and computes statistics of the DA methods. Use new_series() to register your own stat time series. \"\"\" def __init__ ( self , xp , HMM , xx , yy , liveplots = False , store_u = rc . store_u ): \"\"\"Init the default statistics. Note: Python allows dynamically creating attributes, so you can easily add custom stat. series to a Stat instance within a particular method, for example. Use ``new_series`` to get automatic averaging too. \"\"\" ###################################### # Preamble ###################################### self . xp = xp self . HMM = HMM self . xx = xx self . yy = yy self . liveplots = liveplots self . store_u = store_u self . store_s = hasattr ( xp , 'Lag' ) # Shapes K = xx . shape [ 0 ] - 1 Nx = xx . shape [ 1 ] KObs = yy . shape [ 0 ] - 1 Ny = yy . shape [ 1 ] self . K , self . Nx = K , Nx self . KObs , self . Ny = KObs , Ny # Methods for summarizing multivariate stats (\"fields\") as scalars # Don't use nanmean here; nan's should get propagated! self . field_summaries = dict ( m = lambda x : np . mean ( x ), # mean-field rms = lambda x : np . sqrt ( np . mean ( x ** 2 )), # root-mean-square ma = lambda x : np . mean ( np . abs ( x )), # mean-absolute gm = lambda x : np . exp ( np . mean ( np . log ( x ))), # geometric mean ) # Only keep the methods listed in rc self . field_summaries = dict_tools . intersect ( self . field_summaries , rc . field_summaries ) # Define similar methods, but restricted to sectors self . sector_summaries = {} def restrict ( fun , inds ): return ( lambda x : fun ( x [ inds ])) for suffix , formula in self . field_summaries . items (): for sector , inds in HMM . sectors . items (): f = restrict ( formula , inds ) self . sector_summaries [ ' %s . %s ' % ( suffix , sector )] = f ###################################### # Allocate time series of various stats ###################################### self . new_series ( 'mu' , Nx , MS = 'sec' ) # Mean self . new_series ( 'std' , Nx , MS = 'sec' ) # Std. dev. (\"spread\") self . new_series ( 'err' , Nx , MS = 'sec' ) # Error (mu - truth) self . new_series ( 'gscore' , Nx , MS = 'sec' ) # Gaussian (log) score # To save memory, we only store these field means: self . new_series ( 'mad' , 1 ) # Mean abs deviations self . new_series ( 'skew' , 1 ) # Skewness self . new_series ( 'kurt' , 1 ) # Kurtosis if hasattr ( xp , 'N' ): N = xp . N self . new_series ( 'w' , N , MS = True ) # Importance weights self . new_series ( 'rh' , Nx , dtype = int ) # Rank histogram self . _is_ens = True minN = min ( Nx , N ) do_spectral = np . sqrt ( Nx * N ) <= rc . comp_threshold_b else : self . _is_ens = False minN = Nx do_spectral = Nx <= rc . comp_threshold_b if do_spectral : # Note: the mean-field and RMS time-series of # (i) svals and (ii) umisf should match the corresponding series of # (i) std and (ii) err. self . new_series ( 'svals' , minN ) # Principal component (SVD) scores self . new_series ( 'umisf' , minN ) # Error in component directions ###################################### # Allocate a few series for outside use ###################################### self . new_series ( 'trHK' , 1 , KObs + 1 ) self . new_series ( 'infl' , 1 , KObs + 1 ) self . new_series ( 'iters' , 1 , KObs + 1 ) # Weight-related self . new_series ( 'N_eff' , 1 , KObs + 1 ) self . new_series ( 'wroot' , 1 , KObs + 1 ) self . new_series ( 'resmpl' , 1 , KObs + 1 ) def new_series ( self , name , shape , length = 'FAUSt' , MS = False , ** kws ): \"\"\"Create (and register) a statistics time series. Series are initialized with nan's. Example: Create ndarray of length KObs+1 for inflation time series: >>> self.new_series('infl', 1, KObs+1) NB: The ``sliding_diagnostics`` liveplotting relies on detecting ``nan``'s to avoid plotting stats that are not being used. => Cannot use ``dtype=bool`` or ``int`` for stats that get plotted. \"\"\" # Convert int shape to tuple if not hasattr ( shape , '__len__' ): if shape == 1 : shape = () else : shape = ( shape ,) def make_series ( parent , name , shape ): if length == 'FAUSt' : total_shape = self . K , self . KObs , shape store_opts = self . store_u , self . store_s tseries = series . FAUSt ( * total_shape , * store_opts , ** kws ) else : total_shape = ( length ,) + shape tseries = DataSeries ( total_shape , * kws ) register_stat ( parent , name , tseries ) # Principal series make_series ( self , name , shape ) # Summary (scalar) series: if shape != (): if MS : for suffix in self . field_summaries : make_series ( getattr ( self , name ), suffix , ()) # Make a nested level for sectors if MS == 'sec' : for ss in self . sector_summaries : suffix , sector = ss . split ( '.' ) make_series ( dict_tools . deep_getattr ( self , f \" { name } . { suffix } \" ), sector , ()) @property def data_series ( self ): return [ k for k in vars ( self ) if isinstance ( getattr ( self , k ), DataSeries )] def assess ( self , k , kObs = None , faus = None , E = None , w = None , mu = None , Cov = None ): \"\"\"Common interface for both assess_ens and _ext. The _ens assessment function gets called if E is not None, and _ext if mu is not None. faus: One or more of ['f',' a', 'u'], indicating that the result should be stored in (respectively) the forecast/analysis/universal attribute. Default: 'u' if kObs is None else 'au' ('a' and 'u'). \"\"\" # Initial consistency checks. if k == 0 : if kObs is not None : raise KeyError ( \"DAPPER convention: no obs at t=0.\" \" Helps avoid bugs.\" ) if faus is None : faus = 'u' if self . _is_ens == True : if E is None : raise TypeError ( \"Expected ensemble input but E is None\" ) if mu is not None : raise TypeError ( \"Expected ensemble input but mu/Cov is not None\" ) else : if E is not None : raise TypeError ( \"Expected mu/Cov input but E is not None\" ) if mu is None : raise TypeError ( \"Expected mu/Cov input but mu is None\" ) # Default. Don't add more defaults. It just gets confusing. if faus is None : faus = 'u' if kObs is None else 'au' # Select assessment call and arguments if self . _is_ens : _assess = self . assess_ens _prms = { 'E' : E , 'w' : w } else : _assess = self . assess_ext _prms = { 'mu' : mu , 'P' : Cov } for sub in faus : # Skip assessment if ('u' and stats not stored or plotted) if k != 0 and kObs == None : if not ( self . store_u or self . LP_instance . any_figs ): continue # Silence repeat warnings caused by zero variance with np . errstate ( divide = 'call' , invalid = 'call' ): np . seterrcall ( warn_zero_variance ) # Assess stats_now = Avrgs () _assess ( stats_now , self . xx [ k ], ** _prms ) self . derivative_stats ( stats_now ) self . summarize_marginals ( stats_now ) # Write current stats to series for name , val in stats_now . items (): stat = dict_tools . deep_getattr ( self , name ) isFaust = isinstance ( stat , series . FAUSt ) stat [( k , kObs , sub ) if isFaust else kObs ] = val # LivePlot -- Both init and update must come after the assessment. try : self . LP_instance . update (( k , kObs , sub ), E , Cov ) except AttributeError : self . LP_instance = liveplotting . LivePlot ( self , self . liveplots , ( k , kObs , sub ), E , Cov ) def summarize_marginals ( self , now ): \"Compute Mean-field and RMS values\" formulae = { ** self . field_summaries , ** self . sector_summaries } with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): for stat in list ( now ): field = now [ stat ] for suffix , formula in formulae . items (): statpath = stat + '.' + suffix if dict_tools . deep_hasattr ( self , statpath ): now [ statpath ] = formula ( field ) def derivative_stats ( self , now ): \"\"\"Stats that derive from others (=> not specific for _ens or _ext).\"\"\" now . gscore = 2 * np . log ( now . std ) + ( now . err / now . std ) ** 2 def assess_ens ( self , now , x , E , w ): \"\"\"Ensemble and Particle filter (weighted/importance) assessment.\"\"\" N , Nx = E . shape if w is None : w = np . ones ( N ) / N # All equal. Also, rm attr from stats: if hasattr ( self , 'w' ): delattr ( self , 'w' ) else : now . w = w if abs ( w . sum () - 1 ) > 1e-5 : raise RuntimeError ( \"Weights did not sum to one.\" ) if not np . all ( np . isfinite ( E )): raise RuntimeError ( \"Ensemble not finite.\" ) if not np . all ( np . isreal ( E )): raise RuntimeError ( \"Ensemble not Real.\" ) now . mu = w @ E now . err = now . mu - x A = E - now . mu # While A**2 is approx as fast as A*A, # A**3 is 10x slower than A**2 (or A**2.0). # => Use A2 = A**2, A3 = A*A2, A4=A*A3. # But, to save memory, only use A_pow. A_pow = A ** 2 # Compute variances var = w @ A_pow ub = unbias_var ( w , avoid_pathological = True ) var *= ub # Compute standard deviation (\"Spread\") std = np . sqrt ( var ) # NB: biased (even though var is unbiased) now . std = std # For simplicity, use naive (biased) formulae, derived # from \"empirical measure\". See doc/unbiased_skew_kurt.jpg. # Normalize by var. Compute \"excess\" kurt, which is 0 for Gaussians. A_pow *= A now . skew = np . nanmean ( w @ A_pow / ( std * std * std )) A_pow *= A now . kurt = np . nanmean ( w @ A_pow / var ** 2 - 3 ) now . mad = np . nanmean ( w @ abs ( A )) if hasattr ( self , 'svals' ): if N <= Nx : _ , s , UT = sla . svd (( np . sqrt ( w ) * A . T ) . T , full_matrices = False ) s *= np . sqrt ( ub ) # Makes s^2 unbiased now . svals = s now . umisf = UT @ now . err else : P = ( A . T * w ) @ A s2 , U = sla . eigh ( P ) s2 *= ub now . svals = np . sqrt ( s2 . clip ( 0 ))[:: - 1 ] now . umisf = U . T [:: - 1 ] @ now . err # For each state dim [i], compute rank of truth (x) among the ensemble (E) E_x = np . sort ( np . vstack (( E , x )), axis = 0 , kind = 'heapsort' ) now . rh = np . asarray ( [ np . where ( E_x [:, i ] == x [ i ])[ 0 ][ 0 ] for i in range ( Nx )]) def assess_ext ( self , now , x , mu , P ): \"\"\"Kalman filter (Gaussian) assessment.\"\"\" if not np . all ( np . isfinite ( mu )): raise RuntimeError ( \"Estimates not finite.\" ) if not np . all ( np . isreal ( mu )): raise RuntimeError ( \"Estimates not Real.\" ) # Don't check the cov (might not be explicitly availble) now . mu = mu now . err = now . mu - x var = P . diag if isinstance ( P , CovMat ) else np . diag ( P ) now . std = np . sqrt ( var ) # Here, sqrt(2/pi) is the ratio, of MAD/STD for Gaussians now . mad = np . nanmean ( now . std ) * np . sqrt ( 2 / np . pi ) if hasattr ( self , 'svals' ): P = P . full if isinstance ( P , CovMat ) else P s2 , U = sla . eigh ( P ) now . svals = np . sqrt ( np . maximum ( s2 , 0.0 ))[:: - 1 ] now . umisf = ( U . T @ now . err )[:: - 1 ] def average_in_time ( self , kk = None , kkObs = None , free = False ): \"\"\"Avarage all univariate (scalar) time series. - ``kk`` time inds for averaging - ``kkObs`` time inds for averaging obs \"\"\" chrono = self . HMM . t if kk is None : kk = chrono . mask_BI if kkObs is None : kkObs = chrono . maskObs_BI def average1 ( tseries ): avrgs = Avrgs () def average_multivariate (): return avrgs # Plain averages of nd-series are rarely interesting. # => Shortcircuit => Leave for manual computations if isinstance ( tseries , series . FAUSt ): # Average series for each subscript if tseries . item_shape != (): return average_multivariate () for sub in [ ch for ch in 'fas' if hasattr ( tseries , ch )]: avrgs [ sub ] = series . mean_with_conf ( tseries [ kkObs , sub ]) if tseries . store_u : avrgs [ 'u' ] = series . mean_with_conf ( tseries [ kk , 'u' ]) elif isinstance ( tseries , DataSeries ): if tseries . array . shape [ 1 :] != (): return average_multivariate () elif len ( tseries . array ) == self . KObs + 1 : avrgs = series . mean_with_conf ( tseries [ kkObs ]) elif len ( tseries . array ) == self . K + 1 : avrgs = series . mean_with_conf ( tseries [ kk ]) else : raise ValueError elif np . isscalar ( tseries ): avrgs = tseries # Eg. just copy over \"duration\" from stats else : raise TypeError ( f \"Don't know how to average { tseries } \" ) return avrgs def recurse_average ( stat_parent , avrgs_parent ): for key in getattr ( stat_parent , \"stat_register\" , []): try : tseries = getattr ( stat_parent , key ) except AttributeError : continue # Eg assess_ens() deletes .weights if None avrgs = average1 ( tseries ) recurse_average ( tseries , avrgs ) avrgs_parent [ key ] = avrgs avrgs = Avrgs () recurse_average ( self , avrgs ) self . xp . avrgs = avrgs if free : delattr ( self . xp , 'stats' ) def replay ( self , figlist = \"default\" , speed = np . inf , t1 = 0 , t2 = None , ** kwargs ): \"\"\"Replay LivePlot with what's been stored in 'self'. - t1, t2: time window to plot. - 'figlist' and 'speed': See LivePlot's doc. .. note:: ``store_u`` (whether to store non-obs-time stats) must have been ``True`` to have smooth graphs as in the actual LivePlot. .. note:: Ensembles are generally not stored in the stats and so cannot be replayed. \"\"\" # Time settings chrono = self . HMM . t if t2 is None : t2 = t1 + chrono . Tplot # Ens does not get stored in stats, so we cannot replay that. # If the LPs are initialized with P0!=None, then they will avoid ens plotting. # TODO 2: This system for switching from Ens to stats must be replaced. # It breaks down when M is very large. try : P0 = np . full_like ( self . HMM . X0 . C . full , np . nan ) except AttributeError : # e.g. if X0 is defined via sampling func P0 = np . eye ( self . HMM . Nx ) LP = liveplotting . LivePlot ( self , figlist , P = P0 , speed = speed , Tplot = t2 - t1 , replay = True , ** kwargs ) plt . pause ( . 01 ) # required when speed=inf # Remember: must use progbar to unblock read1. # Let's also make a proper description. desc = self . xp . da_method + \" (replay)\" # Play through assimilation cycles for k , kObs , t , dt in utils . progbar ( chrono . ticker , desc ): if t1 <= t <= t2 : if kObs is not None : LP . update (( k , kObs , 'f' ), None , None ) LP . update (( k , kObs , 'a' ), None , None ) LP . update (( k , kObs , 'u' ), None , None ) # Pause required when speed=inf. # On Mac, it was also necessary to do it for each fig. for name , ( num , updater ) in LP . figures . items (): if plt . fignum_exists ( num ) and getattr ( updater , 'is_active' , 1 ): plt . figure ( num ) plt . pause ( 0.01 ) def register_stat ( self , name , value ): setattr ( self , name , value ) if not hasattr ( self , \"stat_register\" ): self . stat_register = [] self . stat_register . append ( name ) class Avrgs ( StatPrint , DotDict ): \"\"\"A DotDict specialized for stat. averages. Embellishments: - StatPrint - tabulate - getattr that supports abbreviations. \"\"\" def tabulate ( self , statkeys = ()): columns = tabulate_avrgs ([ self ], statkeys , decimals = None ) return utils . tab ( columns , headers = \"keys\" ) . replace ( '\u2423' , ' ' ) abbrevs = { 'rmse' : 'err.rms' , 'rmss' : 'std.rms' , 'rmv' : 'std.rms' } # Use getattribute coz it gets called before getattr. def __getattribute__ ( self , key ): \"\"\"Support deep and abbreviated lookup.\"\"\" # key = abbrevs[key] # Instead of this, also support rmse.a: key = '.' . join ( Avrgs . abbrevs . get ( seg , seg ) for seg in key . split ( '.' )) if \".\" in key : return dict_tools . deep_getattr ( self , key ) else : return super () . __getattribute__ ( key ) # In case of degeneracy, variance might be 0, causing warnings # in computing skew/kurt/MGLS (which all normalize by variance). # This should and will yield nan's, but we don't want mere diagnostics # computations to cause repetitive warnings, so we only warn once. # # I would have expected this (more elegant solution?) to work, # but it just makes it worse. # with np.errstate(divide='warn',invalid='warn'), warnings.catch_warnings(): # warnings.simplefilter(\"once\",category=RuntimeWarning) # ... @utils . do_once def warn_zero_variance ( err , flag ): msg = \" \\n \" . join ([ \"Numerical error in stat comps.\" , \"Probably caused by a sample variance of 0.\" ]) warnings . warn ( msg ) # Why not do all columns at once using the tabulate module? Coz # - Want subcolumns, including fancy formatting (e.g. +/-) # - Want separation (using '|') of attr and stats # - ... def tabulate_column ( col , header , pad = '\u2423' , missingval = '' , frmt = None ): \"\"\"Format a single column, return as list. - Use tabulate() to get decimal point alignment. - Inf and nan are handled individually so that they don't align left of the decimal point (makes too wide columns). Custom ``frmt`` also supported. - Pad (on the right) each row so that the widths are equal. \"\"\" def preprocess ( x ): try : # Custom frmt supplied if frmt is not None : return frmt ( x ) # Standard formatting if x is None : return missingval elif np . isnan ( x ): return \"NAX\" elif x == - np . inf : return \"-INX\" elif x == np . inf : return \"INX\" return x # leave formatting to tabulate() except TypeError : return missingval def postprocess ( s ): s = s . replace ( \"NAX\" , \"nan\" ) s = s . replace ( \"INX\" , \"inf\" ) return s # Make text column, aligned col = [[ preprocess ( x )] for x in col ] col = utils . tab ( col , [ header ], 'plain' ) col = col . split ( \" \\n \" ) # NOTE: dont use splitlines (removes empty lines) # Undo nan/inf treatment col = [ postprocess ( s ) for s in col ] # Pad on the right, for equal widths mxW = max ( len ( s ) for s in col ) col = [ s . ljust ( mxW ) for s in col ] # Use pad char. on BOTH left/right, to prevent trunc. by later tabulate(). col = [ s . replace ( \" \" , pad ) for s in col ] return col def unpack_uqs ( uq_list , decimals = None , cols = ( \"val\" , \"conf\" )): \"\"\"Make array whose (named) cols are ``[uq.col for uq in uq_list]``. Embellishments: - Insert None (in each col) if uq is None. - Apply uq.round() when extracting val & conf. \"\"\" def unpack1 ( arr , i , uq ): if uq is None : return # val/conf if decimals is None : v , c = uq . round ( mult = 0.1 ) else : v , c = np . round ([ uq . val , uq . conf ], decimals ) arr [ \"val\" ][ i ], arr [ \"conf\" ][ i ] = v , c # Others for col in dict_tools . complement ( cols , [ \"val\" , \"conf\" ]): try : arr [ col ][ i ] = getattr ( uq , col ) except AttributeError : pass # np.array with named columns. \"O\" => allow storing None's. dtypes = np . dtype ([( c , \"O\" ) for c in cols ]) arr = np . full_like ( uq_list , dtype = dtypes , fill_value = None ) for i , uq in enumerate ( uq_list ): unpack1 ( arr , i , uq ) return arr def tabulate_avrgs ( avrgs_list , statkeys = (), decimals = None ): \"\"\"Tabulate avrgs (val\u00b1conf).\"\"\" if not statkeys : statkeys = [ 'rmse.a' , 'rmv.a' , 'rmse.f' ] columns = {} for stat in statkeys : column = unpack_uqs ( [ getattr ( a , stat , None ) for a in avrgs_list ], decimals ) vals = tabulate_column ( column [ \"val\" ], stat ) confs = tabulate_column ( column [ \"conf\" ], '1\u03c3' ) headr = vals [ 0 ] + ' 1\u03c3' mattr = [ v + ' \u00b1' + c for v , c in zip ( vals , confs )][ 1 :] columns [ headr ] = mattr return columns Functions register_stat def register_stat ( self , name , value ) View Source def register_stat ( self , name , value ) : setattr ( self , name , value ) if not hasattr ( self , \"stat_register\" ) : self . stat_register = [] self . stat_register . append ( name ) tabulate_avrgs def tabulate_avrgs ( avrgs_list , statkeys = (), decimals = None ) Tabulate avrgs (val\u00b1conf). View Source def tabulate_avrgs ( avrgs_list , statkeys = (), decimals = None ) : \"\"\"Tabulate avrgs (val\u00b1conf).\"\"\" if not statkeys : statkeys = [ 'rmse.a', 'rmv.a', 'rmse.f' ] columns = {} for stat in statkeys : column = unpack_uqs ( [ getattr(a, stat, None) for a in avrgs_list ] , decimals ) vals = tabulate_column ( column [ \"val\" ] , stat ) confs = tabulate_column ( column [ \"conf\" ] , '1\u03c3' ) headr = vals [ 0 ]+ ' 1\u03c3' mattr = [ v + ' \u00b1'+c for v, c in zip(vals, confs) ][ 1: ] columns [ headr ] = mattr return columns tabulate_column def tabulate_column ( col , header , pad = '\u2423' , missingval = '' , frmt = None ) Format a single column, return as list. Use tabulate() to get decimal point alignment. Inf and nan are handled individually so that they don't align left of the decimal point (makes too wide columns). Custom frmt also supported. Pad (on the right) each row so that the widths are equal. View Source def tabulate_column ( col , header , pad = '\u2423' , missingval = '' , frmt = None ) : \" \"\" Format a single column, return as list. - Use tabulate() to get decimal point alignment. - Inf and nan are handled individually so that they don't align left of the decimal point (makes too wide columns). Custom ``frmt`` also supported. - Pad (on the right) each row so that the widths are equal. \"\" \" def preprocess ( x ) : try : # Custom frmt supplied if frmt is not None : return frmt ( x ) # Standard formatting if x is None : return missingval elif np . isnan ( x ) : return \"NAX\" elif x == - np . inf : return \"-INX\" elif x == np . inf : return \"INX\" return x # leave formatting to tabulate() except TypeError : return missingval def postprocess ( s ) : s = s . replace ( \"NAX\" , \"nan\" ) s = s . replace ( \"INX\" , \"inf\" ) return s # Make text column, aligned col = [[ preprocess ( x ) ] for x in col ] col = utils . tab ( col , [ header ] , 'plain' ) col = col . split ( \" \\n \" ) # NOTE: dont use splitlines (removes empty lines) # Undo nan/inf treatment col = [ postprocess ( s ) for s in col ] # Pad on the right, for equal widths mxW = max ( len ( s ) for s in col ) col = [ s . ljust ( mxW ) for s in col ] # Use pad char. on BOTH left/right, to prevent trunc. by later tabulate(). col = [ s . replace ( \" \" , pad ) for s in col ] return col unpack_uqs def unpack_uqs ( uq_list , decimals = None , cols = ( 'val' , 'conf' ) ) Make array whose (named) cols are [uq.col for uq in uq_list] . Embellishments: - Insert None (in each col) if uq is None. - Apply uq.round() when extracting val & conf. View Source def unpack_uqs ( uq_list , decimals = None , cols = ( \"val\" , \"conf\" )) : \"\"\"Make array whose (named) cols are ``[uq.col for uq in uq_list]``. Embellishments: - Insert None (in each col) if uq is None. - Apply uq.round() when extracting val & conf. \"\"\" def unpack1 ( arr , i , uq ) : if uq is None : return # val / conf if decimals is None : v , c = uq . round ( mult = 0.1 ) else : v , c = np . round ( [ uq.val, uq.conf ] , decimals ) arr [ \"val\" ][ i ] , arr [ \"conf\" ][ i ] = v , c # Others for col in dict_tools . complement ( cols , [ \"val\", \"conf\" ] ) : try : arr [ col ][ i ] = getattr ( uq , col ) except AttributeError : pass # np . array with named columns . \"O\" => allow storing None ' s . dtypes = np . dtype ( [ (c, \"O\") for c in cols ] ) arr = np . full_like ( uq_list , dtype = dtypes , fill_value = None ) for i , uq in enumerate ( uq_list ) : unpack1 ( arr , i , uq ) return arr Classes Avrgs class Avrgs ( * args , ** kwargs ) A DotDict specialized for stat. averages. Embellishments: - StatPrint - tabulate - getattr that supports abbreviations. View Source class Avrgs ( StatPrint , DotDict ) : \"\"\"A DotDict specialized for stat. averages. Embellishments: - StatPrint - tabulate - getattr that supports abbreviations. \"\"\" def tabulate ( self , statkeys = ()) : columns = tabulate_avrgs ( [ self ] , statkeys , decimals = None ) return utils . tab ( columns , headers = \"keys\" ). replace ( '\u2423' , ' ' ) abbrevs = { 'rmse' : 'err.rms' , 'rmss' : 'std.rms' , 'rmv' : 'std.rms' } # Use getattribute coz it gets called before getattr . def __getattribute__ ( self , key ) : \"\"\"Support deep and abbreviated lookup.\"\"\" # key = abbrevs [ key ] # Instead of this , also support rmse . a : key = '.' . join ( Avrgs . abbrevs . get ( seg , seg ) for seg in key . split ( '.' )) if \".\" in key : return dict_tools . deep_getattr ( self , key ) else : return super (). __getattribute__ ( key ) Ancestors (in MRO) dapper.tools.series.StatPrint dapper.dict_tools.NicePrint dapper.dict_tools.DotDict dapper.dict_tools.AlignedDict builtins.dict Class variables abbrevs printopts Methods clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. tabulate def tabulate ( self , statkeys = () ) View Source def tabulate ( self , statkeys = ()) : columns = tabulate_avrgs ( [ self ] , statkeys , decimals = None ) return utils . tab ( columns , headers = \"keys\" ). replace ( '\u2423' , ' ' ) update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] values def values ( ... ) D.values() -> an object providing a view on D's values Stats class Stats ( xp , HMM , xx , yy , liveplots = False , store_u = False ) Contains and computes statistics of the DA methods. Use new_series() to register your own stat time series. View Source class Stats ( StatPrint ) : \"\"\"Contains and computes statistics of the DA methods. Use new_series() to register your own stat time series. \"\"\" def __ init__ ( self , xp , HMM , xx , yy , liveplots = False , store_u = rc . store_u ) : \"\"\"Init the default statistics. Note: Python allows dynamically creating attributes, so you can easily add custom stat. series to a Stat instance within a particular method, for example. Use ``new_series`` to get automatic averaging too. \"\"\" ###################################### # Preamble ###################################### self . xp = xp self . HMM = HMM self . xx = xx self . yy = yy self . liveplots = liveplots self . store_u = store_u self . store_s = hasattr ( xp , 'Lag' ) # Shapes K = xx . shape [ 0 ] - 1 Nx = xx . shape [ 1 ] KObs = yy . shape [ 0 ] - 1 Ny = yy . shape [ 1 ] self . K , self . Nx = K , Nx self . KObs , self . Ny = KObs , Ny # Methods for summarizing multivariate stats ( \"fields\" ) as scalars # Don 't use nanmean here; nan's should get propagated ! self . field_summaries = dict ( m = lambda x : np . mean ( x ), # mean - field rms = lambda x : np . sqrt ( np . mean ( x** 2 )), # root - mean - square ma = lambda x : np . mean ( np . abs ( x )), # mean - absolute gm = lambda x : np . exp ( np . mean ( np . log ( x ))), # geometric mean ) # Only keep the methods listed in rc self . field_summaries = dict_tools . intersect ( self . field_summaries , rc . field_summaries ) # Define similar methods , but restricted to sectors self . sector_summaries = {} def restrict ( fun , inds ) : return ( lambda x : fun ( x [ inds ])) for suffix , formula in self . field_summaries . items () : for sector , inds in HMM . sectors . items () : f = restrict ( formula , inds ) self . sector_summaries [ '%s.%s' % (suffix, sector)] = f ###################################### # Allocate time series of various stats ###################################### self . new_series ( 'mu' , Nx , MS ='sec' ) # Mean self . new_series ( 'std' , Nx , MS ='sec' ) # Std . dev . ( \"spread\" ) self . new_series ( 'err' , Nx , MS ='sec' ) # Error ( mu - truth ) self . new_series ( 'gscore' , Nx , MS ='sec' ) # Gaussian ( log ) score # To save memory , we only store these field means : self . new_series ( 'mad' , 1 ) # Mean abs deviations self . new_series ( 'skew' , 1 ) # Skewness self . new_series ( 'kurt' , 1 ) # Kurtosis if hasattr ( xp , 'N' ) : N = xp . N self . new_series ( 'w' , N , MS = True ) # Importance weights self . new_series ( 'rh' , Nx , dtype = int ) # Rank histogram self . _ is_ens = True minN = min ( Nx , N ) do_spectral = np . sqrt ( Nx * N ) <= rc . comp_threshold_b else : self . _ is_ens = False minN = Nx do_spectral = Nx <= rc . comp_threshold_b if do_spectral: # Note : the mean - field and RMS time - series of # ( i ) svals and ( ii ) umisf should match the corresponding series of # ( i ) std and ( ii ) err . self . new_series ( 'svals' , minN ) # Principal component ( SVD ) scores self . new_series ( 'umisf' , minN ) # Error in component directions ###################################### # Allocate a few series for outside use ###################################### self . new_series ( 'trHK' , 1 , KObs + 1 ) self . new_series ( 'infl' , 1 , KObs + 1 ) self . new_series ( 'iters' , 1 , KObs + 1 ) # Weight - related self . new_series ( 'N_eff' , 1 , KObs + 1 ) self . new_series ( 'wroot' , 1 , KObs + 1 ) self . new_series ( 'resmpl' , 1 , KObs + 1 ) def new_series ( self , name , shape , length='FAUSt' , MS = False , **kws ) : \"\"\"Create (and register) a statistics time series. Series are initialized with nan's. Example: Create ndarray of length KObs+1 for inflation time series: >>> self.new_series('infl', 1, KObs+1) NB: The ``sliding_diagnostics`` liveplotting relies on detecting ``nan``'s to avoid plotting stats that are not being used. => Cannot use ``dtype=bool`` or ``int`` for stats that get plotted. \"\"\" # Convert int shape to tuple if not hasattr ( shape , '__len__' ) : if shape == 1 : shape = () else : shape = ( shape ,) def make_series ( parent , name , shape ) : if length == 'FAUSt': total_shape = self . K , self . KObs , shape store_opts = self . store_u , self . store_s tseries = series . FAUSt ( * total_shape , * store_opts , **kws ) else : total_shape = ( length ,) + shape tseries = DataSeries ( total_shape , * kws ) register_stat ( parent , name , tseries ) # Principal series make_series ( self , name , shape ) # Summary ( scalar ) series : if shape ! = () : if MS : for suffix in self . field_summaries: make_series ( getattr ( self , name ), suffix , ()) # Make a nested level for sectors if MS == 'sec': for ss in self . sector_summaries: suffix , sector = ss . split ( '.' ) make_series ( dict_tools . deep_getattr ( self , f \"{name}.{suffix}\" ), sector , ()) @property def data_series ( self ) : return [ k for k in vars ( self ) if isinstance ( getattr ( self , k ), DataSeries )] def assess ( self , k , kObs = None , faus = None , E = None , w = None , mu = None , Cov = None ) : \"\"\"Common interface for both assess_ens and _ext. The _ens assessment function gets called if E is not None, and _ext if mu is not None. faus: One or more of ['f',' a', 'u'], indicating that the result should be stored in (respectively) the forecast/analysis/universal attribute. Default: 'u' if kObs is None else 'au' ('a' and 'u'). \"\"\" # Initial consistency checks . if k == 0 : if kObs is not None : raise KeyError ( \"DAPPER convention: no obs at t=0.\" \" Helps avoid bugs.\" ) if faus is None : faus = 'u' if self . _ is_ens == True : if E is None : raise TypeError ( \"Expected ensemble input but E is None\" ) if mu is not None : raise TypeError ( \"Expected ensemble input but mu/Cov is not None\" ) else : if E is not None : raise TypeError ( \"Expected mu/Cov input but E is not None\" ) if mu is None : raise TypeError ( \"Expected mu/Cov input but mu is None\" ) # Default . Don 't add more defaults. It just gets confusing. if faus is None: faus = 'u' if kObs is None else 'au' # Select assessment call and arguments if self._is_ens: _assess = self.assess_ens _prms = {' E ': E, 'w': w} else: _assess = self.assess_ext _prms = {'mu': mu, ' P ': Cov} for sub in faus: # Skip assessment if ('u' and stats not stored or plotted) if k != 0 and kObs == None: if not (self.store_u or self.LP_instance.any_figs): continue # Silence repeat warnings caused by zero variance with np.errstate(divide='call', invalid='call'): np.seterrcall(warn_zero_variance) # Assess stats_now = Avrgs() _assess(stats_now, self.xx[k], **_prms) self.derivative_stats(stats_now) self.summarize_marginals(stats_now) # Write current stats to series for name, val in stats_now.items(): stat = dict_tools.deep_getattr(self, name) isFaust = isinstance(stat, series.FAUSt) stat[(k, kObs, sub) if isFaust else kObs] = val # LivePlot -- Both init and update must come after the assessment. try: self.LP_instance.update((k, kObs, sub), E, Cov) except AttributeError: self.LP_instance = liveplotting.LivePlot( self, self.liveplots, (k, kObs, sub), E, Cov) def summarize_marginals(self, now): \"Compute Mean-field and RMS values\" formulae = {**self.field_summaries, **self.sector_summaries} with np.errstate(divide='ignore', invalid='ignore'): for stat in list(now): field = now[stat] for suffix, formula in formulae.items(): statpath = stat+' . '+suffix if dict_tools.deep_hasattr(self, statpath): now[statpath] = formula(field) def derivative_stats(self, now): \"\"\"Stats that derive from others (=> not specific for _ens or _ext).\"\"\" now.gscore = 2*np.log(now.std) + (now.err/now.std)**2 def assess_ens(self, now, x, E, w): \"\"\"Ensemble and Particle filter (weighted/importance) assessment.\"\"\" N, Nx = E.shape if w is None: w = np.ones(N)/N # All equal. Also, rm attr from stats: if hasattr(self, 'w'): delattr(self, 'w') else: now.w = w if abs(w.sum()-1) > 1e-5: raise RuntimeError(\"Weights did not sum to one.\") if not np.all(np.isfinite(E)): raise RuntimeError(\"Ensemble not finite.\") if not np.all(np.isreal(E)): raise RuntimeError(\"Ensemble not Real.\") now.mu = w @ E now.err = now.mu - x A = E - now.mu # While A**2 is approx as fast as A*A, # A**3 is 10x slower than A**2 (or A**2.0). # => Use A2 = A**2, A3 = A*A2, A4=A*A3. # But, to save memory, only use A_pow. A_pow = A**2 # Compute variances var = w @ A_pow ub = unbias_var(w, avoid_pathological=True) var *= ub # Compute standard deviation (\"Spread\") std = np.sqrt(var) # NB: biased (even though var is unbiased) now.std = std # For simplicity, use naive (biased) formulae, derived # from \"empirical measure\". See doc/unbiased_skew_kurt.jpg. # Normalize by var. Compute \"excess\" kurt, which is 0 for Gaussians. A_pow *= A now.skew = np.nanmean(w @ A_pow / (std*std*std)) A_pow *= A now.kurt = np.nanmean(w @ A_pow / var**2 - 3) now.mad = np.nanmean(w @ abs(A)) if hasattr(self, 'svals'): if N <= Nx: _, s, UT = sla.svd((np.sqrt(w)*A.T).T, full_matrices=False) s *= np.sqrt(ub) # Makes s^2 unbiased now.svals = s now.umisf = UT @ now.err else: P = (A.T * w) @ A s2, U = sla.eigh(P) s2 *= ub now.svals = np.sqrt(s2.clip(0))[::-1] now.umisf = U.T[::-1] @ now.err # For each state dim [i], compute rank of truth (x) among the ensemble (E) E_x = np.sort(np.vstack((E, x)), axis=0, kind='heapsort') now.rh = np.asarray( [np.where(E_x[:, i] == x[i])[0][0] for i in range(Nx)]) def assess_ext(self, now, x, mu, P): \"\"\"Kalman filter (Gaussian) assessment.\"\"\" if not np.all(np.isfinite(mu)): raise RuntimeError(\"Estimates not finite.\") if not np.all(np.isreal(mu)): raise RuntimeError(\"Estimates not Real.\") # Don't check the cov ( might not be explicitly availble ) now . mu = mu now . err = now . mu - x var = P . diag if isinstance ( P , CovMat ) else np . diag ( P ) now . std = np . sqrt ( var ) # Here , sqrt ( 2 / pi ) is the ratio , of MAD / STD for Gaussians now . mad = np . nanmean ( now . std ) * np . sqrt ( 2 / np . pi ) if hasattr ( self , 'svals' ) : P = P . full if isinstance ( P , CovMat ) else P s2 , U = sla . eigh ( P ) now . svals = np . sqrt ( np . maximum ( s2 , 0.0 ))[ ::- 1 ] now . umisf = ( U . T @ now . err )[ ::- 1 ] def average_in_time ( self , kk = None , kkObs = None , free = False ) : \"\"\"Avarage all univariate (scalar) time series. - ``kk`` time inds for averaging - ``kkObs`` time inds for averaging obs \"\"\" chrono = self . HMM . t if kk is None : kk = chrono . mask_BI if kkObs is None : kkObs = chrono . maskObs_BI def average1 ( tseries ) : avrgs = Avrgs () def average_multivariate () : return avrgs # Plain averages of nd - series are rarely interesting . # => Shortcircuit => Leave for manual computations if isinstance ( tseries , series . FAUSt ) : # Average series for each subscript if tseries . item_shape ! = () : return average_multivariate () for sub in [ ch for ch in 'fas' if hasattr ( tseries , ch )] : avrgs [ sub ] = series . mean_with_conf ( tseries [ kkObs , sub ]) if tseries . store_u: avrgs [ 'u' ] = series . mean_with_conf ( tseries [ kk , 'u' ]) elif isinstance ( tseries , DataSeries ) : if tseries . array . shape [ 1 : ] ! = () : return average_multivariate () elif len ( tseries . array ) == self . KObs + 1 : avrgs = series . mean_with_conf ( tseries [ kkObs ]) elif len ( tseries . array ) == self . K + 1 : avrgs = series . mean_with_conf ( tseries [ kk ]) else : raise ValueError elif np . isscalar ( tseries ) : avrgs = tseries # Eg . just copy over \"duration\" from stats else : raise TypeError ( f \"Don't know how to average {tseries}\" ) return avrgs def recurse_average ( stat_parent , avrgs_parent ) : for key in getattr ( stat_parent , \"stat_register\" , []) : try : tseries = getattr ( stat_parent , key ) except AttributeError : continue # Eg assess_ens () deletes . weights if None avrgs = average1 ( tseries ) recurse_average ( tseries , avrgs ) avrgs_parent [ key ] = avrgs avrgs = Avrgs () recurse_average ( self , avrgs ) self . xp . avrgs = avrgs if free : delattr ( self . xp , 'stats' ) def replay ( self , figlist= \"default\" , speed = np . inf , t1 = 0 , t2 = None , **kwargs ) : \"\"\"Replay LivePlot with what's been stored in 'self'. - t1, t2: time window to plot. - 'figlist' and 'speed': See LivePlot's doc. .. note:: ``store_u`` (whether to store non-obs-time stats) must have been ``True`` to have smooth graphs as in the actual LivePlot. .. note:: Ensembles are generally not stored in the stats and so cannot be replayed. \"\"\" # Time settings chrono = self . HMM . t if t2 is None : t2 = t1 + chrono . Tplot # Ens does not get stored in stats , so we cannot replay that . # If the LPs are initialized with P0 ! = None , then they will avoid ens plotting . # TODO 2 : This system for switching from Ens to stats must be replaced . # It breaks down when M is very large . try : P0 = np . full_like ( self . HMM . X0 . C . full , np . nan ) except AttributeError : # e . g . if X0 is defined via sampling func P0 = np . eye ( self . HMM . Nx ) LP = liveplotting . LivePlot ( self , figlist , P = P0 , speed = speed , Tplot = t2 - t1 , replay = True , **kwargs ) plt . pause ( .01 ) # required when speed = inf # Remember : must use progbar to unblock read1 . # Let 's also make a proper description. desc = self.xp.da_method + \" (replay)\" # Play through assimilation cycles for k, kObs, t, dt in utils.progbar(chrono.ticker, desc): if t1 <= t <= t2: if kObs is not None: LP.update((k, kObs, 'f'), None, None) LP.update((k, kObs, 'a'), None, None) LP.update((k, kObs, 'u'), None, None) # Pause required when speed=inf. # On Mac, it was also necessary to do it for each fig. for name, (num, updater) in LP.figures.items(): if plt.fignum_exists(num) and getattr(updater, 'is_active ' , 1 ) : plt . figure ( num ) plt . pause ( 0.01 ) Ancestors (in MRO) dapper.tools.series.StatPrint dapper.dict_tools.NicePrint Class variables printopts Instance variables data_series Methods assess def assess ( self , k , kObs = None , faus = None , E = None , w = None , mu = None , Cov = None ) Common interface for both assess_ens and _ext. The _ens assessment function gets called if E is not None, and _ext if mu is not None. faus: One or more of ['f',' a', 'u'], indicating that the result should be stored in (respectively) the forecast/analysis/universal attribute. Default: 'u' if kObs is None else 'au' ('a' and 'u'). View Source def assess ( self , k , kObs = None , faus = None , E = None , w = None , mu = None , Cov = None ) : \"\"\"Common interface for both assess_ens and _ext. The _ens assessment function gets called if E is not None, and _ext if mu is not None. faus: One or more of ['f',' a', 'u'], indicating that the result should be stored in (respectively) the forecast/analysis/universal attribute. Default: 'u' if kObs is None else 'au' ('a' and 'u'). \"\"\" # Initial consistency checks . if k == 0 : if kObs is not None : raise KeyError ( \"DAPPER convention: no obs at t=0.\" \" Helps avoid bugs.\" ) if faus is None : faus = 'u' if self . _is_ens == True : if E is None : raise TypeError ( \"Expected ensemble input but E is None\" ) if mu is not None : raise TypeError ( \"Expected ensemble input but mu/Cov is not None\" ) else : if E is not None : raise TypeError ( \"Expected mu/Cov input but E is not None\" ) if mu is None : raise TypeError ( \"Expected mu/Cov input but mu is None\" ) # Default . Don 't add more defaults. It just gets confusing. if faus is None: faus = ' u ' if kObs is None else ' au ' # Select assessment call and arguments if self._is_ens: _assess = self.assess_ens _prms = {' E ': E, ' w ': w} else: _assess = self.assess_ext _prms = {' mu ': mu, ' P ': Cov} for sub in faus: # Skip assessment if (' u ' and stats not stored or plotted) if k != 0 and kObs == None: if not (self.store_u or self.LP_instance.any_figs): continue # Silence repeat warnings caused by zero variance with np.errstate(divide=' call ', invalid=' call ' ) : np . seterrcall ( warn_zero_variance ) # Assess stats_now = Avrgs () _assess ( stats_now , self . xx [ k ] , ** _prms ) self . derivative_stats ( stats_now ) self . summarize_marginals ( stats_now ) # Write current stats to series for name , val in stats_now . items () : stat = dict_tools . deep_getattr ( self , name ) isFaust = isinstance ( stat , series . FAUSt ) stat [ (k, kObs, sub) if isFaust else kObs ] = val # LivePlot -- Both init and update must come after the assessment . try : self . LP_instance . update (( k , kObs , sub ), E , Cov ) except AttributeError : self . LP_instance = liveplotting . LivePlot ( self , self . liveplots , ( k , kObs , sub ), E , Cov ) assess_ens def assess_ens ( self , now , x , E , w ) Ensemble and Particle filter (weighted/importance) assessment. View Source def assess_ens ( self , now , x , E , w ) : \"\"\"Ensemble and Particle filter (weighted/importance) assessment.\"\"\" N , Nx = E . shape if w is None : w = np . ones ( N ) / N # All equal . Also , rm attr from stats : if hasattr ( self , 'w' ) : delattr ( self , 'w' ) else : now . w = w if abs ( w . sum () - 1 ) > 1 e - 5 : raise RuntimeError ( \"Weights did not sum to one.\" ) if not np . all ( np . isfinite ( E )) : raise RuntimeError ( \"Ensemble not finite.\" ) if not np . all ( np . isreal ( E )) : raise RuntimeError ( \"Ensemble not Real.\" ) now . mu = w @ E now . err = now . mu - x A = E - now . mu # While A ** 2 is approx as fast as A * A , # A ** 3 is 10 x slower than A ** 2 ( or A ** 2.0 ). # => Use A2 = A ** 2 , A3 = A * A2 , A4 = A * A3 . # But , to save memory , only use A_pow . A_pow = A ** 2 # Compute variances var = w @ A_pow ub = unbias_var ( w , avoid_pathological = True ) var *= ub # Compute standard deviation ( \"Spread\" ) std = np . sqrt ( var ) # NB : biased ( even though var is unbiased ) now . std = std # For simplicity , use naive ( biased ) formulae , derived # from \"empirical measure\" . See doc / unbiased_skew_kurt . jpg . # Normalize by var . Compute \"excess\" kurt , which is 0 for Gaussians . A_pow *= A now . skew = np . nanmean ( w @ A_pow / ( std * std * std )) A_pow *= A now . kurt = np . nanmean ( w @ A_pow / var** 2 - 3 ) now . mad = np . nanmean ( w @ abs ( A )) if hasattr ( self , 'svals' ) : if N <= Nx : _ , s , UT = sla . svd (( np . sqrt ( w ) * A . T ). T , full_matrices = False ) s *= np . sqrt ( ub ) # Makes s^ 2 unbiased now . svals = s now . umisf = UT @ now . err else : P = ( A . T * w ) @ A s2 , U = sla . eigh ( P ) s2 *= ub now . svals = np . sqrt ( s2 . clip ( 0 ))[ ::- 1 ] now . umisf = U . T [ ::- 1 ] @ now . err # For each state dim [ i ], compute rank of truth ( x ) among the ensemble ( E ) E_x = np . sort ( np . vstack (( E , x )), axis = 0 , kind='heapsort' ) now . rh = np . asarray ( [ np . where ( E_x [ : , i ] == x [ i ])[ 0 ][ 0 ] for i in range ( Nx )]) assess_ext def assess_ext ( self , now , x , mu , P ) Kalman filter (Gaussian) assessment. View Source def assess_ext ( self , now , x , mu , P ) : \"\"\"Kalman filter (Gaussian) assessment.\"\"\" if not np . all ( np . isfinite ( mu )) : raise RuntimeError ( \"Estimates not finite.\" ) if not np . all ( np . isreal ( mu )) : raise RuntimeError ( \"Estimates not Real.\" ) # Don 't check the cov (might not be explicitly availble) now.mu = mu now.err = now.mu - x var = P.diag if isinstance(P, CovMat) else np.diag(P) now.std = np.sqrt(var) # Here, sqrt(2/pi) is the ratio, of MAD/STD for Gaussians now.mad = np.nanmean(now.std) * np.sqrt(2/np.pi) if hasattr(self, 'svals ' ) : P = P . full if isinstance ( P , CovMat ) else P s2 , U = sla . eigh ( P ) now . svals = np . sqrt ( np . maximum ( s2 , 0.0 ))[ ::- 1 ] now . umisf = ( U . T @ now . err )[ ::- 1 ] average_in_time def average_in_time ( self , kk = None , kkObs = None , free = False ) Avarage all univariate (scalar) time series. kk time inds for averaging kkObs time inds for averaging obs View Source def average_in_time ( self , kk = None , kkObs = None , free = False ) : \"\"\"Avarage all univariate (scalar) time series. - ``kk`` time inds for averaging - ``kkObs`` time inds for averaging obs \"\"\" chrono = self . HMM . t if kk is None : kk = chrono . mask_BI if kkObs is None : kkObs = chrono . maskObs_BI def average1 ( tseries ) : avrgs = Avrgs () def average_multivariate () : return avrgs # Plain averages of nd - series are rarely interesting . # => Shortcircuit => Leave for manual computations if isinstance ( tseries , series . FAUSt ) : # Average series for each subscript if tseries . item_shape != () : return average_multivariate () for sub in [ ch for ch in 'fas' if hasattr(tseries, ch) ] : avrgs [ sub ] = series . mean_with_conf ( tseries [ kkObs, sub ] ) if tseries . store_u : avrgs [ 'u' ] = series . mean_with_conf ( tseries [ kk, 'u' ] ) elif isinstance ( tseries , DataSeries ) : if tseries . array . shape [ 1: ] != () : return average_multivariate () elif len ( tseries . array ) == self . KObs + 1 : avrgs = series . mean_with_conf ( tseries [ kkObs ] ) elif len ( tseries . array ) == self . K + 1 : avrgs = series . mean_with_conf ( tseries [ kk ] ) else : raise ValueError elif np . isscalar ( tseries ) : avrgs = tseries # Eg . just copy over \"duration\" from stats else : raise TypeError ( f \"Don't know how to average {tseries}\" ) return avrgs def recurse_average ( stat_parent , avrgs_parent ) : for key in getattr ( stat_parent , \"stat_register\" , [] ) : try : tseries = getattr ( stat_parent , key ) except AttributeError : continue # Eg assess_ens () deletes . weights if None avrgs = average1 ( tseries ) recurse_average ( tseries , avrgs ) avrgs_parent [ key ] = avrgs avrgs = Avrgs () recurse_average ( self , avrgs ) self . xp . avrgs = avrgs if free : delattr ( self . xp , 'stats' ) derivative_stats def derivative_stats ( self , now ) Stats that derive from others (=> not specific for _ens or _ext). View Source def derivative_stats ( self , now ): \"\"\"Stats that derive from others (=> not specific for _ens or _ext).\"\"\" now . gscore = 2 * np . log ( now . std ) + ( now . err / now . std ) ** 2 new_series def new_series ( self , name , shape , length = 'FAUSt' , MS = False , ** kws ) Create (and register) a statistics time series. Series are initialized with nan's. Example: Create ndarray of length KObs+1 for inflation time series: self.new_series('infl', 1, KObs+1) NB: The sliding_diagnostics liveplotting relies on detecting nan 's to avoid plotting stats that are not being used. => Cannot use dtype=bool or int for stats that get plotted. View Source def new_series ( self , name , shape , length = 'FAUSt' , MS = False , ** kws ) : \" \"\" Create (and register) a statistics time series. Series are initialized with nan's. Example: Create ndarray of length KObs+1 for inflation time series: >>> self.new_series('infl', 1, KObs+1) NB: The ``sliding_diagnostics`` liveplotting relies on detecting ``nan``'s to avoid plotting stats that are not being used. => Cannot use ``dtype=bool`` or ``int`` for stats that get plotted. \"\" \" # Convert int shape to tuple if not hasattr ( shape , '__len__' ) : if shape == 1 : shape = () else : shape = ( shape ,) def make_series ( parent , name , shape ) : if length == 'FAUSt' : total_shape = self . K , self . KObs , shape store_opts = self . store_u , self . store_s tseries = series . FAUSt ( * total_shape , * store_opts , ** kws ) else : total_shape = ( length ,) + shape tseries = DataSeries ( total_shape , * kws ) register_stat ( parent , name , tseries ) # Principal series make_series ( self , name , shape ) # Summary (scalar) series: if shape != () : if MS : for suffix in self . field_summaries : make_series ( getattr ( self , name ), suffix , ()) # Make a nested level for sectors if MS == 'sec' : for ss in self . sector_summaries : suffix , sector = ss . split ( '.' ) make_series ( dict_tools . deep_getattr ( self , f \"{name}.{suffix}\" ), sector , ()) replay def replay ( self , figlist = 'default' , speed = inf , t1 = 0 , t2 = None , ** kwargs ) Replay LivePlot with what's been stored in 'self'. t1, t2: time window to plot. 'figlist' and 'speed': See LivePlot's doc. .. note:: store_u (whether to store non-obs-time stats) must have been True to have smooth graphs as in the actual LivePlot. .. note:: Ensembles are generally not stored in the stats and so cannot be replayed. View Source def replay ( self , figlist = \"default\" , speed = np . inf , t1 = 0 , t2 = None , ** kwargs ) : \" \"\" Replay LivePlot with what's been stored in 'self'. - t1, t2: time window to plot. - 'figlist' and 'speed': See LivePlot's doc. .. note:: ``store_u`` (whether to store non-obs-time stats) must have been ``True`` to have smooth graphs as in the actual LivePlot. .. note:: Ensembles are generally not stored in the stats and so cannot be replayed. \"\" \" # Time settings chrono = self . HMM . t if t2 is None : t2 = t1 + chrono . Tplot # Ens does not get stored in stats, so we cannot replay that. # If the LPs are initialized with P0!=None, then they will avoid ens plotting. # TODO 2: This system for switching from Ens to stats must be replaced. # It breaks down when M is very large. try : P0 = np . full_like ( self . HMM . X0 . C . full , np . nan ) except AttributeError : # e.g. if X0 is defined via sampling func P0 = np . eye ( self . HMM . Nx ) LP = liveplotting . LivePlot ( self , figlist , P = P0 , speed = speed , Tplot = t2 - t1 , replay = True , ** kwargs ) plt . pause ( .01 ) # required when speed=inf # Remember: must use progbar to unblock read1. # Let's also make a proper description. desc = self . xp . da_method + \" (replay)\" # Play through assimilation cycles for k , kObs , t , dt in utils . progbar ( chrono . ticker , desc ) : if t1 <= t <= t2 : if kObs is not None : LP . update (( k , kObs , 'f' ), None , None ) LP . update (( k , kObs , 'a' ), None , None ) LP . update (( k , kObs , 'u' ), None , None ) # Pause required when speed=inf. # On Mac, it was also necessary to do it for each fig. for name , ( num , updater ) in LP . figures . items () : if plt . fignum_exists ( num ) and getattr ( updater , 'is_active' , 1 ) : plt . figure ( num ) plt . pause ( 0.01 ) summarize_marginals def summarize_marginals ( self , now ) Compute Mean-field and RMS values View Source def summarize_marginals ( self , now ) : \"Compute Mean-field and RMS values\" formulae = { ** self . field_summaries , ** self . sector_summaries } with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : for stat in list ( now ) : field = now [ stat ] for suffix , formula in formulae . items () : statpath = stat + '.' + suffix if dict_tools . deep_hasattr ( self , statpath ) : now [ statpath ] = formula ( field )","title":"Stats"},{"location":"reference/dapper/stats/#module-dapperstats","text":"Provide the stats class which defines the \"builtin\" stats to be computed. View Source \"\"\"Provide the stats class which defines the \"builtin\" stats to be computed.\"\"\" from dapper.dict_tools import DotDict from dapper.tools.matrices import CovMat import dapper.dict_tools as dict_tools from matplotlib import pyplot as plt from dapper.dpr_config import rc import dapper.tools.liveplotting as liveplotting import dapper.tools.series as series from dapper.tools.series import StatPrint , DataSeries import dapper.tools.utils as utils from dapper.tools.math import unbias_var import numpy as np import scipy.linalg as sla import warnings class Stats ( StatPrint ): \"\"\"Contains and computes statistics of the DA methods. Use new_series() to register your own stat time series. \"\"\" def __init__ ( self , xp , HMM , xx , yy , liveplots = False , store_u = rc . store_u ): \"\"\"Init the default statistics. Note: Python allows dynamically creating attributes, so you can easily add custom stat. series to a Stat instance within a particular method, for example. Use ``new_series`` to get automatic averaging too. \"\"\" ###################################### # Preamble ###################################### self . xp = xp self . HMM = HMM self . xx = xx self . yy = yy self . liveplots = liveplots self . store_u = store_u self . store_s = hasattr ( xp , 'Lag' ) # Shapes K = xx . shape [ 0 ] - 1 Nx = xx . shape [ 1 ] KObs = yy . shape [ 0 ] - 1 Ny = yy . shape [ 1 ] self . K , self . Nx = K , Nx self . KObs , self . Ny = KObs , Ny # Methods for summarizing multivariate stats (\"fields\") as scalars # Don't use nanmean here; nan's should get propagated! self . field_summaries = dict ( m = lambda x : np . mean ( x ), # mean-field rms = lambda x : np . sqrt ( np . mean ( x ** 2 )), # root-mean-square ma = lambda x : np . mean ( np . abs ( x )), # mean-absolute gm = lambda x : np . exp ( np . mean ( np . log ( x ))), # geometric mean ) # Only keep the methods listed in rc self . field_summaries = dict_tools . intersect ( self . field_summaries , rc . field_summaries ) # Define similar methods, but restricted to sectors self . sector_summaries = {} def restrict ( fun , inds ): return ( lambda x : fun ( x [ inds ])) for suffix , formula in self . field_summaries . items (): for sector , inds in HMM . sectors . items (): f = restrict ( formula , inds ) self . sector_summaries [ ' %s . %s ' % ( suffix , sector )] = f ###################################### # Allocate time series of various stats ###################################### self . new_series ( 'mu' , Nx , MS = 'sec' ) # Mean self . new_series ( 'std' , Nx , MS = 'sec' ) # Std. dev. (\"spread\") self . new_series ( 'err' , Nx , MS = 'sec' ) # Error (mu - truth) self . new_series ( 'gscore' , Nx , MS = 'sec' ) # Gaussian (log) score # To save memory, we only store these field means: self . new_series ( 'mad' , 1 ) # Mean abs deviations self . new_series ( 'skew' , 1 ) # Skewness self . new_series ( 'kurt' , 1 ) # Kurtosis if hasattr ( xp , 'N' ): N = xp . N self . new_series ( 'w' , N , MS = True ) # Importance weights self . new_series ( 'rh' , Nx , dtype = int ) # Rank histogram self . _is_ens = True minN = min ( Nx , N ) do_spectral = np . sqrt ( Nx * N ) <= rc . comp_threshold_b else : self . _is_ens = False minN = Nx do_spectral = Nx <= rc . comp_threshold_b if do_spectral : # Note: the mean-field and RMS time-series of # (i) svals and (ii) umisf should match the corresponding series of # (i) std and (ii) err. self . new_series ( 'svals' , minN ) # Principal component (SVD) scores self . new_series ( 'umisf' , minN ) # Error in component directions ###################################### # Allocate a few series for outside use ###################################### self . new_series ( 'trHK' , 1 , KObs + 1 ) self . new_series ( 'infl' , 1 , KObs + 1 ) self . new_series ( 'iters' , 1 , KObs + 1 ) # Weight-related self . new_series ( 'N_eff' , 1 , KObs + 1 ) self . new_series ( 'wroot' , 1 , KObs + 1 ) self . new_series ( 'resmpl' , 1 , KObs + 1 ) def new_series ( self , name , shape , length = 'FAUSt' , MS = False , ** kws ): \"\"\"Create (and register) a statistics time series. Series are initialized with nan's. Example: Create ndarray of length KObs+1 for inflation time series: >>> self.new_series('infl', 1, KObs+1) NB: The ``sliding_diagnostics`` liveplotting relies on detecting ``nan``'s to avoid plotting stats that are not being used. => Cannot use ``dtype=bool`` or ``int`` for stats that get plotted. \"\"\" # Convert int shape to tuple if not hasattr ( shape , '__len__' ): if shape == 1 : shape = () else : shape = ( shape ,) def make_series ( parent , name , shape ): if length == 'FAUSt' : total_shape = self . K , self . KObs , shape store_opts = self . store_u , self . store_s tseries = series . FAUSt ( * total_shape , * store_opts , ** kws ) else : total_shape = ( length ,) + shape tseries = DataSeries ( total_shape , * kws ) register_stat ( parent , name , tseries ) # Principal series make_series ( self , name , shape ) # Summary (scalar) series: if shape != (): if MS : for suffix in self . field_summaries : make_series ( getattr ( self , name ), suffix , ()) # Make a nested level for sectors if MS == 'sec' : for ss in self . sector_summaries : suffix , sector = ss . split ( '.' ) make_series ( dict_tools . deep_getattr ( self , f \" { name } . { suffix } \" ), sector , ()) @property def data_series ( self ): return [ k for k in vars ( self ) if isinstance ( getattr ( self , k ), DataSeries )] def assess ( self , k , kObs = None , faus = None , E = None , w = None , mu = None , Cov = None ): \"\"\"Common interface for both assess_ens and _ext. The _ens assessment function gets called if E is not None, and _ext if mu is not None. faus: One or more of ['f',' a', 'u'], indicating that the result should be stored in (respectively) the forecast/analysis/universal attribute. Default: 'u' if kObs is None else 'au' ('a' and 'u'). \"\"\" # Initial consistency checks. if k == 0 : if kObs is not None : raise KeyError ( \"DAPPER convention: no obs at t=0.\" \" Helps avoid bugs.\" ) if faus is None : faus = 'u' if self . _is_ens == True : if E is None : raise TypeError ( \"Expected ensemble input but E is None\" ) if mu is not None : raise TypeError ( \"Expected ensemble input but mu/Cov is not None\" ) else : if E is not None : raise TypeError ( \"Expected mu/Cov input but E is not None\" ) if mu is None : raise TypeError ( \"Expected mu/Cov input but mu is None\" ) # Default. Don't add more defaults. It just gets confusing. if faus is None : faus = 'u' if kObs is None else 'au' # Select assessment call and arguments if self . _is_ens : _assess = self . assess_ens _prms = { 'E' : E , 'w' : w } else : _assess = self . assess_ext _prms = { 'mu' : mu , 'P' : Cov } for sub in faus : # Skip assessment if ('u' and stats not stored or plotted) if k != 0 and kObs == None : if not ( self . store_u or self . LP_instance . any_figs ): continue # Silence repeat warnings caused by zero variance with np . errstate ( divide = 'call' , invalid = 'call' ): np . seterrcall ( warn_zero_variance ) # Assess stats_now = Avrgs () _assess ( stats_now , self . xx [ k ], ** _prms ) self . derivative_stats ( stats_now ) self . summarize_marginals ( stats_now ) # Write current stats to series for name , val in stats_now . items (): stat = dict_tools . deep_getattr ( self , name ) isFaust = isinstance ( stat , series . FAUSt ) stat [( k , kObs , sub ) if isFaust else kObs ] = val # LivePlot -- Both init and update must come after the assessment. try : self . LP_instance . update (( k , kObs , sub ), E , Cov ) except AttributeError : self . LP_instance = liveplotting . LivePlot ( self , self . liveplots , ( k , kObs , sub ), E , Cov ) def summarize_marginals ( self , now ): \"Compute Mean-field and RMS values\" formulae = { ** self . field_summaries , ** self . sector_summaries } with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): for stat in list ( now ): field = now [ stat ] for suffix , formula in formulae . items (): statpath = stat + '.' + suffix if dict_tools . deep_hasattr ( self , statpath ): now [ statpath ] = formula ( field ) def derivative_stats ( self , now ): \"\"\"Stats that derive from others (=> not specific for _ens or _ext).\"\"\" now . gscore = 2 * np . log ( now . std ) + ( now . err / now . std ) ** 2 def assess_ens ( self , now , x , E , w ): \"\"\"Ensemble and Particle filter (weighted/importance) assessment.\"\"\" N , Nx = E . shape if w is None : w = np . ones ( N ) / N # All equal. Also, rm attr from stats: if hasattr ( self , 'w' ): delattr ( self , 'w' ) else : now . w = w if abs ( w . sum () - 1 ) > 1e-5 : raise RuntimeError ( \"Weights did not sum to one.\" ) if not np . all ( np . isfinite ( E )): raise RuntimeError ( \"Ensemble not finite.\" ) if not np . all ( np . isreal ( E )): raise RuntimeError ( \"Ensemble not Real.\" ) now . mu = w @ E now . err = now . mu - x A = E - now . mu # While A**2 is approx as fast as A*A, # A**3 is 10x slower than A**2 (or A**2.0). # => Use A2 = A**2, A3 = A*A2, A4=A*A3. # But, to save memory, only use A_pow. A_pow = A ** 2 # Compute variances var = w @ A_pow ub = unbias_var ( w , avoid_pathological = True ) var *= ub # Compute standard deviation (\"Spread\") std = np . sqrt ( var ) # NB: biased (even though var is unbiased) now . std = std # For simplicity, use naive (biased) formulae, derived # from \"empirical measure\". See doc/unbiased_skew_kurt.jpg. # Normalize by var. Compute \"excess\" kurt, which is 0 for Gaussians. A_pow *= A now . skew = np . nanmean ( w @ A_pow / ( std * std * std )) A_pow *= A now . kurt = np . nanmean ( w @ A_pow / var ** 2 - 3 ) now . mad = np . nanmean ( w @ abs ( A )) if hasattr ( self , 'svals' ): if N <= Nx : _ , s , UT = sla . svd (( np . sqrt ( w ) * A . T ) . T , full_matrices = False ) s *= np . sqrt ( ub ) # Makes s^2 unbiased now . svals = s now . umisf = UT @ now . err else : P = ( A . T * w ) @ A s2 , U = sla . eigh ( P ) s2 *= ub now . svals = np . sqrt ( s2 . clip ( 0 ))[:: - 1 ] now . umisf = U . T [:: - 1 ] @ now . err # For each state dim [i], compute rank of truth (x) among the ensemble (E) E_x = np . sort ( np . vstack (( E , x )), axis = 0 , kind = 'heapsort' ) now . rh = np . asarray ( [ np . where ( E_x [:, i ] == x [ i ])[ 0 ][ 0 ] for i in range ( Nx )]) def assess_ext ( self , now , x , mu , P ): \"\"\"Kalman filter (Gaussian) assessment.\"\"\" if not np . all ( np . isfinite ( mu )): raise RuntimeError ( \"Estimates not finite.\" ) if not np . all ( np . isreal ( mu )): raise RuntimeError ( \"Estimates not Real.\" ) # Don't check the cov (might not be explicitly availble) now . mu = mu now . err = now . mu - x var = P . diag if isinstance ( P , CovMat ) else np . diag ( P ) now . std = np . sqrt ( var ) # Here, sqrt(2/pi) is the ratio, of MAD/STD for Gaussians now . mad = np . nanmean ( now . std ) * np . sqrt ( 2 / np . pi ) if hasattr ( self , 'svals' ): P = P . full if isinstance ( P , CovMat ) else P s2 , U = sla . eigh ( P ) now . svals = np . sqrt ( np . maximum ( s2 , 0.0 ))[:: - 1 ] now . umisf = ( U . T @ now . err )[:: - 1 ] def average_in_time ( self , kk = None , kkObs = None , free = False ): \"\"\"Avarage all univariate (scalar) time series. - ``kk`` time inds for averaging - ``kkObs`` time inds for averaging obs \"\"\" chrono = self . HMM . t if kk is None : kk = chrono . mask_BI if kkObs is None : kkObs = chrono . maskObs_BI def average1 ( tseries ): avrgs = Avrgs () def average_multivariate (): return avrgs # Plain averages of nd-series are rarely interesting. # => Shortcircuit => Leave for manual computations if isinstance ( tseries , series . FAUSt ): # Average series for each subscript if tseries . item_shape != (): return average_multivariate () for sub in [ ch for ch in 'fas' if hasattr ( tseries , ch )]: avrgs [ sub ] = series . mean_with_conf ( tseries [ kkObs , sub ]) if tseries . store_u : avrgs [ 'u' ] = series . mean_with_conf ( tseries [ kk , 'u' ]) elif isinstance ( tseries , DataSeries ): if tseries . array . shape [ 1 :] != (): return average_multivariate () elif len ( tseries . array ) == self . KObs + 1 : avrgs = series . mean_with_conf ( tseries [ kkObs ]) elif len ( tseries . array ) == self . K + 1 : avrgs = series . mean_with_conf ( tseries [ kk ]) else : raise ValueError elif np . isscalar ( tseries ): avrgs = tseries # Eg. just copy over \"duration\" from stats else : raise TypeError ( f \"Don't know how to average { tseries } \" ) return avrgs def recurse_average ( stat_parent , avrgs_parent ): for key in getattr ( stat_parent , \"stat_register\" , []): try : tseries = getattr ( stat_parent , key ) except AttributeError : continue # Eg assess_ens() deletes .weights if None avrgs = average1 ( tseries ) recurse_average ( tseries , avrgs ) avrgs_parent [ key ] = avrgs avrgs = Avrgs () recurse_average ( self , avrgs ) self . xp . avrgs = avrgs if free : delattr ( self . xp , 'stats' ) def replay ( self , figlist = \"default\" , speed = np . inf , t1 = 0 , t2 = None , ** kwargs ): \"\"\"Replay LivePlot with what's been stored in 'self'. - t1, t2: time window to plot. - 'figlist' and 'speed': See LivePlot's doc. .. note:: ``store_u`` (whether to store non-obs-time stats) must have been ``True`` to have smooth graphs as in the actual LivePlot. .. note:: Ensembles are generally not stored in the stats and so cannot be replayed. \"\"\" # Time settings chrono = self . HMM . t if t2 is None : t2 = t1 + chrono . Tplot # Ens does not get stored in stats, so we cannot replay that. # If the LPs are initialized with P0!=None, then they will avoid ens plotting. # TODO 2: This system for switching from Ens to stats must be replaced. # It breaks down when M is very large. try : P0 = np . full_like ( self . HMM . X0 . C . full , np . nan ) except AttributeError : # e.g. if X0 is defined via sampling func P0 = np . eye ( self . HMM . Nx ) LP = liveplotting . LivePlot ( self , figlist , P = P0 , speed = speed , Tplot = t2 - t1 , replay = True , ** kwargs ) plt . pause ( . 01 ) # required when speed=inf # Remember: must use progbar to unblock read1. # Let's also make a proper description. desc = self . xp . da_method + \" (replay)\" # Play through assimilation cycles for k , kObs , t , dt in utils . progbar ( chrono . ticker , desc ): if t1 <= t <= t2 : if kObs is not None : LP . update (( k , kObs , 'f' ), None , None ) LP . update (( k , kObs , 'a' ), None , None ) LP . update (( k , kObs , 'u' ), None , None ) # Pause required when speed=inf. # On Mac, it was also necessary to do it for each fig. for name , ( num , updater ) in LP . figures . items (): if plt . fignum_exists ( num ) and getattr ( updater , 'is_active' , 1 ): plt . figure ( num ) plt . pause ( 0.01 ) def register_stat ( self , name , value ): setattr ( self , name , value ) if not hasattr ( self , \"stat_register\" ): self . stat_register = [] self . stat_register . append ( name ) class Avrgs ( StatPrint , DotDict ): \"\"\"A DotDict specialized for stat. averages. Embellishments: - StatPrint - tabulate - getattr that supports abbreviations. \"\"\" def tabulate ( self , statkeys = ()): columns = tabulate_avrgs ([ self ], statkeys , decimals = None ) return utils . tab ( columns , headers = \"keys\" ) . replace ( '\u2423' , ' ' ) abbrevs = { 'rmse' : 'err.rms' , 'rmss' : 'std.rms' , 'rmv' : 'std.rms' } # Use getattribute coz it gets called before getattr. def __getattribute__ ( self , key ): \"\"\"Support deep and abbreviated lookup.\"\"\" # key = abbrevs[key] # Instead of this, also support rmse.a: key = '.' . join ( Avrgs . abbrevs . get ( seg , seg ) for seg in key . split ( '.' )) if \".\" in key : return dict_tools . deep_getattr ( self , key ) else : return super () . __getattribute__ ( key ) # In case of degeneracy, variance might be 0, causing warnings # in computing skew/kurt/MGLS (which all normalize by variance). # This should and will yield nan's, but we don't want mere diagnostics # computations to cause repetitive warnings, so we only warn once. # # I would have expected this (more elegant solution?) to work, # but it just makes it worse. # with np.errstate(divide='warn',invalid='warn'), warnings.catch_warnings(): # warnings.simplefilter(\"once\",category=RuntimeWarning) # ... @utils . do_once def warn_zero_variance ( err , flag ): msg = \" \\n \" . join ([ \"Numerical error in stat comps.\" , \"Probably caused by a sample variance of 0.\" ]) warnings . warn ( msg ) # Why not do all columns at once using the tabulate module? Coz # - Want subcolumns, including fancy formatting (e.g. +/-) # - Want separation (using '|') of attr and stats # - ... def tabulate_column ( col , header , pad = '\u2423' , missingval = '' , frmt = None ): \"\"\"Format a single column, return as list. - Use tabulate() to get decimal point alignment. - Inf and nan are handled individually so that they don't align left of the decimal point (makes too wide columns). Custom ``frmt`` also supported. - Pad (on the right) each row so that the widths are equal. \"\"\" def preprocess ( x ): try : # Custom frmt supplied if frmt is not None : return frmt ( x ) # Standard formatting if x is None : return missingval elif np . isnan ( x ): return \"NAX\" elif x == - np . inf : return \"-INX\" elif x == np . inf : return \"INX\" return x # leave formatting to tabulate() except TypeError : return missingval def postprocess ( s ): s = s . replace ( \"NAX\" , \"nan\" ) s = s . replace ( \"INX\" , \"inf\" ) return s # Make text column, aligned col = [[ preprocess ( x )] for x in col ] col = utils . tab ( col , [ header ], 'plain' ) col = col . split ( \" \\n \" ) # NOTE: dont use splitlines (removes empty lines) # Undo nan/inf treatment col = [ postprocess ( s ) for s in col ] # Pad on the right, for equal widths mxW = max ( len ( s ) for s in col ) col = [ s . ljust ( mxW ) for s in col ] # Use pad char. on BOTH left/right, to prevent trunc. by later tabulate(). col = [ s . replace ( \" \" , pad ) for s in col ] return col def unpack_uqs ( uq_list , decimals = None , cols = ( \"val\" , \"conf\" )): \"\"\"Make array whose (named) cols are ``[uq.col for uq in uq_list]``. Embellishments: - Insert None (in each col) if uq is None. - Apply uq.round() when extracting val & conf. \"\"\" def unpack1 ( arr , i , uq ): if uq is None : return # val/conf if decimals is None : v , c = uq . round ( mult = 0.1 ) else : v , c = np . round ([ uq . val , uq . conf ], decimals ) arr [ \"val\" ][ i ], arr [ \"conf\" ][ i ] = v , c # Others for col in dict_tools . complement ( cols , [ \"val\" , \"conf\" ]): try : arr [ col ][ i ] = getattr ( uq , col ) except AttributeError : pass # np.array with named columns. \"O\" => allow storing None's. dtypes = np . dtype ([( c , \"O\" ) for c in cols ]) arr = np . full_like ( uq_list , dtype = dtypes , fill_value = None ) for i , uq in enumerate ( uq_list ): unpack1 ( arr , i , uq ) return arr def tabulate_avrgs ( avrgs_list , statkeys = (), decimals = None ): \"\"\"Tabulate avrgs (val\u00b1conf).\"\"\" if not statkeys : statkeys = [ 'rmse.a' , 'rmv.a' , 'rmse.f' ] columns = {} for stat in statkeys : column = unpack_uqs ( [ getattr ( a , stat , None ) for a in avrgs_list ], decimals ) vals = tabulate_column ( column [ \"val\" ], stat ) confs = tabulate_column ( column [ \"conf\" ], '1\u03c3' ) headr = vals [ 0 ] + ' 1\u03c3' mattr = [ v + ' \u00b1' + c for v , c in zip ( vals , confs )][ 1 :] columns [ headr ] = mattr return columns","title":"Module dapper.stats"},{"location":"reference/dapper/stats/#functions","text":"","title":"Functions"},{"location":"reference/dapper/stats/#register_stat","text":"def register_stat ( self , name , value ) View Source def register_stat ( self , name , value ) : setattr ( self , name , value ) if not hasattr ( self , \"stat_register\" ) : self . stat_register = [] self . stat_register . append ( name )","title":"register_stat"},{"location":"reference/dapper/stats/#tabulate_avrgs","text":"def tabulate_avrgs ( avrgs_list , statkeys = (), decimals = None ) Tabulate avrgs (val\u00b1conf). View Source def tabulate_avrgs ( avrgs_list , statkeys = (), decimals = None ) : \"\"\"Tabulate avrgs (val\u00b1conf).\"\"\" if not statkeys : statkeys = [ 'rmse.a', 'rmv.a', 'rmse.f' ] columns = {} for stat in statkeys : column = unpack_uqs ( [ getattr(a, stat, None) for a in avrgs_list ] , decimals ) vals = tabulate_column ( column [ \"val\" ] , stat ) confs = tabulate_column ( column [ \"conf\" ] , '1\u03c3' ) headr = vals [ 0 ]+ ' 1\u03c3' mattr = [ v + ' \u00b1'+c for v, c in zip(vals, confs) ][ 1: ] columns [ headr ] = mattr return columns","title":"tabulate_avrgs"},{"location":"reference/dapper/stats/#tabulate_column","text":"def tabulate_column ( col , header , pad = '\u2423' , missingval = '' , frmt = None ) Format a single column, return as list. Use tabulate() to get decimal point alignment. Inf and nan are handled individually so that they don't align left of the decimal point (makes too wide columns). Custom frmt also supported. Pad (on the right) each row so that the widths are equal. View Source def tabulate_column ( col , header , pad = '\u2423' , missingval = '' , frmt = None ) : \" \"\" Format a single column, return as list. - Use tabulate() to get decimal point alignment. - Inf and nan are handled individually so that they don't align left of the decimal point (makes too wide columns). Custom ``frmt`` also supported. - Pad (on the right) each row so that the widths are equal. \"\" \" def preprocess ( x ) : try : # Custom frmt supplied if frmt is not None : return frmt ( x ) # Standard formatting if x is None : return missingval elif np . isnan ( x ) : return \"NAX\" elif x == - np . inf : return \"-INX\" elif x == np . inf : return \"INX\" return x # leave formatting to tabulate() except TypeError : return missingval def postprocess ( s ) : s = s . replace ( \"NAX\" , \"nan\" ) s = s . replace ( \"INX\" , \"inf\" ) return s # Make text column, aligned col = [[ preprocess ( x ) ] for x in col ] col = utils . tab ( col , [ header ] , 'plain' ) col = col . split ( \" \\n \" ) # NOTE: dont use splitlines (removes empty lines) # Undo nan/inf treatment col = [ postprocess ( s ) for s in col ] # Pad on the right, for equal widths mxW = max ( len ( s ) for s in col ) col = [ s . ljust ( mxW ) for s in col ] # Use pad char. on BOTH left/right, to prevent trunc. by later tabulate(). col = [ s . replace ( \" \" , pad ) for s in col ] return col","title":"tabulate_column"},{"location":"reference/dapper/stats/#unpack_uqs","text":"def unpack_uqs ( uq_list , decimals = None , cols = ( 'val' , 'conf' ) ) Make array whose (named) cols are [uq.col for uq in uq_list] . Embellishments: - Insert None (in each col) if uq is None. - Apply uq.round() when extracting val & conf. View Source def unpack_uqs ( uq_list , decimals = None , cols = ( \"val\" , \"conf\" )) : \"\"\"Make array whose (named) cols are ``[uq.col for uq in uq_list]``. Embellishments: - Insert None (in each col) if uq is None. - Apply uq.round() when extracting val & conf. \"\"\" def unpack1 ( arr , i , uq ) : if uq is None : return # val / conf if decimals is None : v , c = uq . round ( mult = 0.1 ) else : v , c = np . round ( [ uq.val, uq.conf ] , decimals ) arr [ \"val\" ][ i ] , arr [ \"conf\" ][ i ] = v , c # Others for col in dict_tools . complement ( cols , [ \"val\", \"conf\" ] ) : try : arr [ col ][ i ] = getattr ( uq , col ) except AttributeError : pass # np . array with named columns . \"O\" => allow storing None ' s . dtypes = np . dtype ( [ (c, \"O\") for c in cols ] ) arr = np . full_like ( uq_list , dtype = dtypes , fill_value = None ) for i , uq in enumerate ( uq_list ) : unpack1 ( arr , i , uq ) return arr","title":"unpack_uqs"},{"location":"reference/dapper/stats/#classes","text":"","title":"Classes"},{"location":"reference/dapper/stats/#avrgs","text":"class Avrgs ( * args , ** kwargs ) A DotDict specialized for stat. averages. Embellishments: - StatPrint - tabulate - getattr that supports abbreviations. View Source class Avrgs ( StatPrint , DotDict ) : \"\"\"A DotDict specialized for stat. averages. Embellishments: - StatPrint - tabulate - getattr that supports abbreviations. \"\"\" def tabulate ( self , statkeys = ()) : columns = tabulate_avrgs ( [ self ] , statkeys , decimals = None ) return utils . tab ( columns , headers = \"keys\" ). replace ( '\u2423' , ' ' ) abbrevs = { 'rmse' : 'err.rms' , 'rmss' : 'std.rms' , 'rmv' : 'std.rms' } # Use getattribute coz it gets called before getattr . def __getattribute__ ( self , key ) : \"\"\"Support deep and abbreviated lookup.\"\"\" # key = abbrevs [ key ] # Instead of this , also support rmse . a : key = '.' . join ( Avrgs . abbrevs . get ( seg , seg ) for seg in key . split ( '.' )) if \".\" in key : return dict_tools . deep_getattr ( self , key ) else : return super (). __getattribute__ ( key )","title":"Avrgs"},{"location":"reference/dapper/stats/#ancestors-in-mro","text":"dapper.tools.series.StatPrint dapper.dict_tools.NicePrint dapper.dict_tools.DotDict dapper.dict_tools.AlignedDict builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/dapper/stats/#class-variables","text":"abbrevs printopts","title":"Class variables"},{"location":"reference/dapper/stats/#methods","text":"","title":"Methods"},{"location":"reference/dapper/stats/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/dapper/stats/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/dapper/stats/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/dapper/stats/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/dapper/stats/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/dapper/stats/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/dapper/stats/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/dapper/stats/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/dapper/stats/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/dapper/stats/#tabulate","text":"def tabulate ( self , statkeys = () ) View Source def tabulate ( self , statkeys = ()) : columns = tabulate_avrgs ( [ self ] , statkeys , decimals = None ) return utils . tab ( columns , headers = \"keys\" ). replace ( '\u2423' , ' ' )","title":"tabulate"},{"location":"reference/dapper/stats/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/dapper/stats/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/dapper/stats/#stats","text":"class Stats ( xp , HMM , xx , yy , liveplots = False , store_u = False ) Contains and computes statistics of the DA methods. Use new_series() to register your own stat time series. View Source class Stats ( StatPrint ) : \"\"\"Contains and computes statistics of the DA methods. Use new_series() to register your own stat time series. \"\"\" def __ init__ ( self , xp , HMM , xx , yy , liveplots = False , store_u = rc . store_u ) : \"\"\"Init the default statistics. Note: Python allows dynamically creating attributes, so you can easily add custom stat. series to a Stat instance within a particular method, for example. Use ``new_series`` to get automatic averaging too. \"\"\" ###################################### # Preamble ###################################### self . xp = xp self . HMM = HMM self . xx = xx self . yy = yy self . liveplots = liveplots self . store_u = store_u self . store_s = hasattr ( xp , 'Lag' ) # Shapes K = xx . shape [ 0 ] - 1 Nx = xx . shape [ 1 ] KObs = yy . shape [ 0 ] - 1 Ny = yy . shape [ 1 ] self . K , self . Nx = K , Nx self . KObs , self . Ny = KObs , Ny # Methods for summarizing multivariate stats ( \"fields\" ) as scalars # Don 't use nanmean here; nan's should get propagated ! self . field_summaries = dict ( m = lambda x : np . mean ( x ), # mean - field rms = lambda x : np . sqrt ( np . mean ( x** 2 )), # root - mean - square ma = lambda x : np . mean ( np . abs ( x )), # mean - absolute gm = lambda x : np . exp ( np . mean ( np . log ( x ))), # geometric mean ) # Only keep the methods listed in rc self . field_summaries = dict_tools . intersect ( self . field_summaries , rc . field_summaries ) # Define similar methods , but restricted to sectors self . sector_summaries = {} def restrict ( fun , inds ) : return ( lambda x : fun ( x [ inds ])) for suffix , formula in self . field_summaries . items () : for sector , inds in HMM . sectors . items () : f = restrict ( formula , inds ) self . sector_summaries [ '%s.%s' % (suffix, sector)] = f ###################################### # Allocate time series of various stats ###################################### self . new_series ( 'mu' , Nx , MS ='sec' ) # Mean self . new_series ( 'std' , Nx , MS ='sec' ) # Std . dev . ( \"spread\" ) self . new_series ( 'err' , Nx , MS ='sec' ) # Error ( mu - truth ) self . new_series ( 'gscore' , Nx , MS ='sec' ) # Gaussian ( log ) score # To save memory , we only store these field means : self . new_series ( 'mad' , 1 ) # Mean abs deviations self . new_series ( 'skew' , 1 ) # Skewness self . new_series ( 'kurt' , 1 ) # Kurtosis if hasattr ( xp , 'N' ) : N = xp . N self . new_series ( 'w' , N , MS = True ) # Importance weights self . new_series ( 'rh' , Nx , dtype = int ) # Rank histogram self . _ is_ens = True minN = min ( Nx , N ) do_spectral = np . sqrt ( Nx * N ) <= rc . comp_threshold_b else : self . _ is_ens = False minN = Nx do_spectral = Nx <= rc . comp_threshold_b if do_spectral: # Note : the mean - field and RMS time - series of # ( i ) svals and ( ii ) umisf should match the corresponding series of # ( i ) std and ( ii ) err . self . new_series ( 'svals' , minN ) # Principal component ( SVD ) scores self . new_series ( 'umisf' , minN ) # Error in component directions ###################################### # Allocate a few series for outside use ###################################### self . new_series ( 'trHK' , 1 , KObs + 1 ) self . new_series ( 'infl' , 1 , KObs + 1 ) self . new_series ( 'iters' , 1 , KObs + 1 ) # Weight - related self . new_series ( 'N_eff' , 1 , KObs + 1 ) self . new_series ( 'wroot' , 1 , KObs + 1 ) self . new_series ( 'resmpl' , 1 , KObs + 1 ) def new_series ( self , name , shape , length='FAUSt' , MS = False , **kws ) : \"\"\"Create (and register) a statistics time series. Series are initialized with nan's. Example: Create ndarray of length KObs+1 for inflation time series: >>> self.new_series('infl', 1, KObs+1) NB: The ``sliding_diagnostics`` liveplotting relies on detecting ``nan``'s to avoid plotting stats that are not being used. => Cannot use ``dtype=bool`` or ``int`` for stats that get plotted. \"\"\" # Convert int shape to tuple if not hasattr ( shape , '__len__' ) : if shape == 1 : shape = () else : shape = ( shape ,) def make_series ( parent , name , shape ) : if length == 'FAUSt': total_shape = self . K , self . KObs , shape store_opts = self . store_u , self . store_s tseries = series . FAUSt ( * total_shape , * store_opts , **kws ) else : total_shape = ( length ,) + shape tseries = DataSeries ( total_shape , * kws ) register_stat ( parent , name , tseries ) # Principal series make_series ( self , name , shape ) # Summary ( scalar ) series : if shape ! = () : if MS : for suffix in self . field_summaries: make_series ( getattr ( self , name ), suffix , ()) # Make a nested level for sectors if MS == 'sec': for ss in self . sector_summaries: suffix , sector = ss . split ( '.' ) make_series ( dict_tools . deep_getattr ( self , f \"{name}.{suffix}\" ), sector , ()) @property def data_series ( self ) : return [ k for k in vars ( self ) if isinstance ( getattr ( self , k ), DataSeries )] def assess ( self , k , kObs = None , faus = None , E = None , w = None , mu = None , Cov = None ) : \"\"\"Common interface for both assess_ens and _ext. The _ens assessment function gets called if E is not None, and _ext if mu is not None. faus: One or more of ['f',' a', 'u'], indicating that the result should be stored in (respectively) the forecast/analysis/universal attribute. Default: 'u' if kObs is None else 'au' ('a' and 'u'). \"\"\" # Initial consistency checks . if k == 0 : if kObs is not None : raise KeyError ( \"DAPPER convention: no obs at t=0.\" \" Helps avoid bugs.\" ) if faus is None : faus = 'u' if self . _ is_ens == True : if E is None : raise TypeError ( \"Expected ensemble input but E is None\" ) if mu is not None : raise TypeError ( \"Expected ensemble input but mu/Cov is not None\" ) else : if E is not None : raise TypeError ( \"Expected mu/Cov input but E is not None\" ) if mu is None : raise TypeError ( \"Expected mu/Cov input but mu is None\" ) # Default . Don 't add more defaults. It just gets confusing. if faus is None: faus = 'u' if kObs is None else 'au' # Select assessment call and arguments if self._is_ens: _assess = self.assess_ens _prms = {' E ': E, 'w': w} else: _assess = self.assess_ext _prms = {'mu': mu, ' P ': Cov} for sub in faus: # Skip assessment if ('u' and stats not stored or plotted) if k != 0 and kObs == None: if not (self.store_u or self.LP_instance.any_figs): continue # Silence repeat warnings caused by zero variance with np.errstate(divide='call', invalid='call'): np.seterrcall(warn_zero_variance) # Assess stats_now = Avrgs() _assess(stats_now, self.xx[k], **_prms) self.derivative_stats(stats_now) self.summarize_marginals(stats_now) # Write current stats to series for name, val in stats_now.items(): stat = dict_tools.deep_getattr(self, name) isFaust = isinstance(stat, series.FAUSt) stat[(k, kObs, sub) if isFaust else kObs] = val # LivePlot -- Both init and update must come after the assessment. try: self.LP_instance.update((k, kObs, sub), E, Cov) except AttributeError: self.LP_instance = liveplotting.LivePlot( self, self.liveplots, (k, kObs, sub), E, Cov) def summarize_marginals(self, now): \"Compute Mean-field and RMS values\" formulae = {**self.field_summaries, **self.sector_summaries} with np.errstate(divide='ignore', invalid='ignore'): for stat in list(now): field = now[stat] for suffix, formula in formulae.items(): statpath = stat+' . '+suffix if dict_tools.deep_hasattr(self, statpath): now[statpath] = formula(field) def derivative_stats(self, now): \"\"\"Stats that derive from others (=> not specific for _ens or _ext).\"\"\" now.gscore = 2*np.log(now.std) + (now.err/now.std)**2 def assess_ens(self, now, x, E, w): \"\"\"Ensemble and Particle filter (weighted/importance) assessment.\"\"\" N, Nx = E.shape if w is None: w = np.ones(N)/N # All equal. Also, rm attr from stats: if hasattr(self, 'w'): delattr(self, 'w') else: now.w = w if abs(w.sum()-1) > 1e-5: raise RuntimeError(\"Weights did not sum to one.\") if not np.all(np.isfinite(E)): raise RuntimeError(\"Ensemble not finite.\") if not np.all(np.isreal(E)): raise RuntimeError(\"Ensemble not Real.\") now.mu = w @ E now.err = now.mu - x A = E - now.mu # While A**2 is approx as fast as A*A, # A**3 is 10x slower than A**2 (or A**2.0). # => Use A2 = A**2, A3 = A*A2, A4=A*A3. # But, to save memory, only use A_pow. A_pow = A**2 # Compute variances var = w @ A_pow ub = unbias_var(w, avoid_pathological=True) var *= ub # Compute standard deviation (\"Spread\") std = np.sqrt(var) # NB: biased (even though var is unbiased) now.std = std # For simplicity, use naive (biased) formulae, derived # from \"empirical measure\". See doc/unbiased_skew_kurt.jpg. # Normalize by var. Compute \"excess\" kurt, which is 0 for Gaussians. A_pow *= A now.skew = np.nanmean(w @ A_pow / (std*std*std)) A_pow *= A now.kurt = np.nanmean(w @ A_pow / var**2 - 3) now.mad = np.nanmean(w @ abs(A)) if hasattr(self, 'svals'): if N <= Nx: _, s, UT = sla.svd((np.sqrt(w)*A.T).T, full_matrices=False) s *= np.sqrt(ub) # Makes s^2 unbiased now.svals = s now.umisf = UT @ now.err else: P = (A.T * w) @ A s2, U = sla.eigh(P) s2 *= ub now.svals = np.sqrt(s2.clip(0))[::-1] now.umisf = U.T[::-1] @ now.err # For each state dim [i], compute rank of truth (x) among the ensemble (E) E_x = np.sort(np.vstack((E, x)), axis=0, kind='heapsort') now.rh = np.asarray( [np.where(E_x[:, i] == x[i])[0][0] for i in range(Nx)]) def assess_ext(self, now, x, mu, P): \"\"\"Kalman filter (Gaussian) assessment.\"\"\" if not np.all(np.isfinite(mu)): raise RuntimeError(\"Estimates not finite.\") if not np.all(np.isreal(mu)): raise RuntimeError(\"Estimates not Real.\") # Don't check the cov ( might not be explicitly availble ) now . mu = mu now . err = now . mu - x var = P . diag if isinstance ( P , CovMat ) else np . diag ( P ) now . std = np . sqrt ( var ) # Here , sqrt ( 2 / pi ) is the ratio , of MAD / STD for Gaussians now . mad = np . nanmean ( now . std ) * np . sqrt ( 2 / np . pi ) if hasattr ( self , 'svals' ) : P = P . full if isinstance ( P , CovMat ) else P s2 , U = sla . eigh ( P ) now . svals = np . sqrt ( np . maximum ( s2 , 0.0 ))[ ::- 1 ] now . umisf = ( U . T @ now . err )[ ::- 1 ] def average_in_time ( self , kk = None , kkObs = None , free = False ) : \"\"\"Avarage all univariate (scalar) time series. - ``kk`` time inds for averaging - ``kkObs`` time inds for averaging obs \"\"\" chrono = self . HMM . t if kk is None : kk = chrono . mask_BI if kkObs is None : kkObs = chrono . maskObs_BI def average1 ( tseries ) : avrgs = Avrgs () def average_multivariate () : return avrgs # Plain averages of nd - series are rarely interesting . # => Shortcircuit => Leave for manual computations if isinstance ( tseries , series . FAUSt ) : # Average series for each subscript if tseries . item_shape ! = () : return average_multivariate () for sub in [ ch for ch in 'fas' if hasattr ( tseries , ch )] : avrgs [ sub ] = series . mean_with_conf ( tseries [ kkObs , sub ]) if tseries . store_u: avrgs [ 'u' ] = series . mean_with_conf ( tseries [ kk , 'u' ]) elif isinstance ( tseries , DataSeries ) : if tseries . array . shape [ 1 : ] ! = () : return average_multivariate () elif len ( tseries . array ) == self . KObs + 1 : avrgs = series . mean_with_conf ( tseries [ kkObs ]) elif len ( tseries . array ) == self . K + 1 : avrgs = series . mean_with_conf ( tseries [ kk ]) else : raise ValueError elif np . isscalar ( tseries ) : avrgs = tseries # Eg . just copy over \"duration\" from stats else : raise TypeError ( f \"Don't know how to average {tseries}\" ) return avrgs def recurse_average ( stat_parent , avrgs_parent ) : for key in getattr ( stat_parent , \"stat_register\" , []) : try : tseries = getattr ( stat_parent , key ) except AttributeError : continue # Eg assess_ens () deletes . weights if None avrgs = average1 ( tseries ) recurse_average ( tseries , avrgs ) avrgs_parent [ key ] = avrgs avrgs = Avrgs () recurse_average ( self , avrgs ) self . xp . avrgs = avrgs if free : delattr ( self . xp , 'stats' ) def replay ( self , figlist= \"default\" , speed = np . inf , t1 = 0 , t2 = None , **kwargs ) : \"\"\"Replay LivePlot with what's been stored in 'self'. - t1, t2: time window to plot. - 'figlist' and 'speed': See LivePlot's doc. .. note:: ``store_u`` (whether to store non-obs-time stats) must have been ``True`` to have smooth graphs as in the actual LivePlot. .. note:: Ensembles are generally not stored in the stats and so cannot be replayed. \"\"\" # Time settings chrono = self . HMM . t if t2 is None : t2 = t1 + chrono . Tplot # Ens does not get stored in stats , so we cannot replay that . # If the LPs are initialized with P0 ! = None , then they will avoid ens plotting . # TODO 2 : This system for switching from Ens to stats must be replaced . # It breaks down when M is very large . try : P0 = np . full_like ( self . HMM . X0 . C . full , np . nan ) except AttributeError : # e . g . if X0 is defined via sampling func P0 = np . eye ( self . HMM . Nx ) LP = liveplotting . LivePlot ( self , figlist , P = P0 , speed = speed , Tplot = t2 - t1 , replay = True , **kwargs ) plt . pause ( .01 ) # required when speed = inf # Remember : must use progbar to unblock read1 . # Let 's also make a proper description. desc = self.xp.da_method + \" (replay)\" # Play through assimilation cycles for k, kObs, t, dt in utils.progbar(chrono.ticker, desc): if t1 <= t <= t2: if kObs is not None: LP.update((k, kObs, 'f'), None, None) LP.update((k, kObs, 'a'), None, None) LP.update((k, kObs, 'u'), None, None) # Pause required when speed=inf. # On Mac, it was also necessary to do it for each fig. for name, (num, updater) in LP.figures.items(): if plt.fignum_exists(num) and getattr(updater, 'is_active ' , 1 ) : plt . figure ( num ) plt . pause ( 0.01 )","title":"Stats"},{"location":"reference/dapper/stats/#ancestors-in-mro_1","text":"dapper.tools.series.StatPrint dapper.dict_tools.NicePrint","title":"Ancestors (in MRO)"},{"location":"reference/dapper/stats/#class-variables_1","text":"printopts","title":"Class variables"},{"location":"reference/dapper/stats/#instance-variables","text":"data_series","title":"Instance variables"},{"location":"reference/dapper/stats/#methods_1","text":"","title":"Methods"},{"location":"reference/dapper/stats/#assess","text":"def assess ( self , k , kObs = None , faus = None , E = None , w = None , mu = None , Cov = None ) Common interface for both assess_ens and _ext. The _ens assessment function gets called if E is not None, and _ext if mu is not None. faus: One or more of ['f',' a', 'u'], indicating that the result should be stored in (respectively) the forecast/analysis/universal attribute. Default: 'u' if kObs is None else 'au' ('a' and 'u'). View Source def assess ( self , k , kObs = None , faus = None , E = None , w = None , mu = None , Cov = None ) : \"\"\"Common interface for both assess_ens and _ext. The _ens assessment function gets called if E is not None, and _ext if mu is not None. faus: One or more of ['f',' a', 'u'], indicating that the result should be stored in (respectively) the forecast/analysis/universal attribute. Default: 'u' if kObs is None else 'au' ('a' and 'u'). \"\"\" # Initial consistency checks . if k == 0 : if kObs is not None : raise KeyError ( \"DAPPER convention: no obs at t=0.\" \" Helps avoid bugs.\" ) if faus is None : faus = 'u' if self . _is_ens == True : if E is None : raise TypeError ( \"Expected ensemble input but E is None\" ) if mu is not None : raise TypeError ( \"Expected ensemble input but mu/Cov is not None\" ) else : if E is not None : raise TypeError ( \"Expected mu/Cov input but E is not None\" ) if mu is None : raise TypeError ( \"Expected mu/Cov input but mu is None\" ) # Default . Don 't add more defaults. It just gets confusing. if faus is None: faus = ' u ' if kObs is None else ' au ' # Select assessment call and arguments if self._is_ens: _assess = self.assess_ens _prms = {' E ': E, ' w ': w} else: _assess = self.assess_ext _prms = {' mu ': mu, ' P ': Cov} for sub in faus: # Skip assessment if (' u ' and stats not stored or plotted) if k != 0 and kObs == None: if not (self.store_u or self.LP_instance.any_figs): continue # Silence repeat warnings caused by zero variance with np.errstate(divide=' call ', invalid=' call ' ) : np . seterrcall ( warn_zero_variance ) # Assess stats_now = Avrgs () _assess ( stats_now , self . xx [ k ] , ** _prms ) self . derivative_stats ( stats_now ) self . summarize_marginals ( stats_now ) # Write current stats to series for name , val in stats_now . items () : stat = dict_tools . deep_getattr ( self , name ) isFaust = isinstance ( stat , series . FAUSt ) stat [ (k, kObs, sub) if isFaust else kObs ] = val # LivePlot -- Both init and update must come after the assessment . try : self . LP_instance . update (( k , kObs , sub ), E , Cov ) except AttributeError : self . LP_instance = liveplotting . LivePlot ( self , self . liveplots , ( k , kObs , sub ), E , Cov )","title":"assess"},{"location":"reference/dapper/stats/#assess_ens","text":"def assess_ens ( self , now , x , E , w ) Ensemble and Particle filter (weighted/importance) assessment. View Source def assess_ens ( self , now , x , E , w ) : \"\"\"Ensemble and Particle filter (weighted/importance) assessment.\"\"\" N , Nx = E . shape if w is None : w = np . ones ( N ) / N # All equal . Also , rm attr from stats : if hasattr ( self , 'w' ) : delattr ( self , 'w' ) else : now . w = w if abs ( w . sum () - 1 ) > 1 e - 5 : raise RuntimeError ( \"Weights did not sum to one.\" ) if not np . all ( np . isfinite ( E )) : raise RuntimeError ( \"Ensemble not finite.\" ) if not np . all ( np . isreal ( E )) : raise RuntimeError ( \"Ensemble not Real.\" ) now . mu = w @ E now . err = now . mu - x A = E - now . mu # While A ** 2 is approx as fast as A * A , # A ** 3 is 10 x slower than A ** 2 ( or A ** 2.0 ). # => Use A2 = A ** 2 , A3 = A * A2 , A4 = A * A3 . # But , to save memory , only use A_pow . A_pow = A ** 2 # Compute variances var = w @ A_pow ub = unbias_var ( w , avoid_pathological = True ) var *= ub # Compute standard deviation ( \"Spread\" ) std = np . sqrt ( var ) # NB : biased ( even though var is unbiased ) now . std = std # For simplicity , use naive ( biased ) formulae , derived # from \"empirical measure\" . See doc / unbiased_skew_kurt . jpg . # Normalize by var . Compute \"excess\" kurt , which is 0 for Gaussians . A_pow *= A now . skew = np . nanmean ( w @ A_pow / ( std * std * std )) A_pow *= A now . kurt = np . nanmean ( w @ A_pow / var** 2 - 3 ) now . mad = np . nanmean ( w @ abs ( A )) if hasattr ( self , 'svals' ) : if N <= Nx : _ , s , UT = sla . svd (( np . sqrt ( w ) * A . T ). T , full_matrices = False ) s *= np . sqrt ( ub ) # Makes s^ 2 unbiased now . svals = s now . umisf = UT @ now . err else : P = ( A . T * w ) @ A s2 , U = sla . eigh ( P ) s2 *= ub now . svals = np . sqrt ( s2 . clip ( 0 ))[ ::- 1 ] now . umisf = U . T [ ::- 1 ] @ now . err # For each state dim [ i ], compute rank of truth ( x ) among the ensemble ( E ) E_x = np . sort ( np . vstack (( E , x )), axis = 0 , kind='heapsort' ) now . rh = np . asarray ( [ np . where ( E_x [ : , i ] == x [ i ])[ 0 ][ 0 ] for i in range ( Nx )])","title":"assess_ens"},{"location":"reference/dapper/stats/#assess_ext","text":"def assess_ext ( self , now , x , mu , P ) Kalman filter (Gaussian) assessment. View Source def assess_ext ( self , now , x , mu , P ) : \"\"\"Kalman filter (Gaussian) assessment.\"\"\" if not np . all ( np . isfinite ( mu )) : raise RuntimeError ( \"Estimates not finite.\" ) if not np . all ( np . isreal ( mu )) : raise RuntimeError ( \"Estimates not Real.\" ) # Don 't check the cov (might not be explicitly availble) now.mu = mu now.err = now.mu - x var = P.diag if isinstance(P, CovMat) else np.diag(P) now.std = np.sqrt(var) # Here, sqrt(2/pi) is the ratio, of MAD/STD for Gaussians now.mad = np.nanmean(now.std) * np.sqrt(2/np.pi) if hasattr(self, 'svals ' ) : P = P . full if isinstance ( P , CovMat ) else P s2 , U = sla . eigh ( P ) now . svals = np . sqrt ( np . maximum ( s2 , 0.0 ))[ ::- 1 ] now . umisf = ( U . T @ now . err )[ ::- 1 ]","title":"assess_ext"},{"location":"reference/dapper/stats/#average_in_time","text":"def average_in_time ( self , kk = None , kkObs = None , free = False ) Avarage all univariate (scalar) time series. kk time inds for averaging kkObs time inds for averaging obs View Source def average_in_time ( self , kk = None , kkObs = None , free = False ) : \"\"\"Avarage all univariate (scalar) time series. - ``kk`` time inds for averaging - ``kkObs`` time inds for averaging obs \"\"\" chrono = self . HMM . t if kk is None : kk = chrono . mask_BI if kkObs is None : kkObs = chrono . maskObs_BI def average1 ( tseries ) : avrgs = Avrgs () def average_multivariate () : return avrgs # Plain averages of nd - series are rarely interesting . # => Shortcircuit => Leave for manual computations if isinstance ( tseries , series . FAUSt ) : # Average series for each subscript if tseries . item_shape != () : return average_multivariate () for sub in [ ch for ch in 'fas' if hasattr(tseries, ch) ] : avrgs [ sub ] = series . mean_with_conf ( tseries [ kkObs, sub ] ) if tseries . store_u : avrgs [ 'u' ] = series . mean_with_conf ( tseries [ kk, 'u' ] ) elif isinstance ( tseries , DataSeries ) : if tseries . array . shape [ 1: ] != () : return average_multivariate () elif len ( tseries . array ) == self . KObs + 1 : avrgs = series . mean_with_conf ( tseries [ kkObs ] ) elif len ( tseries . array ) == self . K + 1 : avrgs = series . mean_with_conf ( tseries [ kk ] ) else : raise ValueError elif np . isscalar ( tseries ) : avrgs = tseries # Eg . just copy over \"duration\" from stats else : raise TypeError ( f \"Don't know how to average {tseries}\" ) return avrgs def recurse_average ( stat_parent , avrgs_parent ) : for key in getattr ( stat_parent , \"stat_register\" , [] ) : try : tseries = getattr ( stat_parent , key ) except AttributeError : continue # Eg assess_ens () deletes . weights if None avrgs = average1 ( tseries ) recurse_average ( tseries , avrgs ) avrgs_parent [ key ] = avrgs avrgs = Avrgs () recurse_average ( self , avrgs ) self . xp . avrgs = avrgs if free : delattr ( self . xp , 'stats' )","title":"average_in_time"},{"location":"reference/dapper/stats/#derivative_stats","text":"def derivative_stats ( self , now ) Stats that derive from others (=> not specific for _ens or _ext). View Source def derivative_stats ( self , now ): \"\"\"Stats that derive from others (=> not specific for _ens or _ext).\"\"\" now . gscore = 2 * np . log ( now . std ) + ( now . err / now . std ) ** 2","title":"derivative_stats"},{"location":"reference/dapper/stats/#new_series","text":"def new_series ( self , name , shape , length = 'FAUSt' , MS = False , ** kws ) Create (and register) a statistics time series. Series are initialized with nan's. Example: Create ndarray of length KObs+1 for inflation time series: self.new_series('infl', 1, KObs+1) NB: The sliding_diagnostics liveplotting relies on detecting nan 's to avoid plotting stats that are not being used. => Cannot use dtype=bool or int for stats that get plotted. View Source def new_series ( self , name , shape , length = 'FAUSt' , MS = False , ** kws ) : \" \"\" Create (and register) a statistics time series. Series are initialized with nan's. Example: Create ndarray of length KObs+1 for inflation time series: >>> self.new_series('infl', 1, KObs+1) NB: The ``sliding_diagnostics`` liveplotting relies on detecting ``nan``'s to avoid plotting stats that are not being used. => Cannot use ``dtype=bool`` or ``int`` for stats that get plotted. \"\" \" # Convert int shape to tuple if not hasattr ( shape , '__len__' ) : if shape == 1 : shape = () else : shape = ( shape ,) def make_series ( parent , name , shape ) : if length == 'FAUSt' : total_shape = self . K , self . KObs , shape store_opts = self . store_u , self . store_s tseries = series . FAUSt ( * total_shape , * store_opts , ** kws ) else : total_shape = ( length ,) + shape tseries = DataSeries ( total_shape , * kws ) register_stat ( parent , name , tseries ) # Principal series make_series ( self , name , shape ) # Summary (scalar) series: if shape != () : if MS : for suffix in self . field_summaries : make_series ( getattr ( self , name ), suffix , ()) # Make a nested level for sectors if MS == 'sec' : for ss in self . sector_summaries : suffix , sector = ss . split ( '.' ) make_series ( dict_tools . deep_getattr ( self , f \"{name}.{suffix}\" ), sector , ())","title":"new_series"},{"location":"reference/dapper/stats/#replay","text":"def replay ( self , figlist = 'default' , speed = inf , t1 = 0 , t2 = None , ** kwargs ) Replay LivePlot with what's been stored in 'self'. t1, t2: time window to plot. 'figlist' and 'speed': See LivePlot's doc. .. note:: store_u (whether to store non-obs-time stats) must have been True to have smooth graphs as in the actual LivePlot. .. note:: Ensembles are generally not stored in the stats and so cannot be replayed. View Source def replay ( self , figlist = \"default\" , speed = np . inf , t1 = 0 , t2 = None , ** kwargs ) : \" \"\" Replay LivePlot with what's been stored in 'self'. - t1, t2: time window to plot. - 'figlist' and 'speed': See LivePlot's doc. .. note:: ``store_u`` (whether to store non-obs-time stats) must have been ``True`` to have smooth graphs as in the actual LivePlot. .. note:: Ensembles are generally not stored in the stats and so cannot be replayed. \"\" \" # Time settings chrono = self . HMM . t if t2 is None : t2 = t1 + chrono . Tplot # Ens does not get stored in stats, so we cannot replay that. # If the LPs are initialized with P0!=None, then they will avoid ens plotting. # TODO 2: This system for switching from Ens to stats must be replaced. # It breaks down when M is very large. try : P0 = np . full_like ( self . HMM . X0 . C . full , np . nan ) except AttributeError : # e.g. if X0 is defined via sampling func P0 = np . eye ( self . HMM . Nx ) LP = liveplotting . LivePlot ( self , figlist , P = P0 , speed = speed , Tplot = t2 - t1 , replay = True , ** kwargs ) plt . pause ( .01 ) # required when speed=inf # Remember: must use progbar to unblock read1. # Let's also make a proper description. desc = self . xp . da_method + \" (replay)\" # Play through assimilation cycles for k , kObs , t , dt in utils . progbar ( chrono . ticker , desc ) : if t1 <= t <= t2 : if kObs is not None : LP . update (( k , kObs , 'f' ), None , None ) LP . update (( k , kObs , 'a' ), None , None ) LP . update (( k , kObs , 'u' ), None , None ) # Pause required when speed=inf. # On Mac, it was also necessary to do it for each fig. for name , ( num , updater ) in LP . figures . items () : if plt . fignum_exists ( num ) and getattr ( updater , 'is_active' , 1 ) : plt . figure ( num ) plt . pause ( 0.01 )","title":"replay"},{"location":"reference/dapper/stats/#summarize_marginals","text":"def summarize_marginals ( self , now ) Compute Mean-field and RMS values View Source def summarize_marginals ( self , now ) : \"Compute Mean-field and RMS values\" formulae = { ** self . field_summaries , ** self . sector_summaries } with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : for stat in list ( now ) : field = now [ stat ] for suffix , formula in formulae . items () : statpath = stat + '.' + suffix if dict_tools . deep_hasattr ( self , statpath ) : now [ statpath ] = formula ( field )","title":"summarize_marginals"}]}